[{"title":"C++ primer笔记","url":"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/","content":"<h1 id=\"C-primer笔记\"><a href=\"#C-primer笔记\" class=\"headerlink\" title=\"C++ primer笔记\"></a>C++ primer笔记</h1><h2 id=\"1-开始\"><a href=\"#1-开始\" class=\"headerlink\" title=\"1 开始\"></a>1 开始</h2><h3 id=\"1-2-初识输入输出\"><a href=\"#1-2-初识输入输出\" class=\"headerlink\" title=\"1.2 初识输入输出\"></a>1.2 初识输入输出</h3><ol>\n<li>cout 输出语句时，尽量要以 endl 结束，从而刷新缓存区，否则如果出现程序崩溃，输出可能还留在缓冲区中。</li>\n</ol>\n<h2 id=\"2-变量和基本类型\"><a href=\"#2-变量和基本类型\" class=\"headerlink\" title=\"2 变量和基本类型\"></a>2 变量和基本类型</h2><h3 id=\"2-2-变量\"><a href=\"#2-2-变量\" class=\"headerlink\" title=\"2.2 变量\"></a>2.2 变量</h3><ol>\n<li>extern：<ul>\n<li>只能作用在函数体外。</li>\n<li>extern 声明一个变量，如果赋予变量初值，则表示定义。</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"2-5-处理类型\"><a href=\"#2-5-处理类型\" class=\"headerlink\" title=\"2.5 处理类型\"></a>2.5 处理类型</h3><ol>\n<li><p>类型别名：</p>\n<ul>\n<li>typedef double wages;</li>\n<li>using my_double &#x3D; double;</li>\n<li>在使用包含指针的类型别名的时候，不能简单地将类型别名替换成原类型，见下图。</li>\n</ul>\n <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/typedef%E6%8C%87%E9%92%88%E7%B1%BB%E5%9E%8B%E5%88%AB%E5%90%8D%E4%B8%8Econst%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8.jpg\" class=\"\" title=\"typedef指针类型别名与const同时使用\">\n</li>\n<li><p>auto</p>\n</li>\n<li><p>decltype：<code>decltype(expression)</code> 返回表达式结果类型（可能是左值）。</p>\n</li>\n</ol>\n<h2 id=\"3字符串、向量和数组\"><a href=\"#3字符串、向量和数组\" class=\"headerlink\" title=\"3字符串、向量和数组\"></a>3字符串、向量和数组</h2><h2 id=\"4-表达式\"><a href=\"#4-表达式\" class=\"headerlink\" title=\"4 表达式\"></a>4 表达式</h2><h3 id=\"4-11-类型转换\"><a href=\"#4-11-类型转换\" class=\"headerlink\" title=\"4.11 类型转换\"></a>4.11 类型转换</h3><ol>\n<li>隐式转换</li>\n<li>显示转换<ul>\n<li>强制类型转换：(double)</li>\n<li>命名的强制类型转换：cast-name&lt;type&gt;(expression)<ul>\n<li><p>static_cast：常用于 void* 的具体化，较大的算术类型降级。</p>\n</li>\n<li><p>dynamic_cast：支持运行时类型识别。详细见 19.2 节。</p>\n</li>\n<li><p>const_cast：将 const 对象转换为非 const 对象，常常用于有函数重载的上下文。</p>\n  <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/const_cast%E5%9C%A8%E5%87%BD%E6%95%B0%E9%87%8D%E8%BD%BD%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.jpg\" class=\"\">\n</li>\n<li><p>reinterpret_cast：<strong>最好别用</strong>。</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"5-语句\"><a href=\"#5-语句\" class=\"headerlink\" title=\"5 语句\"></a>5 语句</h2><h2 id=\"6-函数\"><a href=\"#6-函数\" class=\"headerlink\" title=\"6 函数\"></a>6 函数</h2><h2 id=\"7-类\"><a href=\"#7-类\" class=\"headerlink\" title=\"7 类\"></a>7 类</h2><p>一个好的习惯是：将类中的成员函数声明和定义分离，并尽量设置类的成员变量为私有。如此一来，用户代码只能通过声明的接口对类进行操作，类的实现者对类接口的修改不会影响到用户代码。</p>\n<img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/%E5%B0%86%E7%B1%BB%E7%9A%84%E5%A3%B0%E6%98%8E%E5%92%8C%E5%AE%9A%E4%B9%89%E5%88%86%E7%A6%BB.jpg\" class=\"\">\n\n<p>此时，将类以及与该类相关的友元接口声明在 <code>*.h</code> 文件中，并暴露给用户代码，即用户代码只需 <code>include &quot;*.h&quot;</code> 即可；类及相关友元接口的实现则定义在 <code>*.cpp</code> 中。</p>\n<h3 id=\"7-3-类的其它特性\"><a href=\"#7-3-类的其它特性\" class=\"headerlink\" title=\"7.3 类的其它特性\"></a>7.3 类的其它特性</h3><ol>\n<li><p>mutable 数据成员可以被 const 成员函数修改。</p>\n <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/mutable.jpg\" class=\"\" title=\"mutable\"></li>\n</ol>\n<h3 id=\"7-5-再探构造函数\"><a href=\"#7-5-再探构造函数\" class=\"headerlink\" title=\"7.5 再探构造函数\"></a>7.5 再探构造函数</h3><h4 id=\"7-5-3-使用默认构造函数\"><a href=\"#7-5-3-使用默认构造函数\" class=\"headerlink\" title=\"7.5.3 使用默认构造函数\"></a>7.5.3 使用默认构造函数</h4><p><code>Sales_data obj;</code> √</p>\n<p><code>Sales_data obj();</code> ×，这是在声明函数。</p>\n<h4 id=\"7-5-4-隐式的类类型转换\"><a href=\"#7-5-4-隐式的类类型转换\" class=\"headerlink\" title=\"7.5.4 隐式的类类型转换\"></a>7.5.4 隐式的类类型转换</h4><ol>\n<li><p>只接受一个实参的构造函数，也叫做<strong>转换构造函数</strong>。</p>\n<p> <code>Sales_data obj = string(&quot;123&quot;);</code></p>\n</li>\n<li><p>但是，只允许<strong>一步类类型转化</strong>，比如 <code>Sales_data obj = &quot;123&quot;；</code> 就不被允许。</p>\n</li>\n<li><p>抑制构造函数定义的隐式转换：使用 <code>explicit</code> 关键字</p>\n <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/explicit.jpg\" class=\"\" title=\"explicit\"></li>\n</ol>\n<h2 id=\"8-IO-库\"><a href=\"#8-IO-库\" class=\"headerlink\" title=\"8 IO 库\"></a>8 IO 库</h2><h2 id=\"9-顺序容器\"><a href=\"#9-顺序容器\" class=\"headerlink\" title=\"9 顺序容器\"></a>9 顺序容器</h2><h2 id=\"10-泛型\"><a href=\"#10-泛型\" class=\"headerlink\" title=\"10 泛型\"></a>10 泛型</h2>","categories":["C/C++"],"tags":["C/C++"]},{"title":"CMU15445-Lab","url":"/2023/03/14/CMU15445-Lab/","content":"<h1 id=\"CMU15445-Fall2022\"><a href=\"#CMU15445-Fall2022\" class=\"headerlink\" title=\"CMU15445 Fall2022\"></a>CMU15445 Fall2022</h1><h2 id=\"HW1-SQL\"><a href=\"#HW1-SQL\" class=\"headerlink\" title=\"HW1 SQL\"></a>HW1 SQL</h2><h3 id=\"sqlite3-基础\"><a href=\"#sqlite3-基础\" class=\"headerlink\" title=\"sqlite3 基础\"></a>sqlite3 基础</h3><ol>\n<li>命令行键入 <code>sqlite3 db_name.db</code> 进入数据库；</li>\n<li><code>.tables</code> 展示检查数据库内容；</li>\n<li><code>.scheme table_name</code> 熟悉表的模式（结构）（包含什么属性、索引，主键和外键是什么）；</li>\n</ol>\n<h3 id=\"实验问题\"><a href=\"#实验问题\" class=\"headerlink\" title=\"实验问题\"></a>实验问题</h3><ol>\n<li>q2：使用 <code>||</code> 拼接两个字符串；</li>\n<li>q3：使用 <code>data(&#39;now&#39;)</code> 返回当前时间，<code>substr()</code> 截取子字符串；</li>\n<li>q6：sql 子查询似乎很慢…</li>\n<li>q9：<a href=\"https://www.sqlitetutorial.net/sqlite-window-functions/sqlite-ntile/\">NTILE()</a>；with 视图；</li>\n<li>q10：<a href=\"https://sqlite.org/lang_with.html\">Recursive CTEs</a>；窗口函数 <code>ROW_NUMBER()</code>；</li>\n</ol>\n<h2 id=\"Project1-Storage-Manger\"><a href=\"#Project1-Storage-Manger\" class=\"headerlink\" title=\"Project1 Storage Manger\"></a>Project1 Storage Manger</h2><h3 id=\"实验问题-1\"><a href=\"#实验问题-1\" class=\"headerlink\" title=\"实验问题\"></a>实验问题</h3><h4 id=\"Extendible-Hash-Table\"><a href=\"#Extendible-Hash-Table\" class=\"headerlink\" title=\"Extendible Hash Table\"></a><a href=\"https://15445.courses.cs.cmu.edu/fall2022/project1/#extendible-hash-table\"><strong>Extendible Hash Table</strong></a></h4><p>这是本实验的第一个任务，实现一个可扩展的哈希表，这里的可扩展只需要考虑根据需求增加哈希表的大小，而不需要考虑缩小或压缩。其中哈希表的 <code>Find(k, v)</code> 和 <code>Remove(k)</code> 接口实现相对比较容易，比较麻烦的是 <code>Insert(k, v)</code> 接口，该接口需要考虑扩展哈希表以及为部分键值对重新分配哈希桶的问题。在此之前需要明确几个概念，</p>\n<ol>\n<li>哈希表的全局深度：整个哈希表用到了多少 bit 将 key 映射到某个哈希桶；</li>\n<li>哈希表中的某个桶的局部深度：该桶用到了多少个 bit 将 key 映射到该桶；</li>\n<li>重排哈希桶：当某个哈希桶满时，将新增哈希桶，并利用更多的 bit 将 key 映射到具体哈希桶中，从而减小哈希冲突（哈希桶的最大大小固定）。</li>\n</ol>\n<p>从全局深度和局部深度的概念中不难得到<code>gloabl_depth &gt;= max(local_depth_1, ...)</code>，因此，当想要插入新数据时，但新数据映射到的哈希桶满了，则</p>\n<ol>\n<li><p>若 <code>gloabl_depth = local_depth</code>，那么很遗憾，即使是使用了 <code>gloabl_depth</code> 个 bit 去映射，这个数据映射到的哈希桶仍然是满的，因此毫无疑问，我们需要利用到更多的 bit，以减弱哈希冲突，即 <code>gloabl_depth++</code>，相应的，哈希桶的数量应该翻倍。在这里引入了一个新问题，那就是我们是否需要立即分配一倍的哈希桶空间？这似乎有点像操作系统中的 <code>COW（Copy On Write）</code> 问题，我们这个时候只是一个哈希桶出现了冲突，而如果直接暴力地分配一倍的哈希桶空间，可能是浪费的，那么，如果不立即分配哈希桶，我们增加的 ”哈希桶“ 该指向哪里呢？试想一下，新增哈希桶是因为现在将利用更多的 bit 来进行映射，那么在没有利用更多 bit 映射之前，这些 key 将映射到哪个地方呢？</p>\n <figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (global_depth_ == <span class=\"built_in\">GetLocalDepthInternal</span>(idx)) &#123;  <span class=\"comment\">// global depth is not enough, need more bit to hash</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; (<span class=\"number\">1</span> &lt;&lt; global_depth_); i++) &#123;</span><br><span class=\"line\">    \tdir_.<span class=\"built_in\">push_back</span>(dir_[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    global_depth_++;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>当前已满的哈希桶局部深度 <code>local_depth</code> 应该 +1，这是因为使用原 <code>local_depth</code> 的 bit 数量映射已经不足以解决该桶的哈希冲突了。</p>\n <figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">dir_[idx]-&gt;<span class=\"built_in\">IncrementDepth</span>();</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>最后则是重排哈希桶，此时冲突桶使用到了更多的 bit 来进行映射，此时即触发了一次 <code>COW</code> 操作，需要真正地进行分配新哈希桶空间了，并对冲突桶中已满的所有数据进行重排，以决定哪些留在原桶，哪些从原桶中删除并加入到新桶中。</p>\n <figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> K, <span class=\"keyword\">typename</span> V&gt;</span><br><span class=\"line\"><span class=\"keyword\">auto</span> ExtendibleHashTable&lt;K, V&gt;::<span class=\"built_in\">RedistributeBucket</span>(std::shared_ptr&lt;Bucket&gt; bucket, <span class=\"type\">size_t</span> idx)</span><br><span class=\"line\">    -&gt; std::shared_ptr&lt;Bucket&gt; &#123;</span><br><span class=\"line\">  std::shared_ptr&lt;Bucket&gt; new_bucket = std::<span class=\"built_in\">make_shared</span>&lt;Bucket&gt;(<span class=\"built_in\">Bucket</span>(bucket_size_, bucket-&gt;<span class=\"built_in\">GetDepth</span>()));</span><br><span class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> it = bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">begin</span>(); it != bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">end</span>();) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"built_in\">IndexOf</span>(it-&gt;first, bucket-&gt;<span class=\"built_in\">GetDepth</span>()) == idx) &#123;</span><br><span class=\"line\">      it++;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      new_bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">push_back</span>(&#123;it-&gt;first, it-&gt;second&#125;);</span><br><span class=\"line\">      it = bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">erase</span>(it);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> new_bucket;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> 再将指向新桶的指针加入到哈希表 <code>directory</code> 中的正确位置。<strong>值得注意的是，新桶指针可能会覆盖 directory 中的多个位置，具体数量与 <code>gloabl_depth_ - local_depth_</code> 的值有关。</strong></p>\n</li>\n<li><p>此时哈希冲突可能解决了，也可能发生巧合，就是原来已满的哈希桶中的所有数据使用多一 bit 映射时仍然映射到同一位置，因此需要递归进行 <code>Insert(k, v)</code>。</p>\n</li>\n</ol>\n<h4 id=\"LRU-K-Replacement-Policy\"><a href=\"#LRU-K-Replacement-Policy\" class=\"headerlink\" title=\"LRU-K Replacement Policy\"></a><a href=\"https://15445.courses.cs.cmu.edu/fall2022/project1/#lru-k-replacer\"><strong>LRU-K Replacement Policy</strong></a></h4><p>LRU-K 缓存替换算法，问题的关键是设计相关的数据结构。<code>time_map_</code> 之所以需要设置成 <code>multimap</code>，是因为缓存中后向k重用距离为 <code> inf</code> 的对象可能会有多个。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// map frame_id_t to the queue of its k-access timeStamps</span></span><br><span class=\"line\">std::unordered_map&lt;<span class=\"type\">frame_id_t</span>, std::queue&lt;<span class=\"type\">size_t</span>&gt;&gt; refs_map_;</span><br><span class=\"line\"><span class=\"comment\">// real cache set, map frame_id to its iterator in time_map_</span></span><br><span class=\"line\">std::unordered_map&lt;<span class=\"type\">frame_id_t</span>, std::multimap&lt;<span class=\"type\">size_t</span>, <span class=\"type\">frame_id_t</span>&gt;::iterator&gt; cache_map_;</span><br><span class=\"line\"><span class=\"comment\">// multimap timeStamp to frame_id_t, its size should equal cache_map_&#x27;s size</span></span><br><span class=\"line\">std::multimap&lt;<span class=\"type\">size_t</span>, <span class=\"type\">frame_id_t</span>&gt; time_map_;</span><br><span class=\"line\"><span class=\"comment\">// the set contain all evictable frame_id</span></span><br><span class=\"line\">std::unordered_set&lt;<span class=\"type\">frame_id_t</span>&gt; evictable_frame_id_set_;</span><br></pre></td></tr></table></figure>\n\n<p><strong>踩坑：<code>SetEvictable(frame_id_t frame_id, bool set_evictable)</code> 时如果 frame_id 不在缓存中时，直接 return 就可以了。</strong></p>\n<h4 id=\"Buffer-Pool-Manager-Instance\"><a href=\"#Buffer-Pool-Manager-Instance\" class=\"headerlink\" title=\"Buffer Pool Manager Instance\"></a><a href=\"https://15445.courses.cs.cmu.edu/fall2022/project1/#buffer-pool-instance\"><strong>Buffer Pool Manager Instance</strong></a></h4><p>使用前两步完成的扩展哈希表和 LRU-K 缓存替换算法，构建一个缓冲池，相关接口的注释都比较完整。</p>\n<ol>\n<li><code>NewPgImp(page_id_t)</code> 和 <code>FetchPgImp(page_id_t)</code> 接口实现时一定要记得显示地进行 <code>pages_[frame_id].pin_count++</code> 操作，而不仅仅是执行 <code>replacer.SetEvictable(frame_id, false)</code>。</li>\n<li><code>UnpinPgImp</code> 中 is_dirty： <code>pages_[frame_id].is_dirty_ |= is_dirty</code>。</li>\n<li><code>DeletePgImp</code> 需要检查删除的页在 buffer pool 中是否是脏的，如若是脏还需记得刷盘。</li>\n</ol>\n<h3 id=\"感受\"><a href=\"#感受\" class=\"headerlink\" title=\"感受\"></a>感受</h3><p>人麻了，刚写完代码本地测完感觉很好，线上测试狂冒红光，为了过线上测试花了一天。。。</p>\n<p>（很多奇怪的错误可能是由于锁的粒度太细了，用大锁就OK）</p>\n<h2 id=\"Project2-B-Tree-Index\"><a href=\"#Project2-B-Tree-Index\" class=\"headerlink\" title=\"Project2 B+Tree Index\"></a>Project2 B+Tree Index</h2><h3 id=\"实验问题-2\"><a href=\"#实验问题-2\" class=\"headerlink\" title=\"实验问题\"></a>实验问题</h3><h4 id=\"task1：B-Tree-Pages\"><a href=\"#task1：B-Tree-Pages\" class=\"headerlink\" title=\"task1：B+Tree Pages\"></a>task1：B+Tree Pages</h4><ol>\n<li>internal node 和 leaf node 能容纳的 K&#x2F;V 键值对数目不同。internal node 中的 K&#x2F;V 对中的 V 是 page_id，而 leaf node 中的 K&#x2F;V 对中的 V 是 rid（{page_id, slot_num}），同时 leaf node 还包含额外元数据（next_page_id_）；</li>\n<li>B+Tree 非叶子节点的 <code>min_size = (max_size + 1)/2</code>，叶子节点的 <code>min_size = max_size / 2</code>；</li>\n<li>叶子节点插入后的键值对数量达到 max_size_ 则拆分；非叶子节点插入前的 child 数量为 max_size 则拆分；</li>\n</ol>\n<h4 id=\"task2：B-Tree-Data-Structure\"><a href=\"#task2：B-Tree-Data-Structure\" class=\"headerlink\" title=\"task2：B+Tree Data Structure\"></a>task2：B+Tree Data Structure</h4><ol>\n<li><p>Search</p>\n <img src=\"/2023/03/14/CMU15445-Lab/B+Tree_search.jpg\" class=\"\" title=\"B+Tree_search\">\n\n<ul>\n<li><p>需要注意图中的 <code>/*C是叶结点*/</code> 应该和 while 循环是同一缩进；</p>\n</li>\n<li><p>原书中的 非叶子结点 K&#x2F;V 键值对是如下组织的，忽略最后一个 K，因此和实验中忽略第一个 K 的组织方式略有不同，最终会导致取子树 index 时出现加一减一之类的细微区别（寻找非叶子节点中的第一个严格大于查询键的 Key 的索引更简洁）。</p>\n  <img src=\"/2023/03/14/CMU15445-Lab/node.jpg\" class=\"\" title=\"node\"></li>\n</ul>\n</li>\n</ol>\n","categories":["数据库"],"tags":["CMU15445","公开课","数据库"]},{"title":"CacheDedup（FAST 16）","url":"/2023/05/21/CacheDedup%EF%BC%88FAST%2016%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>使用闪存作为较慢主存的缓存层可以解决存储系统的扩展性问题，但是，闪存容量有限且耐用性较差；</li>\n<li>本文提出一种新的架构，将数据缓存和重删元数据（source addresses and fingerprints of the data）结合起来，有效管理两个组件；</li>\n<li>进一步，提出重复感知的 D-LRU 和 D-ARC 缓存替换算法，优化缓存性能和提高闪存寿命。</li>\n</ol>\n<p><strong>创新性：本文强调，虽然已有将重删和闪存结合起来的研究，但本文谈到还可以将压缩感知结合起来，同时重点介绍了整合重删和缓存过程中的挑战和解决办法。</strong></p>\n<h2 id=\"key-idea\"><a href=\"#key-idea\" class=\"headerlink\" title=\"key idea\"></a>key idea</h2><p>将闪存划分成两个部分：数据缓存和元数据缓存。</p>\n<p>元数据缓存中包含两个重要数据结构：</p>\n<ol>\n<li>source address index map：将后端存储的源地址映射到元数据缓存保存的某一指纹。由于 chunk 内容重复的存在，多个源地址可能映射到同一指纹。（多对一）</li>\n<li>fingerprint store：将指纹映射到数据缓存中的块地址。（一对一）</li>\n</ol>\n<p>因此，对于相同内容的 chunk，它们在数据缓存中只会有一份副本。</p>\n","categories":["Cache"],"tags":["Cache","论文阅读","Deduplication"]},{"title":"CacheSifter（FAST 22）","url":"/2023/04/12/CacheSifter%EF%BC%88FAST%2022%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>移动应用程序经常将下载文件作为缓存文件维护在本地存储中，从而获得更好的用户体验。但是这些缓存文件占据了移动闪存中的一大部分，对移动设备的性能和寿命有很大影响；</li>\n<li>本文提出了 CacheSifter 通用框架，并基于重用行为和内存&#x2F;存储使用量区分缓存文件；在线将缓存文件分为三类，通过移除大概率不会被重用的文件来减少闪存的写入量；</li>\n<li>作者在实际安卓设备上实现了 CacheSifter，并在具有代表性的应用中进行了评估。实验结果表示，依靠 ML 模型，CacheSifter 平均减少了 60% 的缓存文件对闪存的写入量，I&#x2F;O 密集型写性能改善了 20% 左右。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>由于应用的动态性质和整体系统的优化，几乎所有移动应用需要从网络下载文件或数据。而移动设备上的缓存文件可以避免从网络中重新下载数据，从而满足执行延迟的需求。</p>\n<p>当前移动设备首先将缓存文件存储在主存中，然后写回闪存（默认在内存中保存 30s）。但是，近年来由于应用程序对数据量的需求增长，缓存文件的数量和大小按指数级增长，降低了移动设备的性能，同时大量缓存文件写回闪存，减少了移动设备的寿命。</p>\n<p>值得注意的是，一些缓存文件在其生命周期内只被使用一次，而其它缓存文件在被删除之前可能会被重新访问多次，但是目前的实际系统中，这些缓存文件被同等对待。</p>\n<p>现有的一些技术只在 DRAM 上存储缓存文件，减少对闪存的写回，但是，随着应用程序对于缓存文件的需求增长，这会有两个问题：（1）缓存数据在访问频率、生命周期和大小上有很大的区别；（2）在移动设备可用内存不足的情况下，维持无用的缓存文件会因为内存竞争而降低系统的整体性能。本文则根据缓存文件的重用行为管理缓存文件，从而提高系统性能和山存的使用寿命。</p>\n<p>本文提出的通用缓存文件管理框架，CacheSifter，使用轻量级机器学习算法将缓存文件动态分类成不同类别，并基于它们的数据访问模式动态地将不同类别的缓存文件放置在 DRAM 或闪存中。基于重用概率，缓存文件被分成三类：Burn-After-Reading (BAR)  文件，Transient 文件和 Long-living 文件。 </p>\n<h1 id=\"Cache-Files-in-Mobile-Systems\"><a href=\"#Cache-Files-in-Mobile-Systems\" class=\"headerlink\" title=\"Cache Files in Mobile Systems\"></a>Cache Files in Mobile Systems</h1><p><strong>为什么能够对不同缓存文件进行分类？</strong></p>\n<ol>\n<li>实验展示了不同厂商的智能手机、应用程序的<strong>缓存文件平均大小</strong>差异很大；</li>\n<li>实验展示了<strong>缓存文件对闪存的写入量</strong>占总写入量的 64%；</li>\n<li>实验展示<strong>不同应用程序的写入模式</strong>差异很大（社交软件的缓存文件写入量很大，单机游戏的缓存文件写入量很小）；</li>\n</ol>\n<p><strong>如何对不同缓存文件进行分类？</strong></p>\n<ol>\n<li>Burn-After-Reading (BAR)：该类缓存文件只在其生命周期开始阶段有些微重用，没有必要将这类文件写入闪存；</li>\n<li>Transient：只在创建之后的短期内活跃，有较多重用；</li>\n<li>Long-living：其它的缓存文件，表现出长期内持续有较多的重用。</li>\n</ol>\n<p><strong>管理缓存文件的挑战</strong></p>\n<ol>\n<li>缓存文件的重用行为可能会随时间变化，因此，在管理时需要实时地适应缓存文件的行为变化；</li>\n<li>现有系统都是将所有缓存文件同等对待，然而，对于不同类别的缓存文件，需要使用不同的策略进行管理。</li>\n</ol>\n<p>本文的主要目标是改善系统性能和存储寿命；探索应用程序缓存文件的访问模式并考虑基于 DRAM 的内存和基于闪存的移动设备存储在性能和寿命上的特点。</p>\n<h1 id=\"CacheSifter-Design\"><a href=\"#CacheSifter-Design\" class=\"headerlink\" title=\"CacheSifter Design\"></a>CacheSifter Design</h1><h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><h3 id=\"设计原则\"><a href=\"#设计原则\" class=\"headerlink\" title=\"设计原则\"></a>设计原则</h3><ol>\n<li>对应用程序透明：CacheSifter 不能对用户体验造成较大影响，同时，也不能对用户应用程序进行任何更改；</li>\n<li>在线分类：分类器需要适应系统状态的动态变化和用户的配置；</li>\n<li>自适应内存管理：虽然使用内存保存缓存文件可以实现高效访问，但是使用太多的内存会导致系统性能下降。因此，CacheSifter 应该根据不同的活跃程序自适应调整内存使用量；</li>\n<li>适应用户行为的改变：缓存文件的类别可能会发生变化，比如该文件过去是 BAR 文件，但用户行为发生变化后，该文件可能成为 Long-living 文件；</li>\n<li>确保删除缓存文件时的安全：在删除 BAR 和 Transient 文件时，应用程序可能正在使用这些文件执行任务，因此，CacheSifter 在删除数据时不能造成应用程序 crash 或用户数据缺失。</li>\n</ol>\n<h3 id=\"框架\"><a href=\"#框架\" class=\"headerlink\" title=\"框架\"></a>框架</h3><p>CacheSifter 处于内核层，能够直接在页缓存上分类缓存文件而不会带来额外的内存开销和数据复制。所有新下载的文件被保存在主存并等待分类。</p>\n<img src=\"/2023/04/12/CacheSifter%EF%BC%88FAST%2022%EF%BC%89/CacheSifter%E6%A1%86%E6%9E%B6.jpg\" class=\"\" title=\"CacheSifter框架\">\n\n<ol>\n<li>使用一个轻量级机器学习分类引擎在线将缓存文件分类成上述三类中的一类；</li>\n<li>为了更好地利用内存和闪存，丢弃所有 BAR 文件；</li>\n<li>将 Transient 文件保存在内存的一个 <strong>LRU-like</strong> 链表中，它包含一个活跃链表和一个不活跃链表。该类文件加入缓存时，插入活跃链表的头部，当离开活跃链表时，如若在活跃链表的存活时间大于阈值，将降级到不活跃链表的头部，否则直接丢弃；当命中不活跃链表的文件时，将提升到活跃链表的头部。当文件从链表中驱逐后，直接丢弃；</li>\n<li>将 Long-living 文件保存在内存中的另一个 LRU 链表中，当文件从链表中驱逐且 dirty，则加入闪存；只有在应用程序将该类文件标记为无效后，才从系统中删除。</li>\n<li>BAR 文件的重新下载，将升级为 “TR，LL” 文件；TR 文件的重新下载，将升级为 LL 文件。</li>\n</ol>\n<p><strong>CacheSifter 带来了三个好处</strong></p>\n<ol>\n<li>避免将 BAR 和 Transient 文件写入闪存，减少了写竞争，延长了闪存寿命，改善了系统性能，节约了存储空间；</li>\n<li>Transient 文件直接从 DRAM 中访问，改善了该类文件的访问延迟；</li>\n<li>在内核层使用轻量级机器学习引擎优化缓存文件管理，仅带来微小开销且对用户程序透明。</li>\n</ol>\n<h2 id=\"Feature-based-Cache-Files-Management\"><a href=\"#Feature-based-Cache-Files-Management\" class=\"headerlink\" title=\"Feature-based Cache Files Management\"></a>Feature-based Cache Files Management</h2><p>CacheSifter 的有效性很大程度上依赖分类引擎的准确性，同时，开销也应该尽可能小。</p>\n<h3 id=\"Lightweight-Categorization-of-Cache-Files\"><a href=\"#Lightweight-Categorization-of-Cache-Files\" class=\"headerlink\" title=\"Lightweight Categorization of Cache Files\"></a>Lightweight Categorization of Cache Files</h3><p><strong>机器学习分类</strong></p>\n<p>本文选择轻量级神经网络 MLP 作为分类模型，而为了进一步降低开销，分类过程分为两步：首先区分是否是 BAR，如若不是，再区分 Transient&#x2F;Long-living（因为 Long-living 文件无法使用短期信息识别出来）。</p>\n<p>选择 K+6 个特征：（1）该文件创建后的前 K 个单位时间点（比如每个时间点 30s）访问的 I&#x2F;O 大小；（2）读取量、读取次数、写入量、写入次数；（3）文件大小和活跃时间（上次访问时间 - 第一次访问时间）。特别地，为权衡准确率和开销，第一个 MLP 模型需要 K 值小于第二个 MLP 模型。</p>\n<p>离线训练两个 MLP 模型。</p>\n<p><strong>模型评价指标</strong></p>\n<p>准确率、召回率、准确率-召回率曲线。</p>\n<p><strong>性能评价指标</strong></p>\n<p>缓存文件对闪存的写入减少量、闪存寿命、读写性能。</p>\n<p><strong>开销分析</strong></p>\n<p>网络开销、内存开销、CPU时间开销。</p>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读","Multi-level"]},{"title":"GL-Cache（FAST 23）","url":"/2023/04/05/GL-Cache%EF%BC%88FAST%2023%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<p>Web 应用程序严重依赖软件缓存从而实现低延迟和高带宽。为适应工作负载的变化，近年提出了三种学习缓存（学习驱逐）：object-level learning，learning-from-distribution 和 learning-from-simple-experts。但是，这三种方法要么粒度太细（object-level），计算和存储开销比较大，要么就像其它两种一样粒度太粗，无法捕捉对象之间的差异而留下较大的效率提升空间。</p>\n<p>本文提出将相似对象集群到同一组并以组为单位执行学习和驱逐，<code>Learning at the group level accumulates more signals for learning, leverages more features with adaptive weights, and amortizes overheads over objects, thereby achieving both high efficiency and high throughput.</code></p>\n<p>与 SOTA 策略比较，GL-Cache 实现了高带宽和高命中率。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li><p>Large-scale cache deployments enable the success of today’s Internet.</p>\n</li>\n<li><p>The main driving force of cache deployments is the cache’s ability to serve data with high throughput and low latency. Retrieving data from a cache is thousands of times faster than retrieving it from the backend.</p>\n<ul>\n<li>但是缓存总是部署在价格昂贵、容量有限的介质中，缓存容量通常比数据集大小小得多，因此<strong>决定哪些数据存储在缓存中</strong>格外重要。</li>\n<li>高效缓存能够存储更有用得数据从而无需请求后端数据系统就能服务更多的请求。缓存有效性通常被命中率衡量。</li>\n<li>当缓存满时，需要使用驱逐算法决定哪些数据被保存，哪些数据被驱逐。因此，<strong>驱逐策略对于缓存效率非常重要</strong>。</li>\n</ul>\n</li>\n<li><p>这些年许多驱逐算法利用不同的对象特征做驱逐决策。比如一些 LRU 的变体使用新近度的不同角度选择驱逐对象；频率+新近度；频率+对象大小；</p>\n<ul>\n<li>不同特征对于不同的工作负载具有不同程度的重要性，因此只是使用特定方式组合一个或者两个对象特征通常也就只能在一些工作负载中实现高效率。</li>\n<li><strong>learned caches</strong>: employed  machine learning to improve cache evictions.</li>\n</ul>\n</li>\n<li><p>将 learned caches 分为三类</p>\n<ul>\n<li><p>object-level learning（LRB）：learns the next access time for each object using dozens of object features and evicts the object with the furthest predicted request time.</p>\n<ul>\n<li><strong>Advantage</strong>：leverages more object features, learns the relative feature importance, and performs fine-grained learning on each cached object, it has the highest potential for achieving high efficiency.</li>\n<li><strong>Disadvantage</strong>：predicting and ranking objects at each eviction incurs significant computation and storage overheads as we observe LRB suffers from a 775× slow down compared to LRU.</li>\n</ul>\n</li>\n<li><p>learning-from-distribution（LHD）：models request probability distributions to inform eviction decisions. For example, LHD measures object hit density using age and size, and evicts the object with the lowest hit density.</p>\n<ul>\n<li><strong>Advantage</strong>：a lower computation and storage overhead because it models request probability using fewer features at a coarser granularity.</li>\n<li><strong>Disadvantage</strong>：still has a lower throughput compared to simple heuristics (e.g., LRU) because it has to <strong>randomly sample and compare many objects at each eviction</strong>. Moreover, the existing design (e.g., LHD [7]) <strong>does not leverage object features other than age and size, limiting its potential for high efficiency</strong>.</li>\n</ul>\n</li>\n<li><p>learning-from-simple-experts（LeCaR, Cacheus）：performs evictions by choosing eviction candidates recommended by experts (e.g., LRU and LFU), and updates experts’ weights based on their past performance on the workload.</p>\n<ul>\n<li><strong>Disadvantage</strong>：受强化学习中的 “delayed rewards” 限制，在不佳驱逐和该对象再次访问之间，专家权重没有得到更新；性能取决于专家的选择，现有系统使用简单的专家，无法利用到专家未考虑的特征。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>为了克服现有 learned caches 的缺点，提出以对象组为粒度进行学习。Group-level learning leverages multiple group-level features to learn object-group utility for evictions.</p>\n<ul>\n<li>It reduces the computation and storage overheads of learning by hundreds of times through amortization compared to learning at the object level.</li>\n<li>object groups accumulate more “signals” for learning and can leverage a variety of features for prediction, enabling better eviction decisions.</li>\n</ul>\n</li>\n<li><p>组级别学习需要回答以下几个问题</p>\n<ul>\n<li><p>How to group objects and perform evictions efficiently? </p>\n<p>  答：GL-Cache clusters similar objects into groups using write time (§3.3) and evicts the least useful groups using a merge-based eviction(§3.6).</p>\n</li>\n<li><p>How to measure the usefulness of object groups (termed “utility”) to determine the best eviction candidate? </p>\n<p>  答：GL-Cache introduces a group utility function (§3.4) to rank groups, which enables group-based eviction to achieve similar efficiency as object-based eviction (§4.2).</p>\n</li>\n<li><p>How to learn and predict the object-group utility online?</p>\n<p>  答：(§3.5).</p>\n</li>\n</ul>\n</li>\n<li><p>本文贡献</p>\n<ul>\n<li>基于学习粒度将 learned caches 分为三类，并提出一个新的缓存学习方法—— group-level learning。</li>\n<li>设计并实现了 GL-Cache，解决了上面提到的组级别学习的挑战。同时也是第一次定义了 group-level utility function 用于缓存驱逐。</li>\n<li>使用生产跟踪评估 GL-Cache，展现了组级别学习的高效率和高吞吐量。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Background-and-motivation\"><a href=\"#Background-and-motivation\" class=\"headerlink\" title=\"Background and motivation\"></a>Background and motivation</h1><p>两个重要的缓存指标：<strong>efficiency</strong> measured using hit ratio, <strong>performance</strong> measured using throughput.</p>\n<h1 id=\"GL-Cache-Group-level-learned-cache\"><a href=\"#GL-Cache-Group-level-learned-cache\" class=\"headerlink\" title=\"GL-Cache: Group-level learned cache\"></a>GL-Cache: Group-level learned cache</h1><p>The key idea behind group-level learning is to learn the usefulness of groups of objects (called “utility”).</p>\n<h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><ol>\n<li>objects are clustered into fixed-size groups when writing to cache (§3.3).</li>\n<li>The training module in GL-Cache collects training data online and periodically trains a model to learn the utility of object groups (§3.5).</li>\n<li>The inference module predicts object-group utility and ranks object groups for eviction. When the cache is full, object groups are evicted using a merge-based eviction which merges multiple groups into one, evicts most objects, and retains a small portion of popular objects (§3.6).</li>\n</ol>\n<h2 id=\"Group-level-learning\"><a href=\"#Group-level-learning\" class=\"headerlink\" title=\"Group-level learning\"></a>Group-level learning</h2><ol>\n<li>优势<ul>\n<li><strong>Grouping amortizes overheads</strong>：these overheads are amortized over multiple objects in the group. The metadata overhead is only added for each group and the cost of inference computation is also amortized over objects.</li>\n<li><strong>Grouping accumulates more signal</strong>：大多缓存工作负载遵守 Zipf 分布，也就是说大部分的对象只会收到少量的请求，对象级的学习每个对象只能收到很少的信号；而将大量对象集群在一起，该集群就能收获更多的请求，从而更好地学习和预测。</li>\n</ul>\n</li>\n<li>挑战<ul>\n<li>How to cluster objects into groups (§3.3)?</li>\n<li>How to compare the usefulness of object groups (§3.4)?</li>\n<li>How to learn the utility of object groups (§3.5)?</li>\n<li>How to perform evictions at group level (§3.6)?</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Object-groups\"><a href=\"#Object-groups\" class=\"headerlink\" title=\"Object groups\"></a>Object groups</h2><ol>\n<li>当对象进入缓存时，对象的分组就被它的简单静态对象特征决定了，如时间、租户 ID、内容类型、对象大小等。本项工作专注于基于写入时间的分组，这在所有系统中都可用，具有通用性。</li>\n<li>相似时间写入的对象展现出相似的行为，它们的重用时间更接近。</li>\n</ol>\n<h2 id=\"Utility-of-object-groups\"><a href=\"#Utility-of-object-groups\" class=\"headerlink\" title=\"Utility of object groups\"></a>Utility of object groups</h2><p>定义一个函数用于评估 group 的实用性。</p>\n<ol>\n<li>大对象占据更多空间，因此越大的对象有更低的实用性；（从另一个角度看，大对象驱逐后可以留下更多的空间存放更多的对象，因此更“值得”驱逐）</li>\n<li>前向重用距离越大的对象实用性更低；</li>\n</ol>\n<p>但是对象的前向重用距离是无法实时获得的，因此作者提出使用 GL-Cache 学习一个模型从而基于特征来预测对象组的实用性。</p>\n<h2 id=\"Learning-object-group-utility-in-GL-Cache\"><a href=\"#Learning-object-group-utility-in-GL-Cache\" class=\"headerlink\" title=\"Learning object-group utility in GL-Cache\"></a>Learning object-group utility in GL-Cache</h2><ol>\n<li>静态特征：request rete, write rate, miss ratio and mean object size；</li>\n<li>动态特征：age, #requests and #requested objects；</li>\n<li>学习模型：<ul>\n<li>gradient boosting machines (GBM) because tree models do not require feature normalization.</li>\n<li>将学习任务指定为一个回归问题，最小化对象组效用的均方损失。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Evictions-of-object-groups\"><a href=\"#Evictions-of-object-groups\" class=\"headerlink\" title=\"Evictions of object groups\"></a>Evictions of object groups</h2><p>GL-Cache 在组级别使用重量级学习（均摊开销）从而识别出最佳驱逐组，再在驱逐组内充分利用轻量级对象指标（新近度、大小）保留一些非常有用的对象。两层驱逐使得 GL-Cache 能够在学习开销和缓存效率之间实现卓越的平衡。</p>\n<p><strong>key insight：组级别驱逐的命中率竟接近 Belady，没有带来命中率的大幅下降，而这种一次性驱逐多个对象可以分摊推理开销</strong>。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><ol>\n<li><p>Will group-based eviction limit the efficiency upper bound when compared to object-based eviction ? </p>\n<p> <strong>组驱逐并不会成为实现高效率的瓶颈</strong></p>\n <img src=\"/2023/04/05/GL-Cache%EF%BC%88FAST%2023%EF%BC%89/similar_hit_with_min.jpg\" class=\"\" title=\"similar_hit_with_min\">\n</li>\n<li><p>Can GL-Cache improve hit ratio and efficiency over other learned caches ?</p>\n</li>\n<li><p>Can GL-Cache meet production-level throughput requirements and how much overhead does GL-Cache add ? </p>\n</li>\n<li><p>How does GL-Cache improve efficiency without compromising throughput ?</p>\n</li>\n</ol>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"HALP（NSDI 23）","url":"/2023/04/25/HALP%EF%BC%88NSDI%2023%EF%BC%89/","content":"<p>本文的主要想法是：通过启发式算法选择驱逐候选集，再使用学习策略从中选择最佳驱逐对象。这样做可以减少 ML 预测的计算开销。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>learned caches 部署在大规模生产环境中的问题：</p>\n<ol>\n<li>对比于启发式算法，learned caches 训练和预测的计算开销太大了；</li>\n<li>大规模的 CDN 生产环境，即使是小部分区域的缓存性能下降也是不能接受的，因此，在 learned caches 效果不佳时，需要做好兜底操作；</li>\n<li>生产环境中即使是同一机架的机器缓存缺失率也不一样，如何准确评估一个新算法在 production noise 上的效果。</li>\n</ol>\n<h1 id=\"Design\"><a href=\"#Design\" class=\"headerlink\" title=\"Design\"></a>Design</h1><img src=\"/2023/04/25/HALP%EF%BC%88NSDI%2023%EF%BC%89/design.jpg\" class=\"\" title=\"design\">\n\n<p>利用启发式算法从缓存尾部选取 4 个候选集，再以锦标赛方式进行 3 对比较选择最佳驱逐对象。</p>\n<p>1% 的机器使用原有启发式算法，其余机器使用 HALP，因此，可以监控 HALP 对性能的影响，从而及时切换为原有启发式算法。</p>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"Hawkeye (ISCA 16)","url":"/2023/09/10/Hawkeye-ISCA-16/","content":"<p>在片上缓存场景下，将 OPT 缓存替换算法应用于过去的访问请求，从中学习以指导未来的缓存替换决策。</p>\n<p>“<strong>If past behavior is a good predictor of future behavior, then our replacement policy will approach the behavior of Belady’s algorithm.</strong>“</p>\n<h1 id=\"Hawkeye\"><a href=\"#Hawkeye\" class=\"headerlink\" title=\"Hawkeye\"></a>Hawkeye</h1><h2 id=\"overview\"><a href=\"#overview\" class=\"headerlink\" title=\"overview\"></a>overview</h2><img src=\"/2023/09/10/Hawkeye-ISCA-16/overview.jpg\" class=\"\" title=\"overview\">\n\n<p>OPTgen 使用过去的访问记录，模拟 OPT 行为以产生训练数据；</p>\n<p>Hawkeye Predictor 对新插入对象进行分类：缓存友好和缓存厌恶。</p>\n<h2 id=\"OPTgen\"><a href=\"#OPTgen\" class=\"headerlink\" title=\"OPTgen\"></a>OPTgen</h2><p>三个定义</p>\n<ol>\n<li>usage interval：从对某数据的引用 X 开始，到下次对相同数据的引用 X’ 的长度；</li>\n<li>liveness interval：在 OPT 策略下，缓存行在缓存中的驻留时间段；</li>\n<li>occupancy vector：记录每个时刻使用的缓存容量。</li>\n</ol>\n<p>如果在 X 的 usage interval 过程中，任何时刻其它对象的 liveness intervals 重叠数目达到了缓存容量，则在下一次的引用 X’ 时，该数据缺失；否则，该数据命中。（对于 Belady 的理解，最直观的是在缓存驱逐决策过程中，驱逐从此时开始，缓存空间中前向重用距离最大的对象，<strong>而这里给了一个全新的视角，某个对象引用的 usage interval 过程中，其它对象的 liveness intervals 最大重叠数达到了缓存容量，这意味着在此最大重叠时刻，该对象的前向重用距离就已经是最大的了，这从缓存驱逐决策的视角转化为了对象下次重用时是否命中的判断</strong>）。</p>\n<p>对于下图所示，X 第一次引用时的 usage interval 为 6，重用距离为 3，但是，在 liveness interval 过程中，其它对象的 liveness intervals 最大重叠数仅为 1，因此，在第一次引用时，OPT 的判断为 X 下次引用将命中。</p>\n<img src=\"/2023/09/10/Hawkeye-ISCA-16/OPTgen_example.jpg\" class=\"\">\n\n<p>对于下面的访问 trace，OPT 策略使得缓存命中数为 4（<strong>实际上这是带准入的 OPT——当缓存容量满时，如若当前缺失对象的前向重用距离大于缓存中的所有对象，则不会准入，这比不关心准入的一般 OPT 命中率更高</strong>），其中，occupancy vector 的使用方式如下</p>\n<ol>\n<li>最近引用的条目对应值为 0；（如果不考虑准入，则条目初始值为 1）</li>\n<li>某对象的首次引用不会对 occupancy vector 的其他值造成影响，这反映出 OPT 只能基于对象的下次重用做出判断；</li>\n<li>如果对象不是首次引用，OPTgen 将检查该对象对应 usage interval 过程中是否每个引用在 occupancy vector 中的值（等于 liveness intervals  重叠数）都小于缓存容量。如果是，则 OPT 在该对象上次引用时会将其加入缓存，此次访问命中，并且将 usage interval 过程中每个引用在 occupancy vector 中的值加 1；否则，此次访问缺失，无需更新其它引用在 occupancy vector 的值。</li>\n</ol>\n<img src=\"/2023/09/10/Hawkeye-ISCA-16/OPTgen_illustrate.jpg\" class=\"\" title=\"OPTgen_illustrate\">\n\n<h2 id=\"Hawkeye-Predictor\"><a href=\"#Hawkeye-Predictor\" class=\"headerlink\" title=\"Hawkeye Predictor\"></a>Hawkeye Predictor</h2><p>“为何可以做预测” 基于这样一个事实：给定执行 load 命令的 PC，OPT 所做出的决策是相似的。文中提到，90.4% 的 load PC，在 OPT 看来有相同的缓存行为（缓存友好和缓存厌恶）。</p>\n<p>文中提到，预测器有 8K 个条目，并使用 13-bit 的 hashed PC 进行索引，每个条目有一个 3-bit 计数器。如果 OPTgen 确定 X 在 OPT 策略下将是缓存命中，则增大上次访问 X 的 PC 对应条目的计数器，否则，减小计数器。</p>\n<p>对于每次访问，预测器都通过当前的 load 指令 PC 进行索引，对应条目的 3-bit 计数器的高位标识其加载的数据是否缓存友好。</p>\n<h2 id=\"Replacement\"><a href=\"#Replacement\" class=\"headerlink\" title=\"Replacement\"></a>Replacement</h2><p>在缓存替换时，Hawkeye 总体目标是先驱逐缓存厌恶对象，如果没有，则驱逐最久没有访问的缓存友好行（LRU），并减小其在预测器中的对应计数器。</p>\n<p>进一步，作者将 Hawkeye 与 RRIP 结合，使用一个 3-bit 的计数器来标志驱逐优先级，具体做法是</p>\n<ol>\n<li>RRIP 计数器值越大，驱逐优先级越高；</li>\n<li>在每次缓存访问时，Hawkeye 也测其生成一个二进制预测来指示该对象是否缓存友好，如果缓存友好，则赋予其 RRIP&#x3D;0，同时，如果该对象不在缓存，还会增加其它 RRIP&lt;6 的缓存对象的 RRIP 计数器（有点像 CLOCK）；如果缓存厌恶，则赋予其 RRIP&#x3D;7；</li>\n<li>在缓存替换时，任何 RRIP&#x3D;7（缓存厌恶）对象都会被选为驱逐候选者，而如果没有，则驱逐 RRIP 值最大的对象（LRU）。</li>\n</ol>\n<p>以上确保了缓存厌恶对象会首先被驱逐（因为缓存友好对象的 RRIP 计数器最多增长到6）。</p>\n<h1 id=\"与-LRB-的异同\"><a href=\"#与-LRB-的异同\" class=\"headerlink\" title=\"与 LRB 的异同\"></a>与 LRB 的异同</h1><p>相同点</p>\n<ol>\n<li>Hawkeye 使用了 8X 缓存容量的时间窗口，LRB 使用了 2X 缓存容量的时间窗口，它们本质上都是希望从过去的时间窗口内探寻对象的缓存行为；</li>\n<li>在进行驱逐决策时，Hawkeye 驱逐 RRIP 值最大的对象，该值是否为7标识了是否为缓存厌恶，小于7时，其大小则反映了对象的新近度；LRB 驱逐缓存对象中重用距离预测值最大的对象。</li>\n</ol>\n<p>不同点</p>\n<ol>\n<li>Hawkeye 将 OPT 的决策看成一个二分类问题，并使用精彩的 liveness intervals 重叠数来非常准确地模拟出了 OPT 的行为，考虑了前向重用距离和缓存的需求（重叠数）；LRB 将 OPT 决策看成一个回归问题，在时间窗口中简单地使用引用距离来作为标签进行训练；</li>\n<li>Hawkeye 应用于片上缓存，特征仅为加载指令的 PC 值，标签为在 OPT 策略下是否缓存命中；LRB 应用于 CDN 缓存，特征非常多，包括指数衰减新近度、频率、对象大小等等，标签为引用距离，而对于引用距离大于时间窗口（2X 缓存大小）的对象，引用距离即 2X 缓存大小。</li>\n</ol>\n<h1 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h1><p>对于学习缓存，Hawkeye 和 LRB 这两篇不同应用场景下的缓存替换经典工作揭示了学习缓存过程中非常重要的几个问题</p>\n<ol>\n<li>如何建模缓存问题？（这两个工作都是缓存替换，因此这里就是建模缓存替换问题，它们都是将问题建模成从 OPT 算法中学习驱逐的对应关系，Hawkeye 驱逐过去在 OPT 策略下不会缓存命中的对象，LRB 驱逐过去在 OPT 策略下引用距离过大的对象）</li>\n<li>如何获取训练数据？（使用和缓存大小长度相关的时间窗口捕获历史 trace）</li>\n<li>特征和标签是什么？（PC，OPT 策略下是否缓存命中）以及（新近度、频率、对象大小等特征，重用距离）</li>\n<li>如何决定驱逐对象？（使用预测器对缓存对象进行预测，驱逐优先级最高的对象，<strong>对于缓存替换，这一步开销太大了</strong>）</li>\n</ol>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"LHD（NSDI 18）","url":"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/","content":"<h1 id=\"动机\"><a href=\"#动机\" class=\"headerlink\" title=\"动机\"></a>动机</h1><ol>\n<li>启发式缓存替换算法对工作负载行为做了强烈的隐式假设，无法适应不同场景以及负载的动态变化，严重影响了命中率性能。</li>\n<li>prior policies rely on implementation primitives that unnecessarily limit their design.（复杂缓存策略需要复杂的数据结构，例如 GDSF 使用 heap，缓存插入和驱逐的时间复杂度较高；LRU 则是因为需要进行同步所以吞吐量也很低）</li>\n</ol>\n<h1 id=\"Least-Hit-Density-LHD\"><a href=\"#Least-Hit-Density-LHD\" class=\"headerlink\" title=\"Least Hit Density (LHD)\"></a>Least Hit Density (LHD)</h1><img src=\"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/%E5%AE%9A%E4%B9%89.jpg\" class=\"\" title=\"定义\">\n\n<p>从式子来看，命中概率越大，对象越应该保留在缓存；对象越大，占用更大的空间，生命周期越长，占用更久的空间，给缓存总体的 OHR 带来负面影响。特别地，当缓存对象命中或驱逐时，它的 “生命周期” 都算作结束。而 LHD 做的事情，就是动态预测每个对象的 expected hits-per-space-consumed（hit density），驱逐密度最低的对象。</p>\n<p><strong>挑战：在不知道对象未来的命中次数、生命周期的情况下，如何预测对象的命中密度。</strong></p>\n<img src=\"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/%E5%91%BD%E4%B8%AD%E5%AF%86%E5%BA%A6.jpg\" class=\"\" title=\"命中密度\">\n\n<p>上图中的公式(5)表示：当对象已经在缓存存活 a，它的命中密度。其中分子是对象在 [1, ∞] 时间段的命中概率之和，也就是<strong>期望命中概率</strong>，分母则是对象大小和<strong>期望剩余生命周期</strong>的乘积。</p>\n<p>因此，LHD 可以在线监控所有对象的 H 和 L 的分布，分布中的每个点表示，在生命周期 x 时命中以及在生命周期 x 时命中或驱逐的概率。有了这两个分布，就可以通过上图中的公式(5)计算所有缓存对象的命中密度（所有缓存对象已经在缓存中待的时间作为元数据保留）。</p>\n<p>更进一步，如果有额外信息，可以对额外信息中表示的每个类别计算一组 H 和 L 分布，如对象的访问次数。</p>\n<img src=\"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/%E9%99%84%E5%B8%A6%E9%A2%9D%E5%A4%96%E4%BF%A1%E6%81%AF%E7%9A%84%E5%91%BD%E4%B8%AD%E5%AF%86%E5%BA%A6.jpg\" class=\"\" title=\"附带额外信息的命中密度\">\n\n","categories":["Cache"],"tags":["Cache","论文阅读"]},{"title":"Large-scale Analysis at Twitter Cache Clusters（TOS 21）","url":"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>现代 Web 服务广泛使用内存缓存来提高吞吐量并减少访问延迟；</li>\n<li>对生产系统进行工作负载分析，可以推动提高内存缓存系统有效性的研究；</li>\n<li>本文对 Twitter 的多个内存缓存集群进行分析，比如生命周期、流行度分布、大小分布等等。</li>\n</ol>\n<h1 id=\"Production-Stats-and-Workload-Analysis\"><a href=\"#Production-Stats-and-Workload-Analysis\" class=\"headerlink\" title=\"Production Stats and Workload Analysis\"></a>Production Stats and Workload Analysis</h1><h2 id=\"Miss-Ratio\"><a href=\"#Miss-Ratio\" class=\"headerlink\" title=\"Miss Ratio\"></a>Miss Ratio</h2><img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E7%BC%BA%E5%A4%B1%E7%8E%87%E5%8F%98%E5%8C%96.jpg\" class=\"\" title=\"缺失率变化\">\n\n<h2 id=\"Request-Rate\"><a href=\"#Request-Rate\" class=\"headerlink\" title=\"Request Rate\"></a>Request Rate</h2><p>请求率呈现昼夜特征，同时，请求率的尖峰可能是由于热键，也可能是请求对象增多了。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E8%AF%B7%E6%B1%82%E7%8E%87%E5%8F%98%E5%8C%96.jpg\" class=\"\" title=\"请求率变化\">\n\n<h2 id=\"Types-of-Operations\"><a href=\"#Types-of-Operations\" class=\"headerlink\" title=\"Types of Operations\"></a>Types of Operations</h2><p>对于多 trace 的数据集分析和评估，图 a 挺棒的。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E6%93%8D%E4%BD%9C%E7%B1%BB%E5%9E%8B.jpg\" class=\"\" title=\"操作类型\">\n\n<h2 id=\"Time-To-Living（TTL）\"><a href=\"#Time-To-Living（TTL）\" class=\"headerlink\" title=\"Time-To-Living（TTL）\"></a>Time-To-Living（TTL）</h2><p>TTL 相关的累计分布曲线。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/TTL%E7%9A%84%E7%B4%AF%E8%AE%A1%E5%88%86%E5%B8%83.jpg\" class=\"\" title=\"TTL的累计分布\">\n\n<p>不考虑 TTL，工作集大小是当前时间之前所有不同对象大小之和，对于 Web 场景，是无限递增的；但若是考虑了 TTL，工作集大小是当前时间之前所有未过期的不同对象大小之和，随着时间的推移，会趋向于平稳，因此，无限大的缓存容量是没有意义的。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/TTL%E5%AF%B9%E5%B7%A5%E4%BD%9C%E9%9B%86%E5%A4%A7%E5%B0%8F%E7%9A%84%E5%BD%B1%E5%93%8D.jpg\" class=\"\" title=\"TTL对工作集大小的影响\">\n\n<h2 id=\"Popularity-Distribution\"><a href=\"#Popularity-Distribution\" class=\"headerlink\" title=\"Popularity Distribution\"></a>Popularity Distribution</h2><p>一些负载在最流行部分和最不流行部分展现出和齐夫分布的差异。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E6%B5%81%E8%A1%8C%E5%BA%A6.jpg\" class=\"\" title=\"流行度\">\n\n<h2 id=\"Object-Size\"><a href=\"#Object-Size\" class=\"headerlink\" title=\"Object Size\"></a>Object Size</h2><p>推特数据集中的对象大多很小。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E5%AF%B9%E8%B1%A1%E5%A4%A7%E5%B0%8F.jpg\" class=\"\" title=\"对象大小\">\n\n<p>请求大小随时间变化的热力图。（相比将几个 CDF 曲线画在一个图里，用热力图可以看到更细粒度的差距）</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E8%AF%B7%E6%B1%82%E5%A4%A7%E5%B0%8F%E5%88%86%E5%B8%83%E9%9A%8F%E6%97%B6%E9%97%B4%E5%8F%98%E5%8C%96.jpg\" class=\"\" title=\"请求大小分布随时间变化\">\n\n<h2 id=\"Reuse-Distance\"><a href=\"#Reuse-Distance\" class=\"headerlink\" title=\"Reuse Distance\"></a>Reuse Distance</h2><p>对于图 a，是一个经典适用 LRU 替换算法的重用距离累计分布，展现出最近访问的对象，有更高的概率被再次访问；而对于图 b，10^6^ 附近的重用距离占比很高，小于这附近值的缓存大小使用 LRU 效果会很差，因为 LRU 会驱逐重用距离为 10^6^ 附近的对象，而选择保留重用距离为 10^2^ ~ 10^5^ 的对象，FIFO 则平等对待所有对象，不会牺牲全部的较大重用距离对象而选择保留全部重用距离较小的对象。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E9%87%8D%E7%94%A8%E8%B7%9D%E7%A6%BB%E7%9A%84CDF.jpg\" class=\"\" title=\"重用距离的CDF\">","categories":["Cache"],"tags":["Cache","论文阅读","Plot","Cache Trace Analysis"]},{"title":"LRB（NSDI 20）","url":"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/","content":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li><p>内容分发网络承担了绝大部分的网络流量；</p>\n</li>\n<li><p><strong>字节缺失率</strong>是内容分发网络的关键性能指标；</p>\n</li>\n<li><p>CDN 场景下，缓存策略对于实现低缺失率起着关键作用，但是，CDN 场景下的缓存策略大多使用启发式算法，随着工作负载的变化，缓存策略的效果可能会很差。最关键的是，作者在很多 trace 中观察到：<strong>SOTA缓存替换算法和 MIN 算法之间存在巨大差距</strong>；</p>\n</li>\n<li><p>作者提出 LRB 算法，使用机器学习近似 MIN 算法。但是，如果<strong>完全模仿 MIN 算法驱逐前向重用距离最大的对象，那么计算成本就太大了</strong>。退而求其次，<strong>只要某个对象的前向重用距离超过某一阈值，它就可以被驱逐</strong>。</p>\n</li>\n<li><p>阈值—— Belady 边界，定义为从开始到现在，MIN 算法驱逐对象中前向重用距离最小的值。Relaxed Belady 带来了两个好处：</p>\n<ul>\n<li>允许系统在小的采样集中进行预测，比如大小为64，减少计算开销；</li>\n<li>允许系统实时获取训练数据（只需在后面实时跟踪驱逐对象的重用距离是否大于阈值，而无需跟踪驱逐对象是否前向重用距离最大，大大节省了内存开销）从而快速适应工作负载变化；</li>\n</ul>\n</li>\n<li><p><strong>端到端的评估指标，例如字节缺失率，模拟起来非常花费时间</strong>。因此，作者提出使用 <strong>good decision ratio</strong> 指标来评价 ML 架构（包括特征、模型、预测目标和损失函数）的好坏，定义为驱逐对象的前向重用距离实际上是否超过了 Belady 边界。使用该指标，就可以在一次模拟中，收集训练数据、预测数据、学习 Belady 边界。</p>\n</li>\n<li><p>作者抛出的设计问题：</p>\n<ul>\n<li><p>控制 ML 训练和预测的计算开销；</p>\n</li>\n<li><p>限制训练和预测过程中的内存开销；</p>\n</li>\n</ul>\n<ul>\n<li>如何在线获取训练数据；</li>\n</ul>\n<ul>\n<li>如何选择替换候选集；</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Background-and-Motivation\"><a href=\"#Background-and-Motivation\" class=\"headerlink\" title=\"Background and Motivation\"></a>Background and Motivation</h1><ol>\n<li>CDN 缓存缺失，将花费高额带宽代价去后端存储中获取原数据，因此需要最小化缓存的字节命中率；</li>\n<li>CDN 缓存的工作集比容量大很多；</li>\n<li>作者观察到现有 SOTA 缓存替换算法和 MIN 算法之间仍存在巨大差距（这点不存在的话，就没有做的必要了）；</li>\n<li>启发式算法难以适应工作负载的变化；</li>\n<li>实际 CDN 服务器的 CPU 占用率很低，</li>\n</ol>\n<h1 id=\"Approximating-Belady’s-MIN-Algorithm\"><a href=\"#Approximating-Belady’s-MIN-Algorithm\" class=\"headerlink\" title=\"Approximating Belady’s MIN Algorithm\"></a>Approximating Belady’s MIN Algorithm</h1><h2 id=\"Relaxed-Belady-Algorithm\"><a href=\"#Relaxed-Belady-Algorithm\" class=\"headerlink\" title=\"Relaxed Belady Algorithm\"></a>Relaxed Belady Algorithm</h2><p>完全模仿 MIN 算法存在以下问题：</p>\n<ol>\n<li>需要对所有缓存对象进行预测；</li>\n<li>ML 预测器需要准确地预测每个对象的前向重用距离；</li>\n</ol>\n<p>近似 Belady 则是对上面两个问题的开销进行了妥协，从驱逐候选集前向重用距离预测结果大于 Belady 边界的对象中随机驱逐，如果没有，则继续对其余所有对象进行预测，变成完全模拟 Belady。</p>\n<p>对于 Belady 边界来说，越小则跟踪的开销越小；越大则使 relaxed belady 的字节缺失率越接近 MIN。</p>\n<h2 id=\"Belady-Boundary\"><a href=\"#Belady-Boundary\" class=\"headerlink\" title=\"Belady Boundary\"></a>Belady Boundary</h2><p>预热阶段更新 Belady 边界，而在实际运行中，假设边界近似静止。</p>\n<h2 id=\"Good-Decision-Ratio\"><a href=\"#Good-Decision-Ratio\" class=\"headerlink\" title=\"Good Decision Ratio\"></a>Good Decision Ratio</h2><p>为了使算法做出更好的决策，需要一个指标来衡量单个决策的好坏，而端到端的指标，例如字节缺失率，只能反映大量决策的聚合好坏。</p>\n<p>作者发现，# good decisions &#x2F; # total eviction decisions 与字节缺失率密切相关。</p>\n<h1 id=\"Design-of-Learning-Relaxed-Belady-Cache\"><a href=\"#Design-of-Learning-Relaxed-Belady-Cache\" class=\"headerlink\" title=\"Design of Learning Relaxed Belady Cache\"></a>Design of Learning Relaxed Belady Cache</h1><h2 id=\"Past-information\"><a href=\"#Past-information\" class=\"headerlink\" title=\"Past information\"></a>Past information</h2><p>使用更多数据可以提高训练质量，但会带来更多的内存开销。</p>\n<p>对于小缓存，使用尽可能长的滑动窗口包含足够多的最近请求以做出更好的决策；对于大缓存，使用最小二乘法回归线拟合小缓存大小与其最佳滑动窗口大小的关系，从而得到大缓存对应的滑动窗口大小。（在论文后面有说，滑动窗口的长度近似于 Belady 边界，在实现中也是这样做的）</p>\n<h2 id=\"Training-Data\"><a href=\"#Training-Data\" class=\"headerlink\" title=\"Training Data\"></a>Training Data</h2><p>获取训练数据这一部分细节挺多。</p>\n<ol>\n<li>数据的特征在第一次访问时就可以得到，但是数据的标签只有在重用时才能知道；</li>\n<li>需要从请求滑动窗口中随机采样不同对象获取训练数据；<ul>\n<li>不能随机采样请求，否则会导致偏向流行对象；</li>\n<li>不能采样缓存中的对象，否则也会导致偏向流行对象；</li>\n</ul>\n</li>\n<li>只需后向重用距离大于 Belady 边界就可以安全地打标签了。有些对象重用距离太长，持续跟踪会导致过多的内存开销，甚至有些对象永远不会被重用，然而这些对象对于训练的质量起着非常重要的作用，将它们的重用距离设置为 2*Belady边界 的值。</li>\n</ol>\n<h2 id=\"ML-Architecture\"><a href=\"#ML-Architecture\" class=\"headerlink\" title=\"ML Architecture\"></a>ML Architecture</h2><p>三种类型的特征：</p>\n<ol>\n<li>Deltas：delta1 表示对象的后向重用距离，delta2 表示对象最近两次重用之间的距离，… 。Deltas 在启发式中广泛应用，例如 LRU，LRU-K 和 S4LRU；（新近度）</li>\n<li>Exponentially decayed counters (EDCs)：每个 EDC 计数器近似该对象一段时间内的流行度。（频率）</li>\n<li>Static features：对象大小、对象类型</li>\n</ol>\n<p>模型：对比了逻辑回归，线性回归，SVM，轻量级神经网络，发现 GBM 最好，且无需特征归一化，能够有效处理缺失值（Deltas）以及训练和测试效率都很高。</p>\n<p>预测目标：log (time-to-next-request)</p>\n<p>损失函数：L2</p>\n<p>训练集大小：128K。一旦积累了 128K 带标签的数据，就会训练一个新的模型。128K 是决策率向训练时间和开销的权衡。</p>\n<p><strong>以上指标的确定，基本都是实验佐证再加以启发式的直觉解释。</strong></p>\n<h2 id=\"Eviction-Candidate-Selection\"><a href=\"#Eviction-Candidate-Selection\" class=\"headerlink\" title=\"Eviction Candidate Selection\"></a>Eviction Candidate Selection</h2><p>候选集大小为 64，从缓存中随机选择 64 个对象进行批量预测，如果最大值超过 Belady 边界，驱逐它；否则，变成对所有缓存对象进行预测并驱逐预测最大值的对象。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><ol>\n<li><p>与 ATS 生产系统进行比较，LRB 降低了多少网络流量；</p>\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E4%B8%8EATS%E7%9A%84%E7%BC%BA%E5%A4%B1%E7%8E%87%E6%AF%94%E8%BE%83.jpg\" class=\"\" title=\"与ATS的缺失率比较\">\n</li>\n<li><p>与 ATS 相比，LRB 的开销；</p>\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E5%BC%80%E9%94%80.jpg\" class=\"\" title=\"开销\">\n\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E5%85%83%E6%95%B0%E6%8D%AE%E5%8D%A0%E7%BC%93%E5%AD%98%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%AF%94%E4%BE%8B.jpg\" class=\"\" title=\"元数据占缓存大小的比例\">\n</li>\n<li><p>与 SOTA 算法比较</p>\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E4%B8%8ESOTA%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83.jpg\" class=\"\" title=\"与SOTA算法比较\"></li>\n</ol>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"NyxCache（FAST 22）","url":"/2023/04/12/NyxCache%EF%BC%88FAST%2022%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>本文提出了一个用于多租户持久性内存（multi-tenant persistent memory）缓存的访问调节框架，<strong>支持轻量级访问调节、每个缓存的资源使用估算和缓存间干扰分析</strong>。</li>\n<li>有了以上机制和准入控制、容量分配逻辑，本文构建了重要的<strong>共享策略</strong>，例如资源限制、Qos-awareness、公平性和按比例共享：Nyx 的资源限制可以准确地限制每个缓存的 PM 使用量，提供比带宽限制方法 5 倍的性能隔离 ；Nyx QoS 可以为延迟敏感型缓存提供 QoS 保证，同时为不受干扰的 best-effort 缓存提供更高的吞吐量（与以前基于 DRAM 的方法相比，最高可达 6 倍）。</li>\n<li>最后，本文也展示了 Nyx 对于真实负载也是有用的，隔离了写入高峰，并确保重要的缓存不会因为增加的 best-effort 流量而减慢。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>为了提高实用性和简化管理，多个缓存实例通常被合并成一个多租户服务器。然而，多租户服务器很难确保每个客户端缓存都能满足性能目标；目前，一系列内存多租户缓存提供了不同的共享策略，比如对使用的内存容量和带宽进行限制，保证一定的 quality-of-service（Qos），并按比例分配资源。</p>\n<p>由于具有大容量、字节低开销、与 DRAM 性能接近的性质，PM 可以作为多租户的缓存。但是，与 DRAM 不同的是，PM 在读写性能上差别很大（最大读带宽 6.6GB&#x2F;s，最大写带宽 2.3GB&#x2F;s），读和写之间有着严重、不公平的干扰（以 1GB&#x2F;s 的速度写入会导致共同运行的读取工作负载的吞吐量和 P99 延迟减慢，这种程度和 8GB&#x2F;s 的速度读取相同）。</p>\n<p>因此，现有多租户 DRAM 和缓存技术无法适配到 PM 上，于是本文就提出针对多租户 PM 键值缓存的访问调节框架，无需硬件支持就能优化 PM。</p>\n<h1 id=\"Motivation-and-Challenges\"><a href=\"#Motivation-and-Challenges\" class=\"headerlink\" title=\"Motivation and Challenges\"></a>Motivation and Challenges</h1><p>评估先前的多租户缓存以及它们对于 PM 的限制。</p>\n<h1 id=\"NyxCache-Design\"><a href=\"#NyxCache-Design\" class=\"headerlink\" title=\"NyxCache Design\"></a>NyxCache Design</h1><p>NyxCache 的设计。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><p>评估 Nyx 机制的开销和策略的有效性。</p>\n<h1 id=\"Discussion\"><a href=\"#Discussion\" class=\"headerlink\" title=\"Discussion\"></a>Discussion</h1><p>讨论潜在的扩展。</p>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><p>对比相关工作。</p>\n","categories":["Cache"],"tags":["Cache","论文阅读","PM"]},{"title":"Pangu","url":"/2023/12/18/Pangu%EF%BC%88FAST%2023%EF%BC%89/","content":"<p>随着硬件技术（SSD，RDMA 等）和商业模式（从面向大容量到面向高性能）的演进，阿里盘古存储系统也在不断进化，存储服务从 ms 级延迟逐渐进化到高性能、高可靠的 100us 级。</p>\n<p><strong>如何做到的呢？</strong></p>\n<ol>\n<li>在第一阶段，为适应 SSD 和 RDMA（性能瓶颈逐渐从硬件转移到了软件层），创新了文件系统和设计用户态的存储操作系统，从而能够大幅降低 I&#x2F;O 延迟，提供高吞吐量和 IOPS；</li>\n<li>在第二阶段，为适应盘古从面向容量的商业模式往面向性能的转变，提高了基础设施中 SSD 和 100 Gbps 带宽的 RDMA 的数量。同时也引入了大量关键设计：降低流量放大、远程直接缓存访问（RDCA）、CPU 计算卸载，确保盘古充分收获硬件升级带来的性能提升。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>盘古作为一个<strong>统一存储平台</strong>，为阿里和阿里云（比如淘宝，天猫，蚂蚁金服等等）提供了可扩展，高性能和高可靠的存储服务；一些云服务，比如 <strong>EBS，OSS</strong> 等等也是建立在其之上。现如今，盘古已经达到了 <strong>EB 级存储容量</strong>，管理了<strong>万亿个文件</strong>。</p>\n<h2 id=\"盘古1-0：面向容量的存储服务提供者。\"><a href=\"#盘古1-0：面向容量的存储服务提供者。\" class=\"headerlink\" title=\"盘古1.0：面向容量的存储服务提供者。\"></a>盘古1.0：面向容量的存储服务提供者。</h2><p>2009-2015，盘古构建在商用 CPU 和 HDD 之上，提供 ms 级 I&#x2F;O 延迟和 Gbps 级数据中心网络。基于 Ext4 和内核态的 TCP，盘古设计了分布式内核态的文件系统，能够提供多种文件类型（比如临时文件，日志文件和随机访问文件）供不同存储服务所使用。受限于 HDD 和 Gbps 级网络，盘古已经达到性能上限，且云计算早期，客户也更关注大容量。</p>\n<h2 id=\"新硬件需要新设计\"><a href=\"#新硬件需要新设计\" class=\"headerlink\" title=\"新硬件需要新设计\"></a>新硬件需要新设计</h2><p>SSD 和 RDMA 能够提供高性能、低延迟的 I&#x2F;O，但（1）<strong>允许随机访问的文件类型在 SSD 上的性能很差，而顺序操作能够提供更高的吞吐量</strong>；（2）由于数据拷贝和频繁中断，内核软件栈无法紧跟 SSD 和 RDMA 的高 IOPS 和低延迟；（3）<strong>从以服务器为中心的数据中心架构到资源分解（内存分离？）的数据中心架构（学习和总结其中的动机）</strong>，这种范式转变为实现低 I&#x2F;O 延迟带来了额外的挑战。</p>\n<h2 id=\"盘古2-0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱-SSD-和-RDMA。\"><a href=\"#盘古2-0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱-SSD-和-RDMA。\" class=\"headerlink\" title=\"盘古2.0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱 SSD 和 RDMA。\"></a>盘古2.0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱 SSD 和 RDMA。</h2><p>为简化整体系统的开发和管理，为盘古的文件系统（1）设计了仅添加的持久层和<strong>引入了独立的块布局减少文件写入操作的 I&#x2F;O 延迟</strong>；（2）设计了用户态存储操作系统（USSOS），使用 run-to-completion 线程模型实现用户态存储栈和用户态网络栈的高效协作，并且提出了<em><strong>用户态调度机制，有效实现了 CPU 和内存资源分配</strong></em>；（3）在动态环境下，部署了 SLA 保护机制。</p>\n<p>通过以上设计，在第一阶段就实现了<strong>ms 级 P999 I&#x2F;O 延迟</strong>。</p>\n<h2 id=\"盘古2-0第二阶段：通过增加-SSD-和-100Gbps-RDMA-的数量以及突破网络-x2F-内存-x2F-CPU瓶颈，适应性能导向的商业模式\"><a href=\"#盘古2-0第二阶段：通过增加-SSD-和-100Gbps-RDMA-的数量以及突破网络-x2F-内存-x2F-CPU瓶颈，适应性能导向的商业模式\" class=\"headerlink\" title=\"盘古2.0第二阶段：通过增加 SSD 和 100Gbps RDMA 的数量以及突破网络&#x2F;内存&#x2F;CPU瓶颈，适应性能导向的商业模式\"></a>盘古2.0第二阶段：通过增加 SSD 和 100Gbps RDMA 的数量以及突破网络&#x2F;内存&#x2F;CPU瓶颈，适应性能导向的商业模式</h2><p>增加 SSD 数量：盘古每台服务器配备 96TB 的 SSD；将 RDMA 带宽从 25Gbps 升级到 100 Gbps。</p>\n<p>为充分收获这些硬件带来的性能提升，需要打破网络、内存和 CPU 的性能瓶颈。（1）通过减少网络流量放大比例和动态适应流量优先级，优化网络带宽；（2）通过提出<em><strong>远程直接缓存访问（RDCA），应对内存瓶颈</strong></em>；（3）通过消除数据序列化、反序列负担和引入 CPU 等待指令同步超线程，解决 CPU 瓶颈。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><img src=\"/2023/12/18/Pangu%EF%BC%88FAST%2023%EF%BC%89/architecture.jpg\" class=\"\" title=\"architecture\">","categories":["Cache"],"tags":["论文阅读","Storage"]},{"title":"Pond（ASPLOS 23）","url":"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/","content":"<p>内存池化对于公有云有很大吸引力，比如提高 DRAM 利用率；但内存池化可能对性能造成伤害，比如远端内存的访问延时大于本地内存的访问延时。</p>\n<p>Pond 是一个建立在 CXL 标准之上的内存池系统，显著降低 DRAM 成本且满足云性能目标。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1>","categories":["Cache"],"tags":["论文阅读","CXL","内存分离"]},{"title":"TinyLFU (TOS 17)","url":"/2023/07/12/TinyLFU%20(TOS%2017)/","content":"<p>SOTA 缓存准入（高效）。</p>\n<h1 id=\"TinyLFU-Architecture\"><a href=\"#TinyLFU-Architecture\" class=\"headerlink\" title=\"TinyLFU Architecture\"></a>TinyLFU Architecture</h1><h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><p>缓存驱逐策略选择驱逐对象，TinyLFU 将驱逐对象和缺失对象进行比较，以决定是否以驱逐对象为代价准入缺失对象。</p>\n<img src=\"/2023/07/12/TinyLFU%20(TOS%2017)/Overview.jpg\" class=\"\" title=\"Overview\">\n\n<h2 id=\"Challenges\"><a href=\"#Challenges\" class=\"headerlink\" title=\"Challenges\"></a>Challenges</h2><ol>\n<li>如何维护最近请求对象的历史信息，删除久远的历史信息；</li>\n<li>减少内存开销。</li>\n</ol>\n<h2 id=\"Freshness-Mechanism\"><a href=\"#Freshness-Mechanism\" class=\"headerlink\" title=\"Freshness Mechanism\"></a>Freshness Mechanism</h2><p>每次请求发往近似 sketch 时，都会增加一个全局计数器 S。一旦该计数器 S 的值达到样本大小（W），就会触发 reset，将 S 和近似 sketch 中的所有其他计数器除以 2。</p>\n<p><strong>优势</strong></p>\n<ol>\n<li>只会额外增加 Log(W) 个 bits 的空间开销；</li>\n<li>久远的请求信息会指数衰减。</li>\n</ol>\n<h2 id=\"Counting-Bloom-Filter\"><a href=\"#Counting-Bloom-Filter\" class=\"headerlink\" title=\"Counting Bloom Filter\"></a>Counting Bloom Filter</h2><img src=\"/2023/07/12/TinyLFU%20(TOS%2017)/count_bloom_filter.jpg\" class=\"\" title=\"count_bloom_filter\">\n\n<p>增加计数：当请求到来时，通过每个哈希函数映射到各自的索引，将最小计数的对应计数器值+1，如 {2, 2, 5} -&gt; {3, 3, 5}（实际代码还是倾向增加所有计数器）；</p>\n<p>估算计数：通过每个哈希函数映射到各自的索引，取对应计数器中的最小值。</p>\n<h3 id=\"Space-Reduction\"><a href=\"#Space-Reduction\" class=\"headerlink\" title=\"Space Reduction\"></a>Space Reduction</h3><ol>\n<li><p>总请求量每到 W，则会对全局计数器和近似 sketch 中的所有计数器除以 2，那么理论上，每个 sketch 中的计数器需要 Log(W) 个 bits，但如果假设请求均匀，对于容量为 C 的缓存，通过牺牲部分准确性（频率最高到达 W&#x2F;C），则只需 Log(W&#x2F;C) 个 bits。；</p>\n</li>\n<li><p>如果很多对象只访问一次，则它们会占据大量的计数器。因此，放置一个常规布隆计数器 DoorKeeper 在 Counting Bloom Filter 之前，对象到来时，先检查是否在 DoorKeeper 中，如果不在，则插入到 DoorKeeper 中，否则，请求 Counting Bloom Filter。每次 reset 操作也会清空 DoorKeeper。</p>\n<p> （门卫，很形象 (:</p>\n</li>\n</ol>\n<h1 id=\"Extension\"><a href=\"#Extension\" class=\"headerlink\" title=\"Extension\"></a>Extension</h1><p>TinyLFU 无法适应对象大小可变的缓存环境，因此，作者在 TOS 22 上提出了一种大小感知的 TinyLFU 策略（AV-TinyLFU，aggregated victims）。</p>\n<p>当准入一个对象需要驱逐多个对象时，使用分离的替换策略选出多个驱逐对象，计算所有驱逐对象的频率和，将之与请求对象的频率进行比较，如果 freq(req) &gt; freq(victims)，则准入；否则，不准入，且将所有 victims 进行 promote。（<strong>考虑 freq&#x2F;size 是否更好</strong>）</p>\n","categories":["Cache"],"tags":["Cache","论文阅读"]},{"title":"cmake","url":"/2023/05/16/cmake/","content":"","categories":["工具"],"tags":["C/C++","工具"]},{"title":"HDD 和 SSD","url":"/2023/11/10/%E5%AD%98%E5%82%A8%E7%A1%AC%E4%BB%B6/","content":"<h1 id=\"存储硬盘\"><a href=\"#存储硬盘\" class=\"headerlink\" title=\"存储硬盘\"></a>存储硬盘</h1><p>经常傻傻地分不清存储硬盘协议或接口之间的关系和区别，因此在这里记录一下。</p>\n<h2 id=\"HDD-和-SSD\"><a href=\"#HDD-和-SSD\" class=\"headerlink\" title=\"HDD 和 SSD\"></a>HDD 和 SSD</h2><p>常用的非易失性存储包括机械硬盘（Hard Disk Drive，HDD）和固态硬盘（Solid State Drive，SSD），这是按照它们的硬盘结构原理区分的。HDD 将数据存储在盘片上，使用磁头在盘片上进行旋转进行读取和处理数据；SSD 则是用固态电子存储芯片阵列而制成的硬盘。</p>\n<h2 id=\"SATA-和-PCIe\"><a href=\"#SATA-和-PCIe\" class=\"headerlink\" title=\"SATA 和 PCIe\"></a>SATA 和 PCIe</h2><p>SATA 和 PCIe 都是传输接口，可以形象地将它们描述成<strong>马路</strong>。PCIe 的路宽而直，数据传输速度上限更快；SATA 的路窄而弯，数据传输速度上限较低。</p>\n<p>HDD 最高理论速度非常低，使用 SATA 接口就足够了；而在 SSD 流行后，SATA 接口成为了 SSD 速度的瓶颈，此时 PCIe 取代 SATA 成为了 SSD 最新的高带宽接口。</p>\n<h2 id=\"AHCI-和-NVMe\"><a href=\"#AHCI-和-NVMe\" class=\"headerlink\" title=\"AHCI 和 NVMe\"></a>AHCI 和 NVMe</h2><p>AHCI 和 NVMe 都是传输协议，可以形象地将它们描述成<strong>交通规则</strong>。</p>\n<p>AHCI 是针对 SATA 这种窄而弯的 “马路” 定制的 “交通规则”，此时修建了 PCIe 这种宽而直的 “马路” 后，仍然采用 “单行道” 实在太浪费 PCIe 通道了，于是就有了 NVMe，可以让 SSD 更有效地利用 PCIe 的高带宽。</p>\n<img src=\"/2023/11/10/%E5%AD%98%E5%82%A8%E7%A1%AC%E4%BB%B6/AHCI%E5%92%8CNVMe.jpg\" class=\"\" title=\"AHCI和NVMe\">\n\n\n\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><ol>\n<li><a href=\"https://baijiahao.baidu.com/s?id=1616207956596122967&wfr=spider&for=pc\">固态硬盘的PCIE，SATA，M2，NVMe，AHCI分别都指什么？别再搞混了</a></li>\n<li><a href=\"https://www.zhihu.com/question/48972075\">SSD中，SATA、m2、PCIE和NVME各有什么意义呢？</a></li>\n<li><a href=\"https://www.userbenchmark.com/Faq/What-s-the-difference-between-SATA-PCIe-and-NVMe/105\">What’s the difference between SATA, PCIe and NVMe?</a></li>\n</ol>\n","categories":["SSD","HDD"],"tags":["硬盘"]},{"title":"数据处理杂谈","url":"/2023/03/05/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%9D%82%E8%B0%88/","content":"<h1 id=\"数据处理杂谈\"><a href=\"#数据处理杂谈\" class=\"headerlink\" title=\"数据处理杂谈\"></a>数据处理杂谈</h1><h2 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h2><ol>\n<li>对数据集<strong>唯一对象</strong>进行筛选和处理时，可以考虑使用 <code>glib.h</code> 中的 <code>GHashTable</code> 来代替 <code>STL</code> 中的 <code>unordered_map</code>；非常简单易用，例如：<ul>\n<li><code> g_hash_table_new(g_direct_hash, g_direct_equal)</code> 创建  <code>GHashTable</code>；</li>\n<li><code>g_hash_table_contains(obj_table, (gconstpointer)req-&gt;obj_id)</code> 判断 <code>obj_id</code> 是否在哈希表中；</li>\n<li><code>g_hash_table_add(obj_table, (gpointer)req-&gt;obj_id)</code> 将 <code>obj_id</code> 插入哈希表中。</li>\n</ul>\n</li>\n</ol>\n","categories":["数据集"],"tags":["数据集","数据处理"]},{"title":"Hello World","url":"/2022/10/20/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"初识 CXL","url":"/2023/11/14/%E5%88%9D%E8%AF%86CXL/","content":"<h1 id=\"CXL\"><a href=\"#CXL\" class=\"headerlink\" title=\"CXL\"></a>CXL</h1><p>CXL（<strong>Compute Express Link</strong>，计算快速链接）是处理器、加速器、智能网卡和内存设备之间的高速互连且支持缓存一致性的标准。</p>\n<p>CXL 技术保持了 CPU 内存空间和连接设备上的内存之间的一致性，这样就可以实现资源共享（或池化）以提高性能。</p>\n<h2 id=\"CXL-协议和标准\"><a href=\"#CXL-协议和标准\" class=\"headerlink\" title=\"CXL 协议和标准\"></a>CXL 协议和标准</h2><p>CXL 标准通过三种协议支持各种应用场景：CXL.io、CXL.cache 和 CXL.memory。</p>\n<ol>\n<li>CXL.io：该协议在功能上等同于 PCIe 协议。作为基本的通信协议，CXL.io 用途广泛，可满足各种应用场景的需求；</li>\n<li>CXL.cache：此协议专为更具体的应用程序而设计，使加速器能够有效地访问和缓存主机内存，从而优化性能；</li>\n<li>CXL.memory：此协议使主机（如处理器）能够使用 load&#x2F;store 命令访问设备连接的内存。</li>\n</ol>\n<p>这三种协议共同促进了计算设备（例如 CPU 主机和 AI 加速器）之间内存资源的一致共享。从本质上讲，这通过共享内存实现通信来简化编程。</p>\n<h2 id=\"CXL-与-PCIe-的关系\"><a href=\"#CXL-与-PCIe-的关系\" class=\"headerlink\" title=\"CXL 与 PCIe 的关系\"></a>CXL 与 PCIe 的关系</h2><p>CXL 建立在 PCIe 的物理和电气接口之上（CXL1.1 和 CXL2.0 使用 PCIe5.0 物理层，CXL3.0 使用 PCIe6.0 的物理层），允许备用协议使用 PCIe 的物理层。</p>\n<p>支持 CXL 的设备插入插槽后将默认作为 PCIe 设备，而如果主机处理器也支持 CXL，则它们的 CXL 事务协议将得到激活。 </p>\n<h2 id=\"为什么需要-CXL？\"><a href=\"#为什么需要-CXL？\" class=\"headerlink\" title=\"为什么需要 CXL？\"></a>为什么需要 CXL？</h2><ol>\n<li>为提高性能，数据中心使用大量的异构设备，比如从 CPU 卸载专用工作负载到专用加速器上。有了 CXL 后，<strong>CXL 的内存缓存一致性允许在 CPU 和加速器之间共享内存资源，而无需软件层的控制</strong>；</li>\n<li><strong>CXL 支持部署新的内存层</strong>，以弥合主内存和 SSD 存储之间的性能差距；</li>\n<li><strong>CXL 支持内存扩展功能</strong>，可在当今服务器中提供超出直连 DIMM 插槽的额外容量和带宽，通过连接 CXL 的设备向 CPU 主机处理器添加更多内存，提高内存容量敏感型工作负载（如 AI）的性能。</li>\n</ol>\n<h2 id=\"CXL-的特性和优势\"><a href=\"#CXL-的特性和优势\" class=\"headerlink\" title=\"CXL 的特性和优势\"></a>CXL 的特性和优势</h2><ol>\n<li>简化和改进了低延迟连接和内存一致性，从而显著提高了计算性能和效率;</li>\n<li>CXL 内存扩展功能；</li>\n</ol>\n<h2 id=\"哪些设备从-CXL-互连中受益？\"><a href=\"#哪些设备从-CXL-互连中受益？\" class=\"headerlink\" title=\"哪些设备从 CXL 互连中受益？\"></a>哪些设备从 CXL 互连中受益？</h2><ol>\n<li><strong>智能网卡等通常缺少本地内存的加速器</strong>。通过 CXL.io 和 CXL.cache 协议，这些设备可以与主机处理器的 DDR 内存进行通信；</li>\n<li><strong>CPU、GPU、ASIC 和 FPGA 等配备有 DDR 或 HBM 内存的设备</strong>。通过 CXL.io、CXL.cache 和 CXL.memory 协议，CPU 的内存在本地可以供 GPU 等加速器使用，GPU 等加速器的的内存在本地也可以供 CPU 使用，有助于提升异构工作负载；</li>\n<li><strong>内存设备</strong>。通过 CXL.io 和 CXL.memory 协议，内存设备可以被扩展和池化，为主机处理器提供额外的内存容量。</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.rambus.com/blogs/compute-express-link/\">Compute Express Link (CXL): All you need to know</a></li>\n<li></li>\n</ol>\n","categories":["CXL","新硬件"],"tags":["CXL"]},{"title":"测试文章","url":"/2022/10/20/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/","content":"<p>这是一篇测试文章。</p>\n<ul>\n<li>测试在hexo-blog文件夹下运行hexo clean &amp;&amp; hexo deploy是否可以执行成功。√</li>\n<li>测试在hexo-blog文件夹下运行hexo generate &amp;&amp; hexo deploy是否可以执行成功。√</li>\n<li><strong>推荐</strong>：hexo g &amp;&amp; hexo d</li>\n</ul>\n<img src=\"/2022/10/20/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/%E6%98%9F%E5%A4%9C.jpg\" class=\"\" title=\"图片引用方法一\">\n\n","categories":["测试"],"tags":["测试"]},{"title":"牛客网——算法入门（栈）","url":"/2022/11/15/%E7%89%9B%E5%AE%A2%E7%BD%91%E2%80%94%E2%80%94%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%EF%BC%88%E6%A0%88%EF%BC%89/","content":"<h1 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h1><h2 id=\"AB2\"><a href=\"#AB2\" class=\"headerlink\" title=\"AB2\"></a>AB2</h2><h3 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h3><p>输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。</p>\n<ol>\n<li><p>0&lt;&#x3D;pushV.length &#x3D;&#x3D; popV.length &lt;&#x3D;1000</p>\n</li>\n<li><p>-1000&lt;&#x3D;pushV[i]&lt;&#x3D;1000</p>\n</li>\n<li><p>pushV 的所有数字均不相同</p>\n</li>\n</ol>\n<h3 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">输入：[1,2,3,4,5],[4,5,3,2,1]</span><br><span class=\"line\">返回值：true</span><br><span class=\"line\">说明：可以通过push(1)=&gt;push(2)=&gt;push(3)=&gt;push(4)=&gt;pop()=&gt;push(5)=&gt;pop()=&gt;pop()=&gt;pop()=&gt;pop()</span><br><span class=\"line\">这样的顺序得到[4,5,3,2,1]这个序列，返回true</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">输入：[1,2,3,4,5],[4,3,5,1,2]</span><br><span class=\"line\">返回值：false</span><br><span class=\"line\">说明：由于是[1,2,3,4,5]的压入顺序，[4,3,5,1,2]的弹出顺序，要求4，3，5必须在1，2后压入，且1，2不能弹出，但是这样压入的顺序，1又不能在2之前弹出，所以无法形成的，返回false  </span><br></pre></td></tr></table></figure>\n\n<h3 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h3><p>模拟栈操作。</p>\n<ol>\n<li>记popV的第一个元素为p<sub>1</sub>，那么pushV中p<sub>1</sub>之前元素全部顺序入栈，p<sub>1</sub>入栈再出栈；popV的后续元素如果依次等于栈顶元素，持续进行出栈操作，直至不相等或栈为空，记popV此时元素为p<sub>i</sub>。</li>\n<li>pushV中p<sub>1</sub>之后，p<sub>i</sub>之前的元素继续顺序入栈，而p<sub>i</sub>入栈再出栈；popV的后续元素如果依次等于栈顶元素，持续进行出栈操作，直至不相等或栈为空。</li>\n<li>……</li>\n<li>直到pushV中元素全部入栈，此时后续只有一种出栈方式，即栈中元素依次出栈与popV后续元素一一比较。如果某次不相等，则无法以pushV顺序压入，popV顺序弹出。</li>\n</ol>\n<p>代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Solution</span> &#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">IsPopOrder</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; pushV,vector&lt;<span class=\"type\">int</span>&gt; popV)</span> </span>&#123;</span><br><span class=\"line\">        stack&lt;<span class=\"type\">int</span>&gt; s;</span><br><span class=\"line\">        <span class=\"type\">int</span> p_length = pushV.<span class=\"built_in\">size</span>(), v_length = popV.<span class=\"built_in\">size</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(p_length == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"type\">int</span> cur_push_pos = <span class=\"number\">0</span>, cur_pop_pos = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">do</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(pushV[cur_push_pos] != popV[cur_pop_pos])&#123;</span><br><span class=\"line\">                s.<span class=\"built_in\">push</span>(pushV[cur_push_pos++]); <span class=\"comment\">// pushV中popV[cur_pop_pos]之前元素全部入栈</span></span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                cur_push_pos++;</span><br><span class=\"line\">                cur_pop_pos++; <span class=\"comment\">// popV[cur_pop_pos]入栈再出栈</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(!s.<span class=\"built_in\">empty</span>())&#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(s.<span class=\"built_in\">top</span>() == popV[cur_pop_pos])&#123; <span class=\"comment\">// 是否连续出栈操作</span></span><br><span class=\"line\">                    s.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">                    cur_pop_pos++;</span><br><span class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">while</span>(cur_push_pos &lt; p_length);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(!s.<span class=\"built_in\">empty</span>() &amp;&amp; cur_pop_pos &lt; v_length)&#123; <span class=\"comment\">// pushV全部压入后，此时后续只有一种出栈</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span>(s.<span class=\"built_in\">top</span>() == popV[cur_pop_pos])&#123;</span><br><span class=\"line\">                s.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">                cur_pop_pos++;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> s.<span class=\"built_in\">empty</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>","categories":["刷题"],"tags":["算法","数据结构","栈"]},{"title":"论文阅读","url":"/2023/04/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","content":"","categories":["论文阅读"],"tags":["论文阅读"]},{"title":"Redis","url":"/2023/04/20/Redis/","content":"<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"简单动态字符串\"><a href=\"#简单动态字符串\" class=\"headerlink\" title=\"简单动态字符串\"></a>简单动态字符串</h2><p>位于 <code>sds.h</code> 和 <code>sds.c</code> 中。</p>\n<p>C语言用 ‘\\0’ 作为字符串结束符，而如果字符串内容本身就包含 ‘\\0’ 则会被截断，无法做到二进制安全。因此，redis 将<strong>柔性字符数组</strong>包装到 sds 数据结构中，同时使用 len 和 alloc 表示已使用字符数组的长度和预分配长度。</p>\n<p><strong>注</strong>：柔性数组只能放在结构体的末尾，而之所以使用柔性数组而不是字符指针，是因为柔性数组的地址和结构体是连续的，可以方便地通过柔性数组的首地址偏移得到结构体的首地址（<code>buf[-1]</code> 一定是 <code>flags</code> 字段，进一步可以知道所在结构体的大小）。</p>\n<p>为精细地管理字符串，节约内存空间，设置了各个长度范围的字符串结构体。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr5 &#123;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3 个最低有效位表示类型, 同时 5 个最高有效位表示字符串长度 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr8 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> len;         <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> alloc;       <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                            &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr16 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint16_t</span> len;        <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint16_t</span> alloc;      <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                           &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr32 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> len;        <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> alloc;      <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                           &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr64 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint64_t</span> len;        <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint64_t</span> alloc;      <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                           &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"双向链表\"><a href=\"#双向链表\" class=\"headerlink\" title=\"双向链表\"></a>双向链表</h2><p>位于 <code>adlist.h</code> 和 <code>adlist.c</code> 中。</p>\n<p>redis 双向链表的节点可以存储任意类型的 value，因此，每个 list 可以设置自己的 dup、free 和 match 函数，达到多态目的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* 双端链表节点 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">listNode</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向前驱节点的指针 */</span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">listNode</span> *prev;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向后继节点的指针 */</span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">listNode</span> *next;</span><br><span class=\"line\">    <span class=\"comment\">/* void * 指针，指向具体的元素，节点可以是任意类型 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *value;</span><br><span class=\"line\">&#125; listNode;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 双端链表</span></span><br><span class=\"line\"><span class=\"comment\"> * 有记录头尾两节点，支持从链表头部或者尾部进行遍历，是早期列表键 PUSH/POP 实现高效的关键</span></span><br><span class=\"line\"><span class=\"comment\"> * 每个链表节点有记录前驱节点和后继节点的指针，可以使得列表键支持往后或者往前进行遍历</span></span><br><span class=\"line\"><span class=\"comment\"> * 有额外用 len 存储链表长度，O(1) 的时间复杂度获取节点个数，是 LLEN 命令高效的关键 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">list</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向链表头节点的指针，支持从表头开始遍历 */</span></span><br><span class=\"line\">    listNode *head;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向链表尾节点的指针，支持从表尾开始遍历 */</span></span><br><span class=\"line\">    listNode *tail;</span><br><span class=\"line\">    <span class=\"comment\">/* 各种类型的链表可以定义自己的复制函数 / 释放函数 / 比较函数 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *(*dup)(<span class=\"type\">void</span> *ptr);</span><br><span class=\"line\">    <span class=\"built_in\">void</span> (*free)(<span class=\"type\">void</span> *ptr);</span><br><span class=\"line\">    <span class=\"built_in\">int</span> (*match)(<span class=\"type\">void</span> *ptr, <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 链表长度，即链表节点数量，O(1) 时间复杂度获取 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> len;</span><br><span class=\"line\">&#125; list;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"字典\"><a href=\"#字典\" class=\"headerlink\" title=\"字典\"></a>字典</h2><p>位于 <code>dict.h</code> 和 <code>dict.c</code> 中。</p>\n<p>dictEntry 存储键值对；dict 为字典结构，其中使用了两个哈希表，第二个哈希表一般情况下不用，只有在扩容缩容操作时，避免长时间 rehash 导致阻塞，将新申请空间存在第二个哈希表中，rehash 过程中，添加操作往第二个哈希表中进行，查找、删除和修改在两个哈希表中依次进行，除此之外，第一个哈希表中的键值对需要在服务空闲时重新计算哈希桶索引，全部迁移插入到第二个哈希表中；dictType 为字典类型，针对不同类型的键和值配置合适的复制、析构和比较函数等，达到多态目的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">dictEntry</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* void * 类型的 key，可以指向任意类型的键 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *key;</span><br><span class=\"line\">    <span class=\"comment\">/* 联合体 v 中包含了指向实际值的指针 *val、无符号的 64 位整数、有符号的 64 位整数，以及 double 双精度浮点数。</span></span><br><span class=\"line\"><span class=\"comment\">     * 这是一种节省内存的方式，因为当值为整数或者双精度浮点数时，由于它们本身就是 64 位的，void *val 指针也是占用 64 位（64 操作系统下），</span></span><br><span class=\"line\"><span class=\"comment\">     * 所以它们可以直接存在键值对的结构体中，避免再使用一个指针，从而节省内存开销（8 个字节）</span></span><br><span class=\"line\"><span class=\"comment\">     * 当然也可以是 void *，存储任何类型的数据，最早 redis1.0 版本就只是 void* */</span></span><br><span class=\"line\">    <span class=\"keyword\">union</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">void</span> *val;</span><br><span class=\"line\">        <span class=\"type\">uint64_t</span> u64;</span><br><span class=\"line\">        <span class=\"type\">int64_t</span> s64;</span><br><span class=\"line\">        <span class=\"type\">double</span> d;</span><br><span class=\"line\">    &#125; v;</span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">dictEntry</span> *next;     <span class=\"comment\">/* 同一个 hash 桶中的下一个条目.</span></span><br><span class=\"line\"><span class=\"comment\">                                 * 通过形成一个链表解决桶内的哈希冲突. */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *metadata[];           <span class=\"comment\">/* 一块任意长度的数据 (按 void* 的大小对齐),</span></span><br><span class=\"line\"><span class=\"comment\">                                 * 具体长度由 &#x27;dictType&#x27; 中的</span></span><br><span class=\"line\"><span class=\"comment\">                                 * dictEntryMetadataBytes() 返回. */</span></span><br><span class=\"line\">&#125; dictEntry;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 字典类型，因为我们会将字典用在各个地方，例如键空间、过期字典等等等，只要是想用字典（哈希表）的场景都可以用</span></span><br><span class=\"line\"><span class=\"comment\"> * 这样的话每种类型的字典，它对应的 key / value 肯定类型是不一致的，这就需要有一些自定义的方法，例如键值对复制、析构等 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">dictType</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 字典里哈希表的哈希算法，目前使用的是基于 DJB 实现的字符串哈希算法</span></span><br><span class=\"line\"><span class=\"comment\">     * 比较出名的有 siphash，redis 4.0 中引进了它。3.0 之前使用的是 DJBX33A，3.0 - 4.0 使用的是 MurmurHash2 */</span></span><br><span class=\"line\">    <span class=\"built_in\">uint64_t</span> (*hashFunction)(<span class=\"type\">const</span> <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 键拷贝 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *(*keyDup)(dict *d, <span class=\"type\">const</span> <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 值拷贝 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *(*valDup)(dict *d, <span class=\"type\">const</span> <span class=\"type\">void</span> *obj);</span><br><span class=\"line\">    <span class=\"comment\">/* 键比较 */</span></span><br><span class=\"line\">    <span class=\"built_in\">int</span> (*keyCompare)(dict *d, <span class=\"type\">const</span> <span class=\"type\">void</span> *key1, <span class=\"type\">const</span> <span class=\"type\">void</span> *key2);</span><br><span class=\"line\">    <span class=\"comment\">/* 键析构 */</span></span><br><span class=\"line\">    <span class=\"built_in\">void</span> (*keyDestructor)(dict *d, <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 值析构 */</span></span><br><span class=\"line\">    <span class=\"built_in\">void</span> (*valDestructor)(dict *d, <span class=\"type\">void</span> *obj);</span><br><span class=\"line\">    <span class=\"comment\">/* 字典里的哈希表是否允许扩容 */</span></span><br><span class=\"line\">    <span class=\"built_in\">int</span> (*expandAllowed)(<span class=\"type\">size_t</span> moreMem, <span class=\"type\">double</span> usedRatio);</span><br><span class=\"line\">    <span class=\"comment\">/* 允许调用者向条目 (dictEntry) 中添加额外的元信息.</span></span><br><span class=\"line\"><span class=\"comment\">     * 这段额外信息的内存会在条目分配时被零初始化. */</span></span><br><span class=\"line\">    <span class=\"built_in\">size_t</span> (*dictEntryMetadataBytes)(dict *d);</span><br><span class=\"line\">&#125; dictType;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">dict</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 字典类型，8 bytes */</span></span><br><span class=\"line\">    dictType *type;</span><br><span class=\"line\">    <span class=\"comment\">/* 字典中使用了两个哈希表,</span></span><br><span class=\"line\"><span class=\"comment\">     * (看看那些以 &#x27;ht_&#x27; 为前缀的成员, 它们都是一个长度为 2 的数组)</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * 我们可以将它们视为</span></span><br><span class=\"line\"><span class=\"comment\">     * struct&#123;</span></span><br><span class=\"line\"><span class=\"comment\">     *   ht_table[2];</span></span><br><span class=\"line\"><span class=\"comment\">     *   ht_used[2];</span></span><br><span class=\"line\"><span class=\"comment\">     *   ht_size_exp[2];</span></span><br><span class=\"line\"><span class=\"comment\">     * &#125; hash_table[2];</span></span><br><span class=\"line\"><span class=\"comment\">     * 为了优化字典的内存结构,</span></span><br><span class=\"line\"><span class=\"comment\">     * 减少对齐产生的空洞,</span></span><br><span class=\"line\"><span class=\"comment\">     * 我们将这些数据分散于整个结构体中.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * 平时只使用下标为 0 的哈希表.</span></span><br><span class=\"line\"><span class=\"comment\">     * 当需要进行 rehash 时 (&#x27;rehashidx&#x27; != -1),</span></span><br><span class=\"line\"><span class=\"comment\">     * 下标为 1 的一组数据会作为一组新的哈希表,</span></span><br><span class=\"line\"><span class=\"comment\">     * 渐进地进行 rehash 避免一次性 rehash 造成长时间的阻塞.</span></span><br><span class=\"line\"><span class=\"comment\">     * 当 rehash 完成时, 将新的哈希表置入下标为 0 的组别中,</span></span><br><span class=\"line\"><span class=\"comment\">     * 同时将 &#x27;rehashidx&#x27; 置为 -1.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    dictEntry **ht_table[<span class=\"number\">2</span>];</span><br><span class=\"line\">    <span class=\"comment\">/* 哈希表存储的键数量，它与哈希表的大小 size 的比值就是 load factor 负载因子，</span></span><br><span class=\"line\"><span class=\"comment\">     * 值越大说明哈希碰撞的可能性也越大，字典的平均查找效率也越低</span></span><br><span class=\"line\"><span class=\"comment\">     * 理论上负载因子 &lt;=1 的时候，字典能保持平均 O(1) 的时间复杂度查询</span></span><br><span class=\"line\"><span class=\"comment\">     * 当负载因子等于哈希表大小的时候，说明哈希表退化成链表了，此时查询的时间复杂度退化为 O(N)</span></span><br><span class=\"line\"><span class=\"comment\">     * redis 会监控字典的负载因子，在负载因子变大的时候，会对哈希表进行扩容，后面会提到的渐进式 rehash */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> ht_used[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">long</span> rehashidx; <span class=\"comment\">/* rehashing not in progress if rehashidx == -1 */</span></span><br><span class=\"line\">                    <span class=\"comment\">/* rehash 的进度.</span></span><br><span class=\"line\"><span class=\"comment\">                     * 如果此变量值为 -1, 则当前未进行 rehash. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 将小尺寸的变量置于结构体的尾部, 减少对齐产生的额外空间开销. */</span></span><br><span class=\"line\">    <span class=\"type\">int16_t</span> pauserehash; <span class=\"comment\">/* If &gt;0 rehashing is paused (&lt;0 indicates coding error) */</span></span><br><span class=\"line\">                         <span class=\"comment\">/* 如果此变量值 &gt;0 表示 rehash 暂停</span></span><br><span class=\"line\"><span class=\"comment\">                          * (&lt;0 表示编写的代码出错了). */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储哈希表大小的指数表示，通过这个可以直接计算出哈希表的大小，例如 exp = 10, size = 2 ** 10</span></span><br><span class=\"line\"><span class=\"comment\">     * 能避免说直接存储 size 的实际值，以前 8 字节存储的数值现在变成 1 字节进行存储 */</span></span><br><span class=\"line\">    <span class=\"type\">signed</span> <span class=\"type\">char</span> ht_size_exp[<span class=\"number\">2</span>]; <span class=\"comment\">/* exponent of size. (size = 1&lt;&lt;exp) */</span></span><br><span class=\"line\">                                <span class=\"comment\">/* 哈希表大小的指数表示.</span></span><br><span class=\"line\"><span class=\"comment\">                                 * (以 2 为底, 大小 = 1 &lt;&lt; 指数) */</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"跳跃表\"><a href=\"#跳跃表\" class=\"headerlink\" title=\"跳跃表\"></a>跳跃表</h2><p><code>server.h</code> 中的 <code>zskiplist</code> 结构和 <code>zskiplistNode</code> 结构，<code>t_zset.c</code> 中以 <code>zsl</code> 开头的函数。</p>\n<img src=\"/2023/04/20/Redis/%E8%B7%B3%E8%B7%83%E8%A1%A8.jpg\" class=\"\" title=\"跳跃表\">\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> &#123;</span><br><span class=\"line\">    sds ele;\t\t\t\t\t       <span class=\"comment\">// 存储字符串类型数据</span></span><br><span class=\"line\">    <span class=\"type\">double</span> score;                      <span class=\"comment\">// 优先分数 </span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> *backward;    <span class=\"comment\">// 指向当前节点最底层的前一个节点</span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistLevel</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> *forward; <span class=\"comment\">// 指向本层下一节点，尾节点指向 NULL</span></span><br><span class=\"line\">        <span class=\"type\">unsigned</span> <span class=\"type\">long</span> span;            <span class=\"comment\">// forward 指向的节点和本节点之间的元素个数，span 值越大，跳过的节点个数越多</span></span><br><span class=\"line\">    &#125; level[]; \t\t\t\t\t\t   <span class=\"comment\">// 柔性数组，每个节点的数组长度不一样</span></span><br><span class=\"line\">&#125; zskiplistNode;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplist</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> *header, *tail;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> length;  <span class=\"comment\">// 跳跃表节点个数（等于元素总数，不包括头节点）</span></span><br><span class=\"line\">    <span class=\"type\">int</span> level;             <span class=\"comment\">// 跳跃表高度（除头节点外，层数最多的节点的层高）</span></span><br><span class=\"line\">&#125; zskiplist;</span><br></pre></td></tr></table></figure>\n\n<p>跳跃表的头节点高度为 <code>MAXLEVEL</code>，其它节点创建时随机设置高度，高度越高概率越小。</p>\n<p>重点是 <code>update</code> 和 <code>rank</code> 数组的理解。</p>\n<p><code>update[]</code>：记录每层比插入元素 score 或字典序小的最近节点；</p>\n<p><code>rank[]</code>：记录插入位置在每层中从头节点跨越了多少个节点。</p>\n<h2 id=\"整数集合\"><a href=\"#整数集合\" class=\"headerlink\" title=\"整数集合\"></a>整数集合</h2><p>位于 <code>intset.h</code> 和 <code>intset.c</code> 中。</p>\n<p>整数集合中的元素<strong>从小到大</strong>排列在数组中。为节省空间，有三种底层数组类型，随着插入整数的编码提升，提升整数集合底层数组的类型。（插入和删除操作效率比较低，涉及 <code>memmove</code>）</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* 整数集合 </span></span><br><span class=\"line\"><span class=\"comment\"> * 记录不包含重复元素的各个整数(由小到大的顺序) </span></span><br><span class=\"line\"><span class=\"comment\"> * 底层数组默认是 int16_t 类型, 可能随着新增元素的大小升级至 int32_t 或 int64_t 类型*/</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">intset</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 编码, 记录整数集合底层数组(contents)的类型*/</span></span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> encoding;</span><br><span class=\"line\">    <span class=\"comment\">/* 记录整数集合包含的元素个数 */</span></span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> length;</span><br><span class=\"line\">    <span class=\"comment\">/* 整数集合的底层实现, 虽声明为 int8_t 类型,但真正的类型取决于 encoding */</span></span><br><span class=\"line\">    <span class=\"type\">int8_t</span> contents[];</span><br><span class=\"line\">&#125; intset;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"压缩列表\"><a href=\"#压缩列表\" class=\"headerlink\" title=\"压缩列表\"></a>压缩列表</h2><p>位于 <code>ziplist.h</code> 和 <code>ziplist.c</code> 中。</p>\n<p>压缩列表适合存储小整数以及短字符串，并且元素数量较少。</p>\n<img src=\"/2023/04/20/Redis/%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8.jpg\" class=\"\" title=\"压缩列表\">\n\n<p>压缩列表中的 entry 在内存中有三个字段，previous_entry_length 字段表示<strong>前一个元素</strong>的字节长度，占1个或者5个字节，当前一个元素的长度小于254字节时，用1个字节表示；当前一个元素的长度大于或等于254字节时，用5个字节来表示。而此时 previous_entry_length 字段的第1个字节是固定的0xFE，后面4个字节才真正表示前一个元素的长度。假设已知当前元素的首地址为p，那么 p-previous_entry_length 就是前一个元素的首地址，从而实现压缩列表从尾到头的遍历。</p>\n<img src=\"/2023/04/20/Redis/%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8entry.jpg\" class=\"\" title=\"压缩列表entry\">\n\n<p>encoding 字段同样长度可变，为1字节、2字节或者5字节，用来表示<strong>当前元素</strong>的数据和字节长度；content 字段则是存储的实际内容。</p>\n<img src=\"/2023/04/20/Redis/%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8%E5%85%83%E7%B4%A0%E7%9A%84%E7%BC%96%E7%A0%81.jpg\" class=\"\" title=\"压缩列表元素的编码\">\n\n<p>压缩列表中的 entry 被解码后保存在 zlentry 结构体中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* 这是一个很关键的结构体，将 ziplist 节点信息填充成一个 zlentry 结构体，方便后面进行函数操作</span></span><br><span class=\"line\"><span class=\"comment\"> * 需要注意这并不是一个 ziplist 节点在内存中实际的编码布局，只是为了方便我们使用</span></span><br><span class=\"line\"><span class=\"comment\"> * */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">zlentry</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 存储下面 prevrawlen 所需要的字节数 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> prevrawlensize; <span class=\"comment\">/* Bytes used to encode the previous entry len*/</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储前一个节点的字节长度 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> prevrawlen;     <span class=\"comment\">/* Previous entry len. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储下面 len 所需要的字节数 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> lensize;        <span class=\"comment\">/* Bytes used to encode this entry type/len.</span></span><br><span class=\"line\"><span class=\"comment\">                                    For example strings have a 1, 2 or 5 bytes</span></span><br><span class=\"line\"><span class=\"comment\">                                    header. Integers always use a single byte.*/</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储当前节点的字节长度 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> len;            <span class=\"comment\">/* Bytes used to represent the actual entry.</span></span><br><span class=\"line\"><span class=\"comment\">                                    For strings this is just the string length</span></span><br><span class=\"line\"><span class=\"comment\">                                    while for integers it is 1, 2, 3, 4, 8 or</span></span><br><span class=\"line\"><span class=\"comment\">                                    0 (for 4 bit immediate) depending on the</span></span><br><span class=\"line\"><span class=\"comment\">                                    number range. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* prevrawlensize + lensize 当前节点的头部字节，</span></span><br><span class=\"line\"><span class=\"comment\">     * 其实是 prevlen + encoding 两项占用的字节数 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> headersize;     <span class=\"comment\">/* prevrawlensize + lensize. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储当前节点的数据编码格式 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> encoding;      <span class=\"comment\">/* Set to ZIP_STR_* or ZIP_INT_* depending on</span></span><br><span class=\"line\"><span class=\"comment\">                                    the entry encoding. However for 4 bits</span></span><br><span class=\"line\"><span class=\"comment\">                                    immediate integers this can assume a range</span></span><br><span class=\"line\"><span class=\"comment\">                                    of values and must be range-checked. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 指向当前节点开头第一个字节的指针 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> *p;            <span class=\"comment\">/* Pointer to the very start of the entry, that</span></span><br><span class=\"line\"><span class=\"comment\">                                    is, this points to prev-entry-len field. */</span></span><br><span class=\"line\">&#125; zlentry;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"快速列表\"><a href=\"#快速列表\" class=\"headerlink\" title=\"快速列表\"></a>快速列表</h2><p>位于 <code>quicklist.h</code> 和 <code>quicklist.c</code> 中。</p>\n<p>quicklist 是一个双向链表，链表中的每个节点是一个压缩列表。</p>\n<h1 id=\"数据对象\"><a href=\"#数据对象\" class=\"headerlink\" title=\"数据对象\"></a>数据对象</h1><p>Redis 基于上述数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象，每种对象都用到了至少一种我们前面所介绍的数据结构。</p>\n<p>位于 <code>server.h</code> 中的 <code>redisObject</code> 数据结构（robj）的 type 字段表示对象类型；encoding 字段表示底层数据存储结构（针对某一种类型对象，redis 可能会根据情况采用不同的数据结构存储）；ptr 指向数据所在的底层数据结构的存储位置。对象也可能使用多种数据结构存储，比如有序集合可以采用字典和跳跃表同时存储，分别利用两者的单点查询和范围查询优势，由于数据都是用指针存储，因此额外开销只是相关数据结构的 header 相关字段。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* The actual Redis Object */</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_STRING 0    <span class=\"comment\">/* String object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_LIST 1      <span class=\"comment\">/* List object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_SET 2       <span class=\"comment\">/* Set object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ZSET 3      <span class=\"comment\">/* Sorted set object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_HASH 4      <span class=\"comment\">/* Hash object. */</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* Objects encoding. Some kind of objects like Strings and Hashes can be</span></span><br><span class=\"line\"><span class=\"comment\"> * internally represented in multiple ways. The &#x27;encoding&#x27; field of the object</span></span><br><span class=\"line\"><span class=\"comment\"> * is set to one of this fields for this object. */</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_RAW 0     <span class=\"comment\">/* Raw representation */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_INT 1     <span class=\"comment\">/* Encoded as integer */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_HT 2      <span class=\"comment\">/* Encoded as hash table */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_ZIPMAP 3  <span class=\"comment\">/* No longer used: old hash encoding. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_LINKEDLIST 4 <span class=\"comment\">/* No longer used: old list encoding. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_ZIPLIST 5 <span class=\"comment\">/* No longer used: old list/hash/zset encoding. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_INTSET 6  <span class=\"comment\">/* Encoded as intset */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_SKIPLIST 7  <span class=\"comment\">/* Encoded as skiplist */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_EMBSTR 8  <span class=\"comment\">/* Embedded sds string encoding */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_QUICKLIST 9 <span class=\"comment\">/* Encoded as linked list of listpacks */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_STREAM 10 <span class=\"comment\">/* Encoded as a radix tree of listpacks */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_LISTPACK 11 <span class=\"comment\">/* Encoded as a listpack */</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">redisObject</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> type:<span class=\"number\">4</span>;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> encoding:<span class=\"number\">4</span>;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> lru:LRU_BITS; <span class=\"comment\">/* LRU time (relative to global lru_clock) or</span></span><br><span class=\"line\"><span class=\"comment\">                            * LFU data (least significant 8 bits frequency</span></span><br><span class=\"line\"><span class=\"comment\">                            * and most significant 16 bits access time). */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> refcount;</span><br><span class=\"line\">    <span class=\"type\">void</span> *ptr;</span><br><span class=\"line\">&#125; robj;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Redis-处理客户端命令\"><a href=\"#Redis-处理客户端命令\" class=\"headerlink\" title=\"Redis 处理客户端命令\"></a>Redis 处理客户端命令</h1><h2 id=\"相关数据结构\"><a href=\"#相关数据结构\" class=\"headerlink\" title=\"相关数据结构\"></a>相关数据结构</h2><p>客户端数据结构，位于 <code>server.h</code> 中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">client</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">uint64_t</span> id;            <span class=\"comment\">/* 客户端唯一 id */</span></span><br><span class=\"line\">    redisDb *db;            <span class=\"comment\">/* 指向当前选择的数据库 */</span></span><br><span class=\"line\">    robj *name;             <span class=\"comment\">/* 客户端名称 */</span></span><br><span class=\"line\">    sds querybuf;           <span class=\"comment\">/* 输入缓冲区，recv 函数接收的客户端命令请求暂时缓存在此处 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> argc;               <span class=\"comment\">/* 命令请求的参数个数 */</span></span><br><span class=\"line\">    robj **argv;            <span class=\"comment\">/* 命令请求的参数内容依次解析到此处 */</span></span><br><span class=\"line\"></span><br><span class=\"line\">    list *reply;            <span class=\"comment\">/* 存储待返回给客户端的命令回复数据 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> <span class=\"type\">long</span> reply_bytes; <span class=\"comment\">/* reply 列表所有节点的存储空间之和 */</span></span><br><span class=\"line\">    <span class=\"type\">size_t</span> sentlen;         <span class=\"comment\">/* 已返回给客户端的字节数 */</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">time_t</span> lastinteraction; <span class=\"comment\">/* 客户端上次与服务器交互时间，用于客户端的超时处理 */</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/* Response buffer */</span></span><br><span class=\"line\">    <span class=\"type\">size_t</span> buf_peak; <span class=\"comment\">/* Peak used size of buffer in last 5 sec interval. */</span></span><br><span class=\"line\">    <span class=\"type\">mstime_t</span> buf_peak_last_reset_time; <span class=\"comment\">/* keeps the last time the buffer peak value was reset */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> bufpos;</span><br><span class=\"line\">    <span class=\"type\">size_t</span> buf_usable_size; <span class=\"comment\">/* Usable size of buffer. */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> *buf;</span><br><span class=\"line\">    </span><br><span class=\"line\">    ......</span><br><span class=\"line\">&#125; client;</span><br></pre></td></tr></table></figure>\n\n<p>服务端数据结构，位于 <code>server.h</code> 中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">redisServer</span>&#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> *configfile; \t\t\t\t\t\t<span class=\"comment\">/* 配置文件绝对路径 */</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> dbnum; \t\t\t\t\t\t\t\t<span class=\"comment\">/* 数据库的数目 */</span></span><br><span class=\"line\">\tredisDb *db; \t\t\t\t\t\t\t<span class=\"comment\">/* 数据库数组 */</span></span><br><span class=\"line\">    dict *commands; \t\t\t\t\t\t<span class=\"comment\">/* 命令字典，放置命令名称到命令对象的映射 */</span></span><br><span class=\"line\">    aeEventLoop *el; \t\t\t\t\t\t<span class=\"comment\">/* 事件循环 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> port; \t\t\t\t\t\t\t\t<span class=\"comment\">/* 服务器监听端口号，可通过参数 port 配置，默认为 6379 */</span></span><br><span class=\"line\">\t<span class=\"type\">char</span> * bindaddr[CONFIG_BINDADDR_MAX]; \t<span class=\"comment\">/* 绑定的所有 IP */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> bindaddr_count; \t\t\t\t\t<span class=\"comment\">/* 用户配置的 IP 地址数目 */</span></span><br><span class=\"line\">    socketFds ipfd; \t\t\t\t\t\t<span class=\"comment\">/* 存储所有 IP 地址创建的 socket 文件描述符 */</span></span><br><span class=\"line\">\tlist *clients; \t\t\t\t\t\t\t<span class=\"comment\">/* 当前连接的所有客户端 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> maxidletime; \t\t\t\t\t\t<span class=\"comment\">/* 最大空闲时间 */</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    ......</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>命令数据结构，位于 <code>server.h</code>  中。Redis 支持的所有命令都在全局变量 <code>struct redisCommand redisCommandTable[]</code> 中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">redisCommand</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> *declared_name;\t\t\t<span class=\"comment\">/* 命令名称 */</span></span><br><span class=\"line\">\tredisCommandProc *proc; \t\t<span class=\"comment\">/* 命令处理函数 */</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> arity;\t\t\t\t\t\t<span class=\"comment\">/* 命令参数数目（-N 表示参数数目必须 &gt;= N；N 表示参数数目必须 = N） */</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> flags;\t\t\t\t\t\t<span class=\"comment\">/* 命令的二进制标志 */</span></span><br><span class=\"line\">\t<span class=\"type\">long</span> <span class=\"type\">long</span> microseconds, calls;\t<span class=\"comment\">/* 服务器启动至今该命令总执行时间；该命令总执行次数。用于统计 */</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>当服务端收到一个命令请求时，如果到 redisCommandTable 中查找命令，耗时太高。因此，Redis 在服务器初始时，将命令表转化为一个字典。</p>\n<p>事件循环数据结构，位于 <code>ae.h</code> 中。文件事件（socket 的可读可写时间）和时间事件（定时任务）都封装在该数据结构中</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* State of an event based program */</span></span><br><span class=\"line\"><span class=\"comment\">/* 事件处理器的状态 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">aeEventLoop</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 目前已经注册的最大文件描述符 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> maxfd;   <span class=\"comment\">/* highest file descriptor currently registered */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 目前追踪的最大文件描述符数量，redis 初始化的时候会设置好的一个固定值</span></span><br><span class=\"line\"><span class=\"comment\">     * 初始化事件循环器的时候会把这个值赋值给 setsize，见 server.h 文件的 CONFIG_FDSET_INCR</span></span><br><span class=\"line\"><span class=\"comment\">     * server.maxclients + RESERVED_FDS（32） + 96 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> setsize; <span class=\"comment\">/* max number of file descriptors tracked */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 用于生成时间事件的 ID */</span></span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"type\">long</span> timeEventNextId;</span><br><span class=\"line\">    <span class=\"comment\">/* 已注册的文件事件，在初始化的时候会初始化 setsize 个位置 */</span></span><br><span class=\"line\">    aeFileEvent *events; <span class=\"comment\">/* Registered events */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 已就绪的文件事件 */</span></span><br><span class=\"line\">    aeFiredEvent *fired; <span class=\"comment\">/* Fired events */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 时间事件链表的头节点 */</span></span><br><span class=\"line\">    aeTimeEvent *timeEventHead;</span><br><span class=\"line\">    <span class=\"comment\">/* 事件处理器的开关 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> stop;</span><br><span class=\"line\">    <span class=\"comment\">/* 多路复用库的私有数据 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *apidata; <span class=\"comment\">/* This is used for polling API specific data */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 在处理事件前要执行的函数 */</span></span><br><span class=\"line\">    aeBeforeSleepProc *beforesleep;</span><br><span class=\"line\">    <span class=\"comment\">/* 在处理事件后要执行的函数 */</span></span><br><span class=\"line\">    aeBeforeSleepProc *aftersleep;</span><br><span class=\"line\">    <span class=\"type\">int</span> flags;</span><br><span class=\"line\">&#125; aeEventLoop;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"服务端启动\"><a href=\"#服务端启动\" class=\"headerlink\" title=\"服务端启动\"></a>服务端启动</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><ol>\n<li><p>初始化配置，包括用户可配置的参数，以及命令表数组初始化成字典；</p>\n<p> 位于 <code>server.c</code> 中的 <code>initServerConfig</code> 函数执行。</p>\n</li>\n<li><p>加载并解析配置文件；</p>\n<p> 位于 <code>config.c</code> 中的 <code>loadServerConfig</code> 函数执行。</p>\n</li>\n<li><p>初始化服务器内部变量，比如数据库、全局变量和共享对象等；</p>\n<p> 位于 <code>server.c</code> 中的 <code>initServer</code> 函数执行。</p>\n</li>\n<li><p>创建事件循环 eventLoop，分配结构体所需内存，创建 epoll；</p>\n<p> 位于 <code>ae.c</code> 中的 <code>aeCreateEventLoop</code> 函数执行。</p>\n</li>\n</ol>\n<h3 id=\"启动监听\"><a href=\"#启动监听\" class=\"headerlink\" title=\"启动监听\"></a>启动监听</h3><ol>\n<li><p>创建 socket 并启动监听；</p>\n<p> 位于 <code>server.c</code> 中的 <code>listenToPort</code> 函数执行。</p>\n</li>\n<li><p>创建文件事件和时间事件；</p>\n</li>\n<li><p>开启事件循环；</p>\n<p> 位于 <code>ae.c</code> 中的 <code>aeMain</code> 函数执行。</p>\n</li>\n</ol>\n<h2 id=\"命令处理\"><a href=\"#命令处理\" class=\"headerlink\" title=\"命令处理\"></a>命令处理</h2><p>服务端启动完成后，只需要等待客户端连接并发送命令请求即可。</p>\n<h3 id=\"命令解析\"><a href=\"#命令解析\" class=\"headerlink\" title=\"命令解析\"></a>命令解析</h3><p>命令请求首先通过 <code>readQueryFromClient</code> 函数处理后存储在客户端的 <code>querybuf</code> 中，并调用 <code>processInputBuffer</code> 函数解析命令请求，将参数个数和各个参数存储在客户端的 <code>argc</code> 和 <code>argv</code> 中。</p>\n<h3 id=\"命令调用\"><a href=\"#命令调用\" class=\"headerlink\" title=\"命令调用\"></a>命令调用</h3><p>解析完成后，会调用 <code>processCommand</code> 函数处理命令请求，包括一系列的校验以及最后调用 <code>call</code> 函数，通过 <code>c-&gt;cmd-&gt;proc(c)</code> 执行命令。**(后续如果想要读命令的执行细节，就只需要关注这一块就可以了)**</p>\n<h3 id=\"返回结果\"><a href=\"#返回结果\" class=\"headerlink\" title=\"返回结果\"></a>返回结果</h3><p>结果保存在客户端的 <code>reply</code> 或 <code>buf</code> 中。</p>\n","categories":["Redis"],"tags":["Redis","源码分析"]},{"title":"cache replacement policies","url":"/2023/04/01/cache_replace_policies/","content":"<p>Abstract：</p>\n<ol>\n<li>本书总结了 CPU 数据缓存的缓存替换策略概况；</li>\n<li>重点讨论算法，因此作者及那个以前的策略分为两大类——粗粒度和细粒度，每类又分为三个子类，以描述解决缓存替换问题的不同方法和每个类别中重要工作的总结；</li>\n<li>探索更多的评价指标，包括不单单局限于缓存缺失率指标、针对多喝设置定制的解决方案、考虑和预取之间的影响、考虑新型内存；</li>\n<li>最后，本书讨论未来工作的趋势和挑战。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li><strong>why cache is important？</strong> 如今传输数据的延迟比执行一条指令的延迟还长，而缓存的存在，可以减少内存延迟和减少内存流量。</li>\n<li>启发式——考虑访问频率、新近度；最近的预测技术。</li>\n<li>如何将缓存替换策略和死块预测结合起来。</li>\n</ol>\n<h1 id=\"A-Taxonomy-of-Cache-Replacement-Policies\"><a href=\"#A-Taxonomy-of-Cache-Replacement-Policies\" class=\"headerlink\" title=\"A Taxonomy of Cache Replacement Policies\"></a>A Taxonomy of Cache Replacement Policies</h1><ol>\n<li><strong>本书为什么这样分类？</strong><ul>\n<li>建立在缓存替换策略解决预测问题的观察之上，其目标是预测是否应允许任何给定对象保留在缓存中；</li>\n<li>预测的决策发生在缓存块中的很多地方，从缓存块插入缓存开始，到它从缓存中淘汰；</li>\n</ul>\n</li>\n<li><strong>本书对缓存替换算法如何分类？</strong><ul>\n<li>首先根据插入决策的粒度分为两类：第一类是<strong>粗粒度</strong>策略，对所有插入对象进行相同的处理，并且仅依据它们在缓存中驻留的表现进行区别对待，例如，对象在缓存中被重用次数增加时优先级会提高；第二类是<strong>细粒度</strong>策略，除了在缓存中驻留的表现会影响优先级外，不同对象在插入时就不一样，这依靠缓存访问模式的历史信息，比如，细粒度策略了解到某个特定指令加载的对象在过去没有被重用就淘汰了，它就会给它在插入时更低的优先级。</li>\n</ul>\n</li>\n<li>缓存替换策略包括：<ul>\n<li>插入策略：How does the replacement policy initialize the replacement state of a new line when it is inserted into the cache?</li>\n<li>提升策略：How does the replacement policy update the replacement state of a line when it hits in the cache?</li>\n<li>衰减策略：How does the replacement policy update the replacement state of a line when a competing line is inserted or promoted?</li>\n<li>驱逐策略：which line does the replacement policy evict to hold new line?</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Coarse-Grained-Policies\"><a href=\"#Coarse-Grained-Policies\" class=\"headerlink\" title=\"Coarse-Grained Policies\"></a>Coarse-Grained Policies</h2><p>粗粒度策略可以认为无“插入策略”，它就区分驻留对象优先级的方式又分为三类：新近度、频率、根据工作负载的变化动态选择不同粗粒度策略的混合策略。</p>\n<h2 id=\"Fine-Grained-Policies\"><a href=\"#Fine-Grained-Policies\" class=\"headerlink\" title=\"Fine-Grained Policies\"></a>Fine-Grained Policies</h2><p>细粒度策略就对象插入时的区分方式分为两类：基于分类（对对象分为缓存友好和缓存不友好两种）、基于重用距离（尝试去预测对象的重用距离）</p>\n<ol>\n<li>分类：通常被认为是最先进的，因为（1）可以利用过去的访问历史为未来做出更正确的决策，（2）可以适应各种缓存访问模式。</li>\n<li>重用距离预测：使用历史信息进行重用距离预测有好有坏，在 4.1 节中进行讨论。</li>\n</ol>\n<h2 id=\"Design-Considerations\"><a href=\"#Design-Considerations\" class=\"headerlink\" title=\"Design Considerations\"></a>Design Considerations</h2><p>缓存替换策略的只要目标是提升缓存命中率，以下这些设计因素有助于实现更高的命中率：</p>\n<ol>\n<li>粒度：</li>\n<li>历史信息：替换策略在做决策时使用了多少历史信息；</li>\n<li>访问模式：替换策略是否适应工作负载。</li>\n</ol>\n<p>在大体趋势上，从左到右是更近时候提出的，使用了更长历史记录并且可以适应更多工作负载的算法，可以看到，细粒度预测非常有优势，它提供了：</p>\n<ol>\n<li>允许细粒度策略仅将缓存空间专用于缓存友好对象；</li>\n<li>允许细粒度策略的每个缓存组动态适应工作负载的变化。</li>\n</ol>\n<img src=\"/2023/04/01/cache_replace_policies/%E6%9B%BF%E6%8D%A2%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB.jpg\" class=\"\" title=\"替换算法分类\">\n\n\n\n<h1 id=\"Coarse-Grained-Replacement-Policies\"><a href=\"#Coarse-Grained-Replacement-Policies\" class=\"headerlink\" title=\"Coarse-Grained Replacement Policies\"></a>Coarse-Grained Replacement Policies</h1><p> 每个对象仅和少量替换状态相关，所有新插入对象被统一初始化，然后在缓存命中时进行简单的提升操作。</p>\n<h2 id=\"Recency-Based-Policies\"><a href=\"#Recency-Based-Policies\" class=\"headerlink\" title=\"Recency-Based Policies\"></a>Recency-Based Policies</h2><ol>\n<li>LRU：无法适应搅拌和扫描工作负载；</li>\n<li>LRU变种：<ul>\n<li>MRU：仅能适应搅拌工作负载；难以适应工作负载模式的变化，因为新来的工作集很难被持续缓存；</li>\n<li>EELRU：主要思想是检测工作集大小是否超过缓存大小，当工作集小于缓存容量时，会驱逐 LRU 端对象，当工作集大于缓存容量时，则驱逐距离 MRU 位置长度为 e 的对象；</li>\n<li>Seg-LRU</li>\n<li>LIP：和 MRU 其实一个道理，但它通过改变插入策略而使得 LIP 的驱逐策略和 LRU 一致，这启发我们在设计 LRU 变体算法时，只需要修改插入和提升操作，驱逐操作保持不变即可；</li>\n<li>BIP：以较大概率插入 LRU 端，较低概率插入 MRU 端；</li>\n<li>SRRIP、BRRIP、PDP、GIPPR</li>\n</ul>\n</li>\n</ol>\n<p>可以看到 LRU 家族很多都在考虑适应搅拌工作负载，因为 CPU 缓存容量小于工作集是较易发生的，但这在内存缓存或者 CDN 缓存中似乎很难出现。</p>\n<h2 id=\"Frequency-Based-Policies\"><a href=\"#Frequency-Based-Policies\" class=\"headerlink\" title=\"Frequency-Based Policies\"></a>Frequency-Based Policies</h2><p>基于频率的策略可以一定程度上抵抗扫描工作负载。</p>\n<ol>\n<li>LFU：容易造成缓存污染；</li>\n<li>Frequency-Based Replacement（FBR）：靠近 MRU 端的对象访问频数不会增加，以避免短时间的高频访问拔高对象优先级；</li>\n<li>Least Recently&#x2F;Frequently Used（LRFU）</li>\n</ol>\n<h2 id=\"Hybrid-Policies\"><a href=\"#Hybrid-Policies\" class=\"headerlink\" title=\"Hybrid Policies\"></a>Hybrid Policies</h2><p>混合策略能够适应工作负载的变化。它面临两个挑战：（1）如何准确确定哪个策略是最有利的；（2）以较低的硬件成本管理多个策略。</p>\n<ol>\n<li>ARC</li>\n<li>DIP</li>\n<li>DRRIP</li>\n</ol>\n<h1 id=\"Fine-Grained-Replacement-Policies\"><a href=\"#Fine-Grained-Replacement-Policies\" class=\"headerlink\" title=\"Fine-Grained Replacement Policies\"></a>Fine-Grained Replacement Policies</h1><p>细粒度策略在插入时就会根据不同对象的历史信息进行区分，以决定它们的插入位置。例如，如果对象上一次加入缓存后没有被重用就被驱逐了，当再次请求该对象时，就可以把它插入到较低的位置。</p>\n<p>细粒度策略基于预测插入优先级的指标分为三类：预测重用距离、预测是否被重用、引入的新预测指标。</p>\n<h2 id=\"Reuse-Distance-Prediction-Policies\"><a href=\"#Reuse-Distance-Prediction-Policies\" class=\"headerlink\" title=\"Reuse Distance Prediction Policies\"></a>Reuse Distance Prediction Policies</h2><p>重用距离的具体值很难被准确预测，因此，更现实的解决方案是估计重用距离分布或其它聚合重用距离统计。</p>\n<h3 id=\"Expiration-Based-Dead-Block-Predictors\"><a href=\"#Expiration-Based-Dead-Block-Predictors\" class=\"headerlink\" title=\"Expiration-Based Dead Block Predictors\"></a>Expiration-Based Dead Block Predictors</h3><p>许多死块预测器使用过去的实际后向重用距离来估计未来的前向重用距离，并驱逐没有在预期重用距离内被重用的对象；</p>\n<p>还有一些使用过去的实际生命周期预测未来的生命周期，当在预测的两倍生命周期时间内没有被重用，该块就从缓存中淘汰。</p>\n<h3 id=\"Reuse-Distance-Ordering\"><a href=\"#Reuse-Distance-Ordering\" class=\"headerlink\" title=\"Reuse Distance Ordering\"></a>Reuse Distance Ordering</h3><h2 id=\"Classification-Based-Policies\"><a href=\"#Classification-Based-Policies\" class=\"headerlink\" title=\"Classification-Based Policies\"></a>Classification-Based Policies</h2><p>基于分类的策略相比重用距离预测简单一下，接下来会考虑一下设计问题：</p>\n<ol>\n<li>Which caching solution is the policy learning?</li>\n<li>What is the prediction mechanism, and at what granularity are the predictions being made?</li>\n<li>What is the aging mechanism for ensuring that inaccurate predictions are eventually evicted?</li>\n</ol>\n<h3 id=\"Sampling-Based-Dead-Block-Prediction（SDBP）\"><a href=\"#Sampling-Based-Dead-Block-Prediction（SDBP）\" class=\"headerlink\" title=\"Sampling Based Dead Block Prediction（SDBP）\"></a>Sampling Based Dead Block Prediction（SDBP）</h3><p>为降低对所有告诉缓存块维护指令跟踪的成本，引入了基于采样的死块预测器。预测结果为死块的不会被插入缓存（缓存准入）。对于上面的三个问题的回答：</p>\n<ol>\n<li>SDBP 从 LRU 采样器中学习缓存决策；</li>\n<li>在 PC 的粒度上使用 skewed 预测器预测死块；</li>\n<li>预测为缓存友好的一次性访问按照基本的替换策略衰减，预测为缓存不友好的可以重用的对象永远无法得到重用（不太好）。</li>\n</ol>\n<h3 id=\"Signature-Based-Hit-Prediction（SHIP）\"><a href=\"#Signature-Based-Hit-Prediction（SHIP）\" class=\"headerlink\" title=\"Signature Based Hit Prediction（SHIP）\"></a>Signature Based Hit Prediction（SHIP）</h3><p>idea 是重用行为与将对象插入缓存的 PC 值相关，在 memory cache 和 CDN cache 中似乎没有类似相关的参照。</p>\n<h3 id=\"Hawkeye\"><a href=\"#Hawkeye\" class=\"headerlink\" title=\"Hawkeye\"></a>Hawkeye</h3><p><strong>个人感觉参考价值比较高，也带有一点 learning 的想法。</strong></p>\n<ol>\n<li><p>主要目标：避免基于启发式的解决方案（例如LRU）的病态；</p>\n</li>\n<li><p>idea：建立在 Belady 方法之上，因为 Belady 方法作为理论最优方法，它是适应所有工作负载的，但它又必须依靠未来的信息，无法使用在实际生产环境中；</p>\n</li>\n<li><p>关键见解：虽然无法知道未来的信息，但可以应用 Belady 于过去的内存引用。如果过去的行为和未来的行为相关性很高，那么效果应该也是很好的；</p>\n</li>\n<li><p><strong>该使用多少历史信息？</strong> 为了精准地找到后向重用距离，应该使用所有的历史信息，但这无疑开销很大而且是越来越大的那种，在这里作者使用实验来指导时间窗口的大小，最终使用 8 倍缓存容量的时间窗口。</p>\n <img src=\"/2023/04/01/cache_replace_policies/hawkeye%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3.jpg\" class=\"\" title=\"hawkeye时间窗口\">\n</li>\n<li><p>8 倍缓存空间的历史信息似乎仍然太大了，于是 Hawkeye 只计算几个抽样集的最优解。</p>\n</li>\n</ol>\n<p>通过使用过去的访问记录来模拟 OPT 算法的行为产生输入来训练 Hawkeye Predictor，再基于 Predictor 做决策。</p>\n<h3 id=\"Perceptron-Based-Prediction\"><a href=\"#Perceptron-Based-Prediction\" class=\"headerlink\" title=\"Perceptron-Based Prediction\"></a>Perceptron-Based Prediction</h3><p><strong>这样从发展脉络来看，AI4Cache 是可以走的</strong>。</p>\n<p>SDBP、SHIP 和 Hawkeye 全都使用基于 PC 的预测器实现了接近 70%-80% 的准确率。后面有研究者使用更好的特征和更好的预测器模型提高了预测准确率。</p>\n<p>比如使用感知机（简单的人工神经元）为 PC 增加更多的特征（PC 的历史、来自内存地址的位、数据的压缩表示、块被访问的次数），每个特征都被用来索引一个不同的饱和计数器表，然后将其相加并与阈值进行比较以生成二进制预测结果。小部分访问被采样以便按照感知机规则去更新模型：如果预测不正确，或者如果总和未能超过某个数量级，则计数器在访问时递减并在驱逐时递增。</p>\n<h2 id=\"Other-Prediction-Metrics\"><a href=\"#Other-Prediction-Metrics\" class=\"headerlink\" title=\"Other Prediction Metrics\"></a>Other Prediction Metrics</h2><h3 id=\"Economic-Value-Added（EVA）\"><a href=\"#Economic-Value-Added（EVA）\" class=\"headerlink\" title=\"Economic Value Added（EVA）\"></a>Economic Value Added（EVA）</h3><p>一方面，对象在缓存中得到重用而创造出价值，另一方面，一个对象在缓存中待得越久它就牺牲了部分缓存空间。</p>\n<p>$EVA &#x3D; 预期命中数 - (缓存命中率&#x2F;缓存大小) * 预期在缓存驻留时间$</p>\n<h1 id=\"Richer-Considerations\"><a href=\"#Richer-Considerations\" class=\"headerlink\" title=\"Richer Considerations\"></a>Richer Considerations</h1><p>（前面都是谈 CPU 缓存替换的具体算法，这一节是讨论缓存替换策略中的具体主义事项，值得借鉴的地方会更多。）</p>\n<p>前面都是狭隘地关注缓存替换问题，无论是从指标方面——缓存命中率，还是在和计算机其它组件的配合方面。</p>\n<h2 id=\"Cost-Aware-Cache-Replacement\"><a href=\"#Cost-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Cost-Aware Cache Replacement\"></a>Cost-Aware Cache Replacement</h2><p>如果所有的缓存未命中代价相同，那么设计最小化全局缺失率的缓存替换算法就可以了，但实际上，不同的缓存未命中对性能产生不同的影响，比如孤立的未命中（低内存级并行性）往往比集群未命中（高内存并行性）代价更高，因为<strong>并行性越高，多个缓存缺失同时发生可能只会造成一次停顿</strong>，再比如在程序关键路径上的未命中对程序性能的影响比不在程序关键路径上的更大。考虑到这些成本的智能替换策略可以优先缓存高成本的未命中（以较低的命中率为代价）以获得更好的程序性能。</p>\n<ol>\n<li>最优成本感知替换策略（The optimal cost-aware replacement policy, CSOPT）目的就是将所有未命中的总成本降至最低，而不是将未命中的数量降至最低。其基本思想是遵循 MIN 策略，同样也是离线算法。</li>\n<li>MLP（memory-level parallelism）考虑并行性缺失和独立性缺失的影响（第一个例子）；</li>\n</ol>\n<h2 id=\"Criticality-Driven-Cache-Optimizations\"><a href=\"#Criticality-Driven-Cache-Optimizations\" class=\"headerlink\" title=\"Criticality-Driven Cache Optimizations\"></a>Criticality-Driven Cache Optimizations</h2><p>关键性是比 MLP 更通用的成本函数，将关键负载定义为需要提前完成以防止处理器停顿的任何负载，而非关键负载是可以容忍长时间延迟的负载。而关键驱动的缓存优化优先考虑关键负载而不是非关键负载。</p>\n<p>将关键负载从低级别缓存预取到高级别缓存。</p>\n<h2 id=\"Mutil-Core-Aware-Cache-Management\"><a href=\"#Mutil-Core-Aware-Cache-Management\" class=\"headerlink\" title=\"Mutil-Core-Aware Cache Management\"></a>Mutil-Core-Aware Cache Management</h2><p>多核竞争访问共享缓存容量可能会发生流式应用程序驱逐其它缓存友好应用程序的有用数据。</p>\n<p>为处理这种共享缓存干扰，可以（1）在内核之间进行对缓存进行分区以避免竞争；（2）修改替换策略以避免病态的访问行为。</p>\n<p>缓存分区保证了强隔离和公平性保证。在实现时它有两个主要考虑因素：（1）如何在缓存中强制执行分区？（2）分区大小如何确定？</p>\n<p>执行缓存分区最常用的机制是为每个应用程序分配专用通道，这样每个应用程序只能从自己的分区中插入和驱逐数据。更高级的做法是避免硬分区，取而代之的是通过修改替换策略来实现隐式分区。</p>\n<p>分区大小可以由用户、操作系统或硬件决定。</p>\n<p>显示分区如 UCP、ASM-Cache，隐式分区如 TADIP、PIPP 等。</p>\n<h2 id=\"Prefetch-Aware-Cache-Replacement\"><a href=\"#Prefetch-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Prefetch-Aware Cache Replacement\"></a>Prefetch-Aware Cache Replacement</h2><p>有效的预取隐藏 DRAM 访问的长延时，因此，将预取器和缓存替换策略结合起来是一个很好的想法。</p>\n<p>设计预取感知的缓存替换策略有两个主要目标：（1）替换策略应该避免不准确的预取导致的<strong>缓存污染</strong>；（2）替换策略应该优先丢弃可以预取的对象而不是难以预取的对象。</p>\n<h3 id=\"Cache-Pollution\"><a href=\"#Cache-Pollution\" class=\"headerlink\" title=\"Cache Pollution\"></a>Cache Pollution</h3><p>大多数预取感知缓存替换策略侧重于通过识别和驱逐不准确的预取来减少缓存污染。这样的解决方案可以分为两大类。</p>\n<ol>\n<li>从预取器那里获取反馈来识别可能不准确的预取请求；</li>\n<li>独立于预取器工作并监视缓存行为以适应替换决策，但可能缺少精确的预取器特定信息。</li>\n</ol>\n<h3 id=\"Deprioritizing-Prefetchable-Lines\"><a href=\"#Deprioritizing-Prefetchable-Lines\" class=\"headerlink\" title=\"Deprioritizing Prefetchable Lines\"></a>Deprioritizing Prefetchable Lines</h3><p>当存在预取行为时，MIN 算法是不完整的，因为它不区分可预取和难以预取的对象。</p>\n<h2 id=\"Cache-Architecture-Aware-Cache-Replacement\"><a href=\"#Cache-Architecture-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Cache Architecture-Aware Cache Replacement\"></a>Cache Architecture-Aware Cache Replacement</h2><p>到目前为止，我们假设由缓存替换策略推断的优先级排序与缓存架构无关。但是，缓存架构的变化可能会对缓存替换产生影响。我们现在讨论缓存架构的两个变化。</p>\n<h3 id=\"Inclusion-Aware-Cache-Replacement\"><a href=\"#Inclusion-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Inclusion-Aware Cache Replacement\"></a>Inclusion-Aware Cache Replacement</h3><p>inclusive 缓存要求多级缓存层次结构中所有较小的缓存内容是 LLC 的子集，这种方式简化了缓存一致性协议，但是限制了缓存层次的有效容量为 LLC 的大小，同时当对象从 LLC 中逐出时，小容量缓存中的该对象被强制无效。</p>\n<p>Inclusion-Aware Cache Replacement 通常在较小缓存中持续驻留热数据，同时延长 LLC 中此类缓存的生命周期。</p>\n<h3 id=\"Compression-Aware-Cache-Replacement\"><a href=\"#Compression-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Compression-Aware Cache Replacement\"></a>Compression-Aware Cache Replacement</h3><p>增加缓存容量可以提高命中率，提高系统性能，但它是以缓存成本和耗电为代价的。压缩缓存提供了另一种解决方案，其中缓存中的数据被压缩以获得更高的有效容量。例如，如果每个缓存块都可以压缩 4 倍，则有效缓存容量可以增加 4 倍。</p>\n<p>当然，并非所有缓存条目都可以压缩，因此压缩会生成可变大小的缓存块，较大的（未压缩的）块比较小的（压缩的）块消耗更多的缓存空间。</p>\n<p>进行驱逐操作时，往往更倾向于驱逐较大的、时间局部性较差的缓存块。</p>\n<h2 id=\"New-Technology-Considerations\"><a href=\"#New-Technology-Considerations\" class=\"headerlink\" title=\"New Technology Considerations\"></a>New Technology Considerations</h2><p>几十年来，缓存一直使用 SRAM 技术构建，但更新的内存技术将改变这一现状，因为传统 SRAM 缓存已经被证明有许多限制。</p>\n<h3 id=\"NVM-Cache\"><a href=\"#NVM-Cache\" class=\"headerlink\" title=\"NVM Cache\"></a>NVM Cache</h3><p>基于 NVM 的 LLC 缓存容量大且能耗低，但是由于其高写入延迟而导致性能下降。</p>\n<p>为了缓解这个问题，有人提出了两种缓存替换策略。首先，引入 Write Congestion Aware Bypass (WCAB) 策略，消除了对 NVM 缓存的大部分写入，同时避免了缓存命中率的大幅降低。其次，建立一个虚拟混合缓存，吸收并消除了冗余写入，否则会导致 NVM 写入缓慢。</p>\n<h3 id=\"DRAM-Cache\"><a href=\"#DRAM-Cache\" class=\"headerlink\" title=\"DRAM Cache\"></a>DRAM Cache</h3><p>使用 DRAM Cache 作为 L3 缓存和主存之间的桥梁。</p>\n<h1 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h1><ol>\n<li>现在的趋势是细粒度策略；</li>\n<li>现有细粒度策略大多直接使用粗粒度策略的老化机制，未来可以为细粒度策略定制老化机制；</li>\n<li>细粒度策略从过去的行为中学习，但影响其性能主要因素时预测准确性和处理不准确预测的能力；（将缓存替换看作监督学习问题 4.2.3节）</li>\n<li>将缓存替换策略与系统其它组件结合起来，例如预取器，似乎还可以考虑准入器；</li>\n</ol>\n","categories":["Cache"],"tags":["Cache"]},{"title":"CMU15445-Lecture","url":"/2023/03/14/CMU15445-Lecture/","content":"<h1 id=\"Lec1-关系模型\"><a href=\"#Lec1-关系模型\" class=\"headerlink\" title=\"Lec1 关系模型\"></a>Lec1 关系模型</h1><ol>\n<li><p>每个关系（Relation）都是一个无序集合，也叫数据库表，集合中每个元素都是一个元组（tuple），每个 tuple 由一组属性构成，这些属性在逻辑上通常有内在联系。</p>\n</li>\n<li><p>主键（primary key）在一个关系中唯一确定一个 tuple。</p>\n</li>\n<li><p>外键（foreign key）唯一确定另一个关系中的一个 tuple。</p>\n</li>\n<li><p>Data Manipulation Language（DML），数据操作语言，比如增删改查。</p>\n</li>\n<li><p>Data Definition Language（DDL），对数据结构进行修改的语言，比如加索引，建表等。</p>\n</li>\n<li><p>在关系模型中查找和查询数据的两种方式，使用哪种方式是具体的实现问题，与 Relational Model 本身无关。</p>\n<ul>\n<li>Procedural：查询命令需要指定 DBMS 执行时的具体查询策略，如关系代数（Relational Algebra），会因操作步骤影响执行效率；</li>\n<li>Non-Procedural：查询命令只需要指定想要查询哪些数据，无需关心幕后的故事，如 SQL。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Lec2-SQL-进阶\"><a href=\"#Lec2-SQL-进阶\" class=\"headerlink\" title=\"Lec2 SQL 进阶\"></a>Lec2 SQL 进阶</h1><h2 id=\"SQL-历史\"><a href=\"#SQL-历史\" class=\"headerlink\" title=\"SQL 历史\"></a>SQL 历史</h2><p>Structured Query Language，结构化查询语言。</p>\n<h2 id=\"SQL-特性\"><a href=\"#SQL-特性\" class=\"headerlink\" title=\"SQL 特性\"></a>SQL 特性</h2><ol>\n<li>Aggregates：聚类函数，通常可以将查询结果聚合成一个值。<ul>\n<li>AVG(<distinct> col)</li>\n<li>MIN(col)</li>\n<li>MAX(col)</li>\n<li>SUM(<distinct> col)</li>\n<li>COUNT(<distinct> col)</li>\n</ul>\n</li>\n<li>Group By：group by 就是把记录按某种方式分成多组，对每组记录分别做 aggregates 操作。<strong>所有非 aggregates 操作的字段，都必须出现在 group by 语句</strong>。</li>\n<li>Having：基于 aggregation 结果的过滤条件不能写在 WHERE 中，而应放在 HAVING 中，从而对 group by 结果进行进一步筛选。</li>\n<li>Output Control：<ul>\n<li><code>ORDER BY &lt;column*&gt; [ASC|DESC]</code></li>\n<li><code>LIMIT &lt;count&gt; [offset]</code></li>\n</ul>\n</li>\n<li>Common Table Expressions：with 视图（虚拟中间表）。</li>\n<li>Nested Queries：子查询。</li>\n<li>String Operations<ul>\n<li>string match</li>\n<li>string functions</li>\n</ul>\n</li>\n<li>Window Functions：<code>ROW_NUMBER() OVER(&lt;PARTITION BY ...&gt;)</code></li>\n<li>Date&#x2F;Time Operations</li>\n</ol>\n<h1 id=\"Lec3-amp-4-Database-Storage\"><a href=\"#Lec3-amp-4-Database-Storage\" class=\"headerlink\" title=\"Lec3&amp;4 Database Storage\"></a>Lec3&amp;4 Database Storage</h1><h2 id=\"Disk-Manager\"><a href=\"#Disk-Manager\" class=\"headerlink\" title=\"Disk Manager\"></a>Disk Manager</h2><ol>\n<li>为什么不适用 OS 磁盘管理的轮子？主要原因在于，OS 的磁盘管理模块并没有、也不可能会有 DBMS 中的领域知识，因此 DBMS 比 OS 拥有更多、更充分的知识来决定数据移动的时机和数量（OS 只是提供一种普适化的接口，但现在既然具体化到了数据库的场景，那么我们在拥有这种特定场景下的负载特征，自然就可以特制化一些更优秀的策略），具体包括：<ul>\n<li>Flushing dirty pages to disk in the correct order</li>\n<li>Specialized prefetching</li>\n<li>Buffer replacement policy</li>\n<li>Thread&#x2F;process scheduling</li>\n</ul>\n</li>\n<li>DBMS 的磁盘模块主要解决两个问题：<ul>\n<li>How the DBMS represents the database in files on disk（如何使用磁盘文件来表示数据库的数据，比如元数据、索引、数据表等）</li>\n<li>How the DBMS manages its memory and moves data back-and-forth from disk（如何管理数据在内存与磁盘之间的移动）</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"在磁盘文件中表示数据库\"><a href=\"#在磁盘文件中表示数据库\" class=\"headerlink\" title=\"在磁盘文件中表示数据库\"></a>在磁盘文件中表示数据库</h2><h3 id=\"File-Storage\"><a href=\"#File-Storage\" class=\"headerlink\" title=\"File Storage\"></a>File Storage</h3><p>DBMS 通常将自己的所有数据作为一个或多个文件存储在磁盘中，而 OS 只当它们是普通文件，并不知道如何解读这些文件。</p>\n<ol>\n<li>Storage Manager：存储管理器负责维护数据库文件，为读写操作安排合适的调度，从而改善页面的时间和空间局部性</li>\n<li>Database Pages：一个页是固定大小的数据块，每个 page 内部可能存储着 tuples、meta-data、indexes 以及 logs 等等，大多数 DBMS 不会把不同类型数据存储在同一个 page 上。每个 page 带着一个唯一的 id，DBMS 使用一个 indirection layer 将 page id 与数据实际存储的物理位置关联起来。</li>\n<li>Database Heap：有几种方法可以找到 DBMS 想要的页面在磁盘和堆文件中的位置，heap 就是其中一种方式。堆文件是页面的无序集合，以随机顺序存储元组，也就是说我们保存的数据无须按照我们插入时的顺序进行保存。（Linked List 和 Page Directory）。</li>\n</ol>\n<h3 id=\"Page-Layout\"><a href=\"#Page-Layout\" class=\"headerlink\" title=\"Page Layout\"></a>Page Layout</h3><img src=\"/2023/03/14/CMU15445-Lecture/page_layout.jpg\" class=\"\" title=\"page_layout\">\n\n<p>每个 page 被分为两个部分：header 和 data。</p>\n<p>header 中通常包含以下信息：</p>\n<ul>\n<li>Page Size</li>\n<li>Checksum</li>\n<li>DBMS Version</li>\n<li>Transaction Visibility</li>\n<li>Compression Information</li>\n</ul>\n<p>data 中存储真正的数据，包含数据本身和数据的操作日志：Tuple-oriented 和 Log-structured</p>\n<ol>\n<li><p>Tuple-oriented：</p>\n<ul>\n<li><p>Strawman Idea：在 header 中记录 tuple 的个数，然后不断的往下 append 即可。这种方法有两种明显问题，（1）一旦出现删除操作，每次插入就需要遍历一遍，寻找空位，否则就会出现碎片；（2）无法处理变长的数据记录（tuple）</p>\n  <img src=\"/2023/03/14/CMU15445-Lecture/Strawman.jpg\" class=\"\" title=\"Strawman\">\n</li>\n<li><p>Slotted Pages：常用。</p>\n  <img src=\"/2023/03/14/CMU15445-Lecture/slot_page.jpg\" class=\"\" title=\"slot_page\"></li>\n</ul>\n</li>\n<li><p>Log-structured：KV 数据库中好用，因为 KV 数据库中每个键只有一个值，因此只需从日志栈顶往下找第一个 update 就行了，而关系数据库有多个字段，只找到一个 update 可能是不够的。</p>\n<ul>\n<li><p>Slotted-Page Design 存在一些问题：（1）碎片化：删除元组会在页面中留下空白（即时能够迁移也需要额外开销）；（2）无用的磁盘 I&#x2F;O：由于非易失性存储的面向块的特性，整个块需要被读取以获取元组；（3）随机磁盘 I&#x2F;O：磁盘读取器可能必须跳转到 20 个不同的位置才能更新 20 个不同的元组，这非常慢。因此日志结构存储模型仅允许创建新数据而不允许覆盖，解决了上面列出的一些问题。</p>\n</li>\n<li><p>写入速度快，读取速度可能较慢。磁盘写入是顺序的，现有页面是不可变的，导致随机磁盘 I&#x2F;O 减少。</p>\n</li>\n<li><p>为了加快读取，可以创建索引以跳转到日志中的一些特定位置。</p>\n</li>\n<li><p>必要时需要压缩日志。</p>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"Tuple-Layout\"><a href=\"#Tuple-Layout\" class=\"headerlink\" title=\"Tuple Layout\"></a>Tuple Layout</h3><p>tuple-oriented 中的数据本身也有别的元信息，比如：</p>\n<ul>\n<li>Visibility information for the DBMS’s concurrency control protocol (i.e., information about which transaction created&#x2F;modified that tuple).</li>\n<li>Bit Map for NULL values. </li>\n<li>Note that the DBMS does not need to store meta-data about the schema of the database here.</li>\n</ul>\n<p>tuple data 中的属性通常按照创建表时指定的顺序存储。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/tuple_data.jpg\" class=\"\" title=\"tuple_data\">\n\n<p>数据库中的每个元组都分配有一个唯一标识符，最常见的是 <code>page id + (offset or slot)</code>，DBMS 可以创建一个 map，将不同的 tuple 映射到 <code>page id + (offset or slot)</code>。</p>\n<h3 id=\"Tuple-Storage\"><a href=\"#Tuple-Storage\" class=\"headerlink\" title=\"Tuple Storage\"></a>Tuple Storage</h3><p>元组中的数据本质上只是字节数组。由 DBMS 决定如何解释这些字节以导出属性的值。数据表示方案是 DBMS 如何存储这些字节。有五种高级数据类型可以存储在元组中：integers, variable-precision numbers, fixedpoint precision numbers, variable length values, and dates&#x2F;times。</p>\n<p>为了让 DBMS 能够破译元组的内容，它维护了一个内部目录（catelogs）来描述数据库的元数据。这些元数据包括数据库包含的表和列，以及列中的属性的类型以及列的顺序。大多数 DBMS 同样以表的格式将它们的 catelogs 存储在自身内部。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ol>\n<li>database is organized in pages;</li>\n<li>different ways to track pages：Heap File Organization…;</li>\n<li>different ways to store pages：Linked List 和 Page Directory;</li>\n<li>different ways to store tuples：Strawman，slotted 和 Log-structured;</li>\n</ol>\n<h1 id=\"Lec5-Storage-Models-amp-Compression\"><a href=\"#Lec5-Storage-Models-amp-Compression\" class=\"headerlink\" title=\"Lec5 Storage Models &amp; Compression\"></a>Lec5 Storage Models &amp; Compression</h1><h2 id=\"Database-Workload\"><a href=\"#Database-Workload\" class=\"headerlink\" title=\"Database Workload\"></a>Database Workload</h2><ol>\n<li>OLTP：Online Transaction Processing，运行速度快、运行时间短、对单个实体进行简单查询。OLTP 工作负载通常会处理更多的写入操作。</li>\n<li>OLAP：Online Analytical Processing，长时间运行、复杂查询、读取数据库中的大部分数据。在 OLAP 工作负载中，数据库系统通过分析大量现有数据，并期望从中获得结论。</li>\n<li>HTAP：Hybrid Transaction + Analytical Processing，尝试在同一个数据库上同时执行 OLTP 和 OLAP 的组合。</li>\n</ol>\n<img src=\"/2023/03/14/CMU15445-Lecture/workloads.jpg\" class=\"\" title=\"workloads\">\n\n<h2 id=\"Storage-Models\"><a href=\"#Storage-Models\" class=\"headerlink\" title=\"Storage Models\"></a>Storage Models</h2><h3 id=\"N-Ary-Storage-Model-NSM\"><a href=\"#N-Ary-Storage-Model-NSM\" class=\"headerlink\" title=\"N-Ary Storage Model (NSM)\"></a>N-Ary Storage Model (NSM)</h3><p>将单个元组的所有属性连续存储在单个页中，这种方法非常适合 OLTP 工作负载，其中请求插入量大且事务倾向于操作单个实体。这是理想的，因为只需要一次读取就可以获取单个元组的所有属性。</p>\n<ol>\n<li>优点<ul>\n<li>快速插入、更新和删除。</li>\n<li>适用于需要整个元组的查询。</li>\n</ul>\n</li>\n<li>缺点：不适合检索表的大部分元组，或者只需要查找小部分的属性。</li>\n</ol>\n<h3 id=\"Decomposition-Storage-Model-DSM\"><a href=\"#Decomposition-Storage-Model-DSM\" class=\"headerlink\" title=\"Decomposition Storage Model (DSM)\"></a>Decomposition Storage Model (DSM)</h3><p>将所有元组的单个属性连续地存储在一个 page 中，这种存储方式特别适用于 OLAP 场景。</p>\n<ol>\n<li>优点<ul>\n<li>减少了浪费的 I&#x2F;O 量，因为 DBMS 只读取该查询所需的数据。</li>\n<li>更好的查询处理和数据压缩支持。</li>\n</ul>\n</li>\n<li>缺点<ul>\n<li>由于元组拆分&#x2F;拼接，点查询、插入、更新和删除速度较慢。</li>\n<li>引入新问题：如何跟踪每个元组的不同属性？（1）Fixed-length Offsets：每个属性都是定长的，直接靠 offset 来跟踪（常用），但是不灵活；（2）Embedded Tuple Ids：在每个属性前面都加上 tupleID，但是需要额外的存储开销。</li>\n</ul>\n</li>\n</ol>\n<p><strong>chose right storage model for the ：OLAP（column store） 和 OLTP（row store）。</strong></p>\n<h2 id=\"Database-Compression\"><a href=\"#Database-Compression\" class=\"headerlink\" title=\"Database Compression\"></a>Database Compression</h2><p>压缩对于基于磁盘的 DBMS 来说非常重要，因为磁盘 I&#x2F;O 几乎总是主要的瓶颈；DBMS 可以通过压缩页面以提高每个 I&#x2F;O 操作能够移动的数据有效量，尤其是在只读分析工作负载中。</p>\n<p>如果数据集是完全随机的，那么将没有办法进行压缩。但是，真实世界中的数据集往往存在 key insight，比如：</p>\n<ol>\n<li>数据集往往具有高度偏斜的属性值分布（例如，齐夫分布）</li>\n<li>同一元组的属性之间具有高度相关性（例如，邮政编码和城市，订购日期和发货日期）。</li>\n</ol>\n<p>基于此，我们希望数据库压缩方案具有如下性质：</p>\n<ol>\n<li>必须产生固定长度的值。唯一的例外是存储在单独池中的变长数据，这是因为 DBMS 应该遵循字对齐并能够使用偏移量访问数据。</li>\n<li>允许 DBMS 在查询执行期间尽可能长时间地推迟解压缩（延迟实现）。</li>\n<li>必须是无损方案，因为人们不喜欢丢失数据。任何一种有损压缩都必须在应用程序层级执行。</li>\n</ol>\n<p>在向 DBMS 添加压缩功能之前，我们需要确定要压缩的数据类型，这决定了压缩方案是否可用。压缩粒度有四个级别：</p>\n<ol>\n<li>块级：压缩一个块，这个块中的所有元组隶属于同一张表。</li>\n<li>元组级别：压缩整个元组的内容（仅限 NSM）。</li>\n<li>属性级别：在一个元组中压缩单个属性值。可以针对同一个元组的多个属性。</li>\n<li>列级：为多个元组压缩一个或多个属性值（仅限 DSM）。这允许更复杂的压缩方案。</li>\n</ol>\n<h2 id=\"Naive-Compression\"><a href=\"#Naive-Compression\" class=\"headerlink\" title=\"Naive Compression\"></a>Naive Compression</h2><p>DBMS 使用通用算法（例如 gzip、LZO、LZ4、Snappy、Brotli、Oracle OZIP，Zstd）。尽管 DBMS 可以使用多种压缩算法，但工程师经常选择具有较低压缩率的压缩算法以换取更快的压缩&#x2F;解压缩速度。</p>\n<p>MySQL InnoDB 中有一个使用简单压缩的示例。DBMS 压缩磁盘页、往里面填充 2 的幂次方 KB 并将它们存储到缓冲池中。然而，每次 DBMS 尝试读取数据时，缓冲池中的压缩数据需要解压。</p>\n<p>由于访问数据需要对压缩数据进行解压，这就限制了压缩机制的范围。如果目标是将整个表压缩成一个巨大的块，使用简单的压缩方案<br>这是不可能的，因为每次访问都需要压缩&#x2F;解压缩整个表。因此，对于 MySQL，由于压缩范围有限，它将表分成更小的块。</p>\n<p>另一个问题是这些简单的方案也没有考虑数据的高级特征或语义。这些算法既不关心数据的结构，也不关心查询计划如何访问数据。因此，这放弃了利用 <code>late materialization</code> 的机会，因为这样的话 DBMS 无法知道什么时候可以延迟数据的解压缩。</p>\n<h2 id=\"Columnar-Compression\"><a href=\"#Columnar-Compression\" class=\"headerlink\" title=\"Columnar Compression\"></a>Columnar Compression</h2><h3 id=\"Run-length-Encoding\"><a href=\"#Run-length-Encoding\" class=\"headerlink\" title=\"Run-length Encoding\"></a>Run-length Encoding</h3><h3 id=\"Bit-Packing-Encoding\"><a href=\"#Bit-Packing-Encoding\" class=\"headerlink\" title=\"Bit-Packing Encoding\"></a>Bit-Packing Encoding</h3><h3 id=\"Bitmap-Encoding\"><a href=\"#Bitmap-Encoding\" class=\"headerlink\" title=\"Bitmap Encoding\"></a>Bitmap Encoding</h3><h3 id=\"Delta-Encoding\"><a href=\"#Delta-Encoding\" class=\"headerlink\" title=\"Delta Encoding\"></a>Delta Encoding</h3><h3 id=\"Incremental-Encoding\"><a href=\"#Incremental-Encoding\" class=\"headerlink\" title=\"Incremental Encoding\"></a>Incremental Encoding</h3><h3 id=\"Dictionary-Encoding\"><a href=\"#Dictionary-Encoding\" class=\"headerlink\" title=\"Dictionary Encoding\"></a>Dictionary Encoding</h3><h1 id=\"Lec6-Memory-Management：Buffer-Pools\"><a href=\"#Lec6-Memory-Management：Buffer-Pools\" class=\"headerlink\" title=\"Lec6 Memory Management：Buffer Pools\"></a>Lec6 Memory Management：Buffer Pools</h1><p>DBMS 的磁盘管理模块主要解决两个问题，一个是在 Lec3 &amp; Lec4 中提到的<strong>如何使用磁盘文件来表示数据库的数据（元数据、索引、数据表等）</strong>，另一个就是本节将介绍的<strong>如何管理数据在内存与磁盘之间的移动</strong>。</p>\n<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>DBMS 负责管理其内存并从磁盘来回移动数据。因为，对于大多数情况，数据不能直接在磁盘上操作，任何数据库都必须能够高效地移动以文件形式表示在磁盘上的数据，将它们装入内存，以便可以使用。</p>\n<p>DBMS 面临的问题是将移动数据的延迟降到最低。理想情况下，延迟应该是0，也就是用户使用数据之前，数据已经在内存中了（预取）。而执行引擎不必考虑数据是如何被预取到内存的（内存管理器考虑）。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/data_move.jpg\" class=\"\" title=\"data_move\">\n\n<p>考虑这个问题的另一种方法是划分为空间和时间控制。</p>\n<ol>\n<li>空间控制策略通过决定将 pages 写到磁盘的哪个位置，使得常常一起使用的 pages 在磁盘的物理距离更近，从而提高 I&#x2F;O 效率。</li>\n<li>时间控制策略通过决定何时将 pages 读入内存，写回磁盘，使得磁盘读写的次数最少，从而提高 I&#x2F;O 效率。</li>\n</ol>\n<h2 id=\"Locks-vs-Latches\"><a href=\"#Locks-vs-Latches\" class=\"headerlink\" title=\"Locks vs. Latches\"></a>Locks vs. Latches</h2><p>在讨论 DBMS 如何保护其内部元素时，我们需要分清 <code>lock</code> 和 <code>latch</code>。</p>\n<p><strong>lock</strong>：lock 是一种更高层级别的逻辑原语，用于保护一个事务中数据库的内容（例如，元组、表、数据库）。事务将在其整个持续时间内持有锁。数据库系统可以向用户公开运行查询时持有哪些 lock。此外，lock 需要能够支持回滚修改。</p>\n<p><strong>latch</strong>：latch 是一种低层级的保护原语，DBMS 在其内部数据结构（例如哈希表、内存区域）中使用以保护临界区。latch 仅在执行操作期间保持。latch 不需要支持回滚修改。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/lock_latch.png\" class=\"\" title=\"lock_latch\">\n\n<h2 id=\"Buffer-Pool\"><a href=\"#Buffer-Pool\" class=\"headerlink\" title=\"Buffer Pool\"></a>Buffer Pool</h2><p>缓冲池本质上是数据库内部分配的一个大内存区域，用于存储从磁盘获取的页面。</p>\n<p>DBMS 启动时会从 OS 申请一片内存区域，即 Buffer Pool，并将这块区域划分成大小相同的 pages，为了与 disk pages 区别，通常称为 frames。当用户请求一个页面时，DBMS 会先查询缓冲池，如果没有找到页面，则 DBMS 会进一步请求原始的 disk page ，并将该 disk page 复制到 Buffer Pool 的一个 frame 中。</p>\n<h3 id=\"Buffer-Pool-元数据\"><a href=\"#Buffer-Pool-元数据\" class=\"headerlink\" title=\"Buffer Pool 元数据\"></a>Buffer Pool 元数据</h3><p>缓冲池必须维护一定的元数据才能被高效且正确地使用。</p>\n<p>首先，<strong>page table</strong> 是一个内存中的哈希表，用于跟踪当前内存中的页面。它将 page id 映射到缓冲池中的帧位置。</p>\n<p>注意：不要将 page table 和 page directory 混淆，page directory 是将 page id 映射到该页在数据库文件中的位置。对页面目录的所有更改都必须记录在磁盘上，以便 DBMS 可以在重启后找到。</p>\n<p>page table 还维护每个页面的附加元数据、dirty flag 和引用计数器。</p>\n<ol>\n<li>dirty flag 由线程在修改页面时设置。这向存储管理器表明该页面已被修改，必须写回磁盘以持久化。</li>\n<li>引用计数器跟踪当前访问该页面（读取或修改它）的线程数。线程必须在访问页面之前递增计数器，如果页面的计数大于零，则<br> 不允许存储管理器从内存中逐出该页面。</li>\n</ol>\n<h3 id=\"Memory-Allocation-Policies\"><a href=\"#Memory-Allocation-Policies\" class=\"headerlink\" title=\"Memory Allocation Policies\"></a>Memory Allocation Policies</h3><p>缓冲池中的缓存有两种分配策略。</p>\n<p>全局策略：考虑所有活动事务以找到分配内存的最佳决策，以使正在执行的整个工作负载受益（比如最小化全局的磁盘读写次数，类似缓存分配&#x2F;分区策略中的 UCP，相对于动态地为缓冲池分区了）。</p>\n<p>局部策略：它做出的决策将使单个查询或事务运行得更快，即使这对整个工作负载不利。局部策略将帧分配给特定事务而不考虑正在并发进行的其它事务行为（first-in-first-out，对每个事务 ”屡求屡给“，也就相当于没有对缓冲池分区）。</p>\n<p>大多数系统结合使用全局视图和局部视图。</p>\n<h2 id=\"Buffer-Pool-优化\"><a href=\"#Buffer-Pool-优化\" class=\"headerlink\" title=\"Buffer Pool 优化\"></a>Buffer Pool 优化</h2><h3 id=\"Multiple-Buffer-Pools\"><a href=\"#Multiple-Buffer-Pools\" class=\"headerlink\" title=\"Multiple Buffer Pools\"></a>Multiple Buffer Pools</h3><p>DBMS 可以拥有多个缓冲池，比如为每个数据库设置一个缓冲池，为每种页面类型（data page、directory page…）设置一个缓冲池，从而每个缓冲池可以为特定的数据库、页面类型设置局部的缓存策略。与此同时，在处理并发事务时，也可以减少 latch 的争用。</p>\n<h3 id=\"Prefetching\"><a href=\"#Prefetching\" class=\"headerlink\" title=\"Prefetching\"></a>Prefetching</h3><p>缓存预取策略。比如在全局扫描查询中，可以预先将该表接下来将扫描的 page 预取到缓冲池中；将 B+ 树的根节点预取到缓冲池中。</p>\n<h3 id=\"Scan-Sharing-Synchronized-Scans\"><a href=\"#Scan-Sharing-Synchronized-Scans\" class=\"headerlink\" title=\"Scan Sharing (Synchronized Scans)\"></a>Scan Sharing (Synchronized Scans)</h3><p>Scan Sharing 技术主要用在多个查询存在数据共用的情况。当两个查询 A, B 先后发生，B 发现自己有一部分数据与 A 共用，于是先共用 A 的 cursor，等 A 扫完后，再扫描自己还需要的其它数据。</p>\n<h3 id=\"Buffer-Pool-Bypass\"><a href=\"#Buffer-Pool-Bypass\" class=\"headerlink\" title=\"Buffer Pool Bypass\"></a>Buffer Pool Bypass</h3><p>缓存准入策略。全表扫描的 page 可能无需加入缓冲池中，因为它们的时间局部性较差，加入缓存可能会驱逐缓冲池中其它一些局部性较好的页面。因此，DBMS 可能单独分配一块局部内存（Bypass，旁路），在该内存中的写入和处理不影响缓冲池中的其它数据。</p>\n<h2 id=\"OS-Page-Cache\"><a href=\"#OS-Page-Cache\" class=\"headerlink\" title=\"OS Page Cache\"></a>OS Page Cache</h2><p>大部分 disk operations 都是通过系统调用完成，通常系统会维护自身的数据缓存，这会导致一份数据分别在操作系统和 DMBS 中被缓存两次。大多数 DBMS 都会使用 (O_DIRECT) 来告诉 OS 不要缓存这些数据，除了 Postgres。</p>\n<h2 id=\"Buffer-Replacement-Policies\"><a href=\"#Buffer-Replacement-Policies\" class=\"headerlink\" title=\"Buffer Replacement Policies\"></a>Buffer Replacement Policies</h2><p>缓存替换策略。</p>\n<h3 id=\"localization\"><a href=\"#localization\" class=\"headerlink\" title=\"localization\"></a>localization</h3><p>每次事务查询只能驱逐有限的缓冲池页面，或者是拥有一份自己独享的缓冲池区域（缓存分区）。</p>\n<h3 id=\"priority-hints\"><a href=\"#priority-hints\" class=\"headerlink\" title=\"priority hints\"></a>priority hints</h3><p>允许事务告知缓冲池哪些页面非常重要，比如根据页面的具体内容，或者是类似于 B+ 树的根节点。</p>\n<h3 id=\"Dirty-Pages\"><a href=\"#Dirty-Pages\" class=\"headerlink\" title=\"Dirty Pages\"></a>Dirty Pages</h3><p>驱逐一个 dirty page 的成本要高于驱逐一般 page，因为前者需要写 disk，后者可以直接 drop，因此 DBMS 在驱逐缓冲池中的 page 时，需要权衡页面局部性和写回的开销。</p>\n<p>除了直接在 Replacement Policies 中考虑，有的 DBMS 使用 Background Writing 的方式来处理。它们定期扫描 page table，发现 dirty page 就写入 disk，在 Replacement 发生时就无需考虑脏数据带来的问题。</p>\n<h2 id=\"Other-Memory-Pools\"><a href=\"#Other-Memory-Pools\" class=\"headerlink\" title=\"Other Memory Pools\"></a>Other Memory Pools</h2><p>除了存储 tuples 和 indexes，DBMS 还需要 Memory Pools 来存储其它数据，如：</p>\n<ul>\n<li>Sorting + Join Buffers</li>\n<li>Query Caches</li>\n<li>Maintenance Buffers </li>\n<li>Log Buffers</li>\n<li>Dictionary Caches</li>\n</ul>\n<h1 id=\"Lec7-Hash-Tabels\"><a href=\"#Lec7-Hash-Tabels\" class=\"headerlink\" title=\"Lec7 Hash Tabels\"></a>Lec7 Hash Tabels</h1><h2 id=\"Data-Structures\"><a href=\"#Data-Structures\" class=\"headerlink\" title=\"Data Structures\"></a>Data Structures</h2><p>DBMS 为系统内部得不同部分设置了相应的数据结构，比如：</p>\n<ol>\n<li><strong>Internal Meta-Data</strong>：用于跟踪数据库和系统状态的数据，比如 page tables，page directories；</li>\n<li><strong>Core Data Storage</strong>：数据库中元组的基本数据结构；</li>\n<li><strong>Temporary Data Structures</strong>：DBMS 可以在处理查询的过程中临时构建数据结构从而加快执行速度（例如，用于连接的哈希表）；</li>\n<li><strong>Table Indexes</strong>：辅助数据结构，可以用来更容易地找到特定的元组。</li>\n</ol>\n<p>在为 DBMS 实现数据结构时，需要考虑两个主要的设计问题：</p>\n<ol>\n<li>数据组织：我们需要弄清楚内存如何布局，以及支持高效访问，里面存放什么信息。</li>\n<li>并发性：我们还需要考虑如何让多个线程访问数据结构，而不会引起问题。</li>\n</ol>\n<h2 id=\"Hash-Table\"><a href=\"#Hash-Table\" class=\"headerlink\" title=\"Hash Table\"></a>Hash Table</h2><p>哈希表实现了 associative array ADT（Abstract Data Type），将键映射到值。</p>\n<p>Hash Table 主要分为两部分：</p>\n<ol>\n<li><p>Hash Function：</p>\n<ul>\n<li>How to map a large key space into a smaller domain</li>\n<li>Trade-off between <strong>being fast</strong> vs <strong>collision rate</strong></li>\n</ul>\n</li>\n<li><p>Hashing Scheme：</p>\n<ul>\n<li>How to handle key collisions after hashing</li>\n<li>Trade-off between <strong>allocating a large hash table</strong> vs <strong>additional instructions to find&#x2F;insert keys</strong></li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Hash-Functions\"><a href=\"#Hash-Functions\" class=\"headerlink\" title=\"Hash Functions\"></a>Hash Functions</h2><p>由于 DBMS 内使用的 Hash Function  并不会暴露在外，因此没必要使用加密哈希函数（例如，SHA-256），我们希望它速度越快，碰撞率越低越好。</p>\n<p>当前最先进的哈希函数是 Facebook XXHash3。</p>\n<h2 id=\"Static-Hashing-Schemes\"><a href=\"#Static-Hashing-Schemes\" class=\"headerlink\" title=\"Static Hashing Schemes\"></a>Static Hashing Schemes</h2><p>静态哈希方案中的哈希表大小固定，这意味着如果哈希表的存储空间用完，就得重建一个更大的哈希表，这样的代价非常昂贵。通常，新哈希表的大小是原始哈希表大小的 2 倍。同时，为了减少发生碰撞时比较的次数，哈希表的槽数会设置成预期元素书的 2 倍，但可惜的是，（1）元素的数量很难提前知道；（2）不同数据通过哈希函数计算得到的键可能会相等；（3）没有一个完美的哈希函数。</p>\n<h3 id=\"Linear-Probe-Hashing\"><a href=\"#Linear-Probe-Hashing\" class=\"headerlink\" title=\"Linear Probe Hashing\"></a>Linear Probe Hashing</h3><h3 id=\"Robin-Hood-Hashing\"><a href=\"#Robin-Hood-Hashing\" class=\"headerlink\" title=\"Robin Hood Hashing\"></a>Robin Hood Hashing</h3><h3 id=\"Cuckoo-Hashing\"><a href=\"#Cuckoo-Hashing\" class=\"headerlink\" title=\"Cuckoo Hashing\"></a>Cuckoo Hashing</h3><h2 id=\"Dynamic-Hashing-Schemes\"><a href=\"#Dynamic-Hashing-Schemes\" class=\"headerlink\" title=\"Dynamic Hashing Schemes\"></a>Dynamic Hashing Schemes</h2><p>动态哈希方案可以按需扩容缩容。</p>\n<h3 id=\"Chained-Hashing\"><a href=\"#Chained-Hashing\" class=\"headerlink\" title=\"Chained Hashing\"></a>Chained Hashing</h3><p>和 Linear Probe Hashing 有点像，只不过 Linear Probe Hashing 中的哈希槽大小是固定的，而 Chained Hashing 的哈希槽大小是可变的，当新插入数据时，创建一个指针指向新数据，并将指针加入到哈希槽的末尾。</p>\n<h3 id=\"Extendible-Hashing\"><a href=\"#Extendible-Hashing\" class=\"headerlink\" title=\"Extendible Hashing\"></a>Extendible Hashing</h3><p>是 project 1 将要实现的一部分，它的基本思想是哈希值的位数慢慢用，一开始数据少时，只是用哈希值的末尾几位，当数据逐渐增多时，哈希冲突越来越剧烈，就逐渐开放使用更多的哈希值位数，同时分配更多的哈希桶来存储数据，进行 rehash 操作，达到扩容的目的。</p>\n<h3 id=\"Linear-Hashing\"><a href=\"#Linear-Hashing\" class=\"headerlink\" title=\"Linear Hashing\"></a>Linear Hashing</h3><h2 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>哈希表提供了 O(1) 的访问效率，因此被大量地应用于 DBMS 的内部实现中。即便如此，她并不适合作为 table index 的数据结构，而 table index 的首选时下节将介绍的 B+ Tree（aka “The Greatest Data Structure of All Time”）。</p>\n<h1 id=\"Lec8-Trees-Indexes\"><a href=\"#Lec8-Trees-Indexes\" class=\"headerlink\" title=\"Lec8 Trees Indexes\"></a>Lec8 Trees Indexes</h1><h2 id=\"Table-Indexes\"><a href=\"#Table-Indexes\" class=\"headerlink\" title=\"Table Indexes\"></a>Table Indexes</h2><p>表索引作为数据库系统内部的数据结构，常常涉及范围扫描查询。有了表索引后，DBMS 在范围查询时，就不需要进行全表扫描，而是可以选择已有的最佳表索引，更快地找到范围内的元组。</p>\n<p>表的内容和索引在逻辑上是同步的。更多的索引可以加快查询速度，但也意味着更大的存储和维护开销。</p>\n<h2 id=\"B-Tree\"><a href=\"#B-Tree\" class=\"headerlink\" title=\"B+Tree\"></a>B+Tree</h2><p>B+Tree 是一种自平衡树，它将数据有序地存储，且在 search、sequential access、insertions 以及 deletions 操作的复杂度上都满足 O(logn)。</p>\n<p>B+Tree 可以看作是 BST (Binary Search Tree) 的衍生结构，它的每个 node 可以有多个孩子，这特别契合 disk-oriented database 的数据存储方式，每个 page 存储一个 node，使得树的结构扁平化，减少获取索引给查询带来的 I&#x2F;O 成本。其基本结构如下图所示：</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/B+Tree.jpg\" class=\"\" title=\"B+Tree\">\n\n<p>通常来说，B+Tree 是一个 M-路 搜索树（M 代表它的最大孩子数量），它有如下性质：</p>\n<ol>\n<li>每个节点最多存储 M-1 个 key，有 M 个 孩子；</li>\n<li>B+Tree 是完全平衡的，即每个叶子节点的深度都一样；</li>\n<li>除了 root 节点，所有其它节点至少处于半满状态，即 M&#x2F;2−1≤ #keys ≤ M−1； </li>\n<li>假设每个非叶子节点中包含 k 个 keys，那么它必然有 k+1 个孩子；</li>\n<li>B+Tree 的叶子节点通过双向链表串联，访问更高效。</li>\n</ol>\n<p>与 B-Tree 相比，B+Tree 仅在叶子节点上存储数据，而 B-Tree 在非叶子节点上也存储数据。</p>\n<h3 id=\"B-Tree-Node\"><a href=\"#B-Tree-Node\" class=\"headerlink\" title=\"B+ Tree Node\"></a>B+ Tree Node</h3><p>B+ Tree 的每个节点包含一个 K&#x2F;V 键值对数组，K 是从表的 attrubute(s) 中提取出来的，即 K 的类型为表的某列的类型；V 值类型取决于节点是否是叶子节点，对于非叶子节点，V 值类型为指向节点的指针，对于叶子节点，V 值类型通常有两种做法：</p>\n<ol>\n<li>Record&#x2F;Tuple Ids：存储指向最终 tuple 的指针；</li>\n<li>Tuple Data：直接将 tuple data 存在 leaf node 中，但这种方式对于 <a href=\"https://docs.oracle.com/cd/E17275_01/html/programmer_reference/am_second.html\">Secondary Indexes</a> 不适用，因为 DBMS 只能将 tuple 数据存储到一个 index 中，否则数据的存储就会出现冗余，同时带来额外的维护成本。</li>\n</ol>\n<p>B+Tree 节点中的 K&#x2F;V 键值对数组基本是按照 K 值排序的，尽管对于 B+Tree 的定义来说不是必需的；从概念上讲，非叶子节点上的 K 只作为标记使用，指导快速查询，但并不意味着这些 K 一定存在于叶子节点上，但是传统上，非叶子节点上的 K 都出现在叶子节点上。</p>\n<h3 id=\"Insertion\"><a href=\"#Insertion\" class=\"headerlink\" title=\"Insertion\"></a>Insertion</h3><ol>\n<li>根据非叶子节点的 K 从树的根部往下找到新 K&#x2F;V 键值对对应的 leaf node，L；</li>\n<li>如果 L 还有空间，则将 K&#x2F;V 键值对插入到 L 中的合适位置，保证 L 中的 K&#x2F;V 键值对仍是有序；否则，需要将 L 均匀分裂成两个节点，同时在 parent node 上新增 entry，新增 entry 的 K 应该是 L 中的中间 K 值，若 parent node 也空间不足，则递归地分裂，直到 root node 为止。</li>\n</ol>\n<h3 id=\"Deletion\"><a href=\"#Deletion\" class=\"headerlink\" title=\"Deletion\"></a>Deletion</h3><p>和在插入后节点满了需要拆分一样，当删除键值对后如果一个节点的大小小于半满时，需要进行合并操作。</p>\n<ol>\n<li>从 root 开始，找到目标 entry 所处的 leaf node, L；</li>\n<li>删除该 entry；</li>\n<li>如果 L 仍然至少处于半满状态，则操作结束；否则先尝试从 siblings 那里拆借 entries，如果失败，则将 L 与相应的 sibling 合并；</li>\n<li>如果合并发生了，则可能需要递归地删除 parent node 中的 entry。</li>\n</ol>\n<p>B+Tree 的 Insert、Delete 过程，可参考<a href=\"https://dichchankinh.com/~galles/visualization/BPlusTree.html\">这里</a>。</p>\n<h3 id=\"Selection-条件\"><a href=\"#Selection-条件\" class=\"headerlink\" title=\"Selection 条件\"></a>Selection 条件</h3><p>因为 B+Tree 是有序的，所以查找很快并且不需要整个键。如果查询条件中提供了键的任何“特征”，DBMS 可以使用 B+Tree 索引。而散列索引需要具体且完整的键，因此无法进行范围查询。</p>\n<h3 id=\"Non-Unique-Indexes\"><a href=\"#Non-Unique-Indexes\" class=\"headerlink\" title=\"Non-Unique Indexes\"></a>Non-Unique Indexes</h3><p>与哈希表一样，B+Tree 也可以处理重复的键。比如存储有相同键的 K&#x2F;V 对，或者是使用关联链表。</p>\n<h3 id=\"Duplicate-Keys\"><a href=\"#Duplicate-Keys\" class=\"headerlink\" title=\"Duplicate Keys\"></a>Duplicate Keys</h3><p>（有点分不清和 Non-Unique Indexes 的关系）</p>\n<h3 id=\"Clustered-Indexes\"><a href=\"#Clustered-Indexes\" class=\"headerlink\" title=\"Clustered Indexes\"></a>Clustered Indexes</h3><p>Clustered Indexes（聚簇索引）规定了 table 本身的物理存储方式，通常即按 primary key 排序存储，因此一个 table 只能建立一个 clustered index。有些 DBMS 对每个 table 的主键都添加聚簇索引，如果该 table 没有 primary key，则 DBMS 会为其自动生成一个。</p>\n<p>table 本身的物理存储按照聚簇索引排序后，通过聚簇索引进行条件查询时，将减少磁盘的读取次数。</p>\n<h3 id=\"Heap-Clustering\"><a href=\"#Heap-Clustering\" class=\"headerlink\" title=\"Heap Clustering\"></a>Heap Clustering</h3><p>有了聚簇索引后，元组在页面中以及页面间按照聚簇索引的规则变得有序。此时访问元组时如若使用了聚类索引（主键），DBMS 就可以直接选择正确的页面，这也就解释了上面说的为什么可以减少磁盘的读取次数。</p>\n<h3 id=\"Index-Scan-Page-Sorting\"><a href=\"#Index-Scan-Page-Sorting\" class=\"headerlink\" title=\"Index Scan Page Sorting\"></a>Index Scan Page Sorting</h3><p>由于直接从非聚集索引中检索元组效率低下（从磁盘中重复读取相同页面），因此 DBMS 可以首先找出它需要的所有元组，然后根据它们的页面 ID 对它们进行排序，再一次从磁盘中读取对应的元组，从而不会重复读取相同的磁盘页面。</p>\n<h2 id=\"B-Tree-Design-Choices\"><a href=\"#B-Tree-Design-Choices\" class=\"headerlink\" title=\"B+Tree Design Choices\"></a>B+Tree Design Choices</h2><h3 id=\"Node-Size\"><a href=\"#Node-Size\" class=\"headerlink\" title=\"Node Size\"></a>Node Size</h3><p>根据存储介质的不同，Node Size 的选择不一样。例如，存储在 HDD 上的节点大小通常在 MB 数量级，以减少查找数据所需的磁盘读取次数，而内存节点大小可能小至 512 字节，以便将整个页面放入 CPU 缓存并减少数据碎片化。这种选择也可以取决于工作负载的类型，因为点查询更喜欢尽可能小的页面以减少不必要的额外信息加载量，而大的顺序 scan 可能更喜欢大页面以减少它需要执行的读取次数。</p>\n<h3 id=\"Merge-Threshold\"><a href=\"#Merge-Threshold\" class=\"headerlink\" title=\"Merge Threshold\"></a>Merge Threshold</h3><p>由于 merge 操作引起的修改较大，有些 DBMS 选择延迟 merge 操作的发生时间，甚至可以利用其它进程来负责周期性地重建 table index。</p>\n<h3 id=\"Variable-Length-Keys\"><a href=\"#Variable-Length-Keys\" class=\"headerlink\" title=\"Variable Length Keys\"></a>Variable Length Keys</h3><p>B+ Tree 中存储的 key 经常是变长的，通常有三种手段来应对：</p>\n<ol>\n<li>Pointers：存储指向 key 的指针；</li>\n<li>Variable-Length Nodes：Node 的大小可以不一致，但这需要精细化的内存管理（几乎没有人这么做）；</li>\n<li>Padding：对 key 的末尾进行 pad 操作，直至 key 最大长度；</li>\n<li>Key Map&#x2F;Indirection：内嵌一个指针数组，数组中的每个元素指向 K&#x2F;V list。</li>\n</ol>\n<h3 id=\"Intra-node-Search\"><a href=\"#Intra-node-Search\" class=\"headerlink\" title=\"Intra-node Search\"></a>Intra-node Search</h3><p>在节点内部搜索，就是在排好序的序列中检索元素，手段通常有：</p>\n<ol>\n<li>Linear Scan：从节点头部向尾部线性搜索；</li>\n<li>Binary Search：二分查找；</li>\n<li>Interpolation：通过 keys 的分布统计信息来估计大概位置进行检索</li>\n</ol>\n<h2 id=\"Optimizations\"><a href=\"#Optimizations\" class=\"headerlink\" title=\"Optimizations\"></a>Optimizations</h2><h3 id=\"Pointer-Swizzling\"><a href=\"#Pointer-Swizzling\" class=\"headerlink\" title=\"Pointer Swizzling\"></a>Pointer Swizzling</h3><p>Node 中的 V 常常使用 page id 来指向其它 Node，这样的话 DBMS 每次需要首先从 page table 中获取对应 page 的 frame_id，然后才能从 buffer pool 获取相应的 node 本身，而如果 page 已经在 buffer pool 中，我们可以直接存储其在 buffer pool 的位置，从而避免查询 page table（常常还需要使用 latch 避免竞争），提高访问效率。</p>\n<h3 id=\"Bulk-Insert\"><a href=\"#Bulk-Insert\" class=\"headerlink\" title=\"Bulk Insert\"></a>Bulk Insert</h3><p>建 B+ Tree 的最快方式是先将 keys 排好序后，再从下往上建树。因此如果有大量插入操作，可以利用这种方式提高效率。</p>\n<h3 id=\"Prefix-Compression\"><a href=\"#Prefix-Compression\" class=\"headerlink\" title=\"Prefix Compression\"></a>Prefix Compression</h3><p>同一个 leaf node 中的 keys 通常有相同的 prefix，为了节省空间，可以只存所有 keys 的不同的 suffix。</p>\n<h3 id=\"Suffix-Truncation\"><a href=\"#Suffix-Truncation\" class=\"headerlink\" title=\"Suffix Truncation\"></a>Suffix Truncation</h3><p>由于非叶子节点只用于引导搜索，因此没有必要在非叶子节点中储存完整的 key，我们可以只存储足够的 prefix 并保证和原来的搜索语义一样即可。</p>\n<h3 id=\"Deduplication\"><a href=\"#Deduplication\" class=\"headerlink\" title=\"Deduplication\"></a>Deduplication</h3><p>Non-unique indexes 可能在节点中存储了多个相同的 K 副本，为了减少冗余，可以替换只存储唯一 K，其中 V 则使用关联列表代替。</p>\n<h1 id=\"Lec9-Index-Concurrency-Control\"><a href=\"#Lec9-Index-Concurrency-Control\" class=\"headerlink\" title=\"Lec9 Index Concurrency Control\"></a>Lec9 Index Concurrency Control</h1><h2 id=\"Latch-Implementations\"><a href=\"#Latch-Implementations\" class=\"headerlink\" title=\"Latch Implementations\"></a>Latch Implementations</h2><ol>\n<li>Blocking OS Mutex：睡眠锁，当该锁被其它线程占用时，当前线程进入睡眠状态，CPU 将调度其它线程。<ul>\n<li>示例：<code>std::mutex</code>；</li>\n<li>优点：使用简单，不需要 DBMS 进行额外编码；</li>\n<li>缺点：由于是操作系统进行调度，代价昂贵（每次锁定&#x2F;解锁大约需要 25 ns）且不可扩展。</li>\n</ul>\n</li>\n<li>Test-and-Set Spin Latch (TAS)：自旋锁，当该锁被其它线程占用时，用户可以自行决定接下来的操作，比如可以选择重试（while 循环）或允许操作系统将当前线程睡眠，因此，该方法给了 DBMS 更多的控制权。<ul>\n<li>示例：<code>std::atomic&lt;T&gt;</code>；</li>\n<li>优点：锁定&#x2F;解锁操作是高效的（因为很多事情交给了用户来做了）；</li>\n<li>缺点：不可扩展且缓存不友好，因为 CAS（compare-and-set）指令将在不同的线程中执行多次。</li>\n</ul>\n</li>\n<li>Reader-Writer Latches：读写锁。<ul>\n<li>示例：<code>std::shared mutex</code>；</li>\n<li>优点：允许并发读；</li>\n<li>缺点：DBMS 需要管理读写队列，避免大量读请求淹没写请求，同时由于额外的元数据，开销比自旋锁搞。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Hash-Table-Latching\"><a href=\"#Hash-Table-Latching\" class=\"headerlink\" title=\"Hash Table Latching\"></a>Hash Table Latching</h2><p>对于静态哈希表，所有线程在遇到哈希冲突时都是自顶向下查看 slot，因此很容易加锁，比如为每个 page 或者 slot 加锁，这样也不会出现死锁的情况（加锁方向一致），当需要重新调整哈希表的大小时，则获取整个哈希表的全局锁即可；</p>\n<ol>\n<li>page 锁：每个页面都有自己的读写锁来保护其全部内容。线程在访问页面之前获取读或写锁。由于一个页面中有多个 slot，这会降低一定的并行度。</li>\n<li>slot 锁：每个 slot 都有一把锁，此时两个线程可以访问同一页中的不同 slot，这增加了存储（每个 slot 有一个锁变量）和计算（一次查找可能涉及很多次加锁&#x2F;解锁操作）开销。</li>\n</ol>\n<p>对于动态哈希表，加锁比较麻烦，但大致思路和上面一样。</p>\n<h2 id=\"B-Tree-Latching\"><a href=\"#B-Tree-Latching\" class=\"headerlink\" title=\"B+Tree Latching\"></a>B+Tree Latching</h2><p>目标：</p>\n<ol>\n<li>允许多线程同时读取和更新 B+Tree；</li>\n<li>同时需要注意两种情形：（1）多个线程同时修改一个节点的内容；（2）一个线程正在遍历树然而另外一个线程正在 split&#x2F;merge 节点；</li>\n</ol>\n<p>举例：T1 想要删除 44，T2 想要查询 41。删除 44 后，DBMS 需要 rebalance，将 H 节点拆分成两个节点。若在拆分前，T2 读取到 D 节点，发现 41 在 H 节点，此时时间片轮转到了 T1，T1 把 D 节点拆分成 H、I 两个节点，同时把 41 转移到 I 节点，之后 CPU 交还给 T2，T2 到 H 节点就找不到 41，如下图所示。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/B+delete.jpg\" class=\"\" title=\"B+delete\">\n\n<img src=\"/2023/03/14/CMU15445-Lecture/B+rebalance.jpg\" class=\"\" title=\"B+rebalance\">\n\n<img src=\"/2023/03/14/CMU15445-Lecture/B+find.jpg\" class=\"\" title=\"B+find\">\n\n<p>由于 B+Tree 是树形结构，有明确的搜索顺序，因此沿着搜索路径加锁是不错的选择。</p>\n<h3 id=\"Latch-Crabbing-x2F-Coupling\"><a href=\"#Latch-Crabbing-x2F-Coupling\" class=\"headerlink\" title=\"Latch Crabbing&#x2F;Coupling\"></a>Latch Crabbing&#x2F;Coupling</h3><p>它的基本思想是</p>\n<ol>\n<li>获取 parent 的 latch</li>\n<li>获取 child 的 latch</li>\n<li>如果 child “安全”（不会出现 split&#x2F;merge 的情况，它的 parent 节点也就不需要修改），则可以释放 parent 的 latch。</li>\n</ol>\n<h3 id=\"Better-Latching-Algorithm\"><a href=\"#Better-Latching-Algorithm\" class=\"headerlink\" title=\"Better Latching Algorithm\"></a>Better Latching Algorithm</h3><p>在实际应用中，每次更新 B+Tree 都需要获取非叶子节点的写锁是不好的，因为实际的更新只是发生在叶子节点上，而叶子节点大多数情况下是不会影响到上层节点的，尤其是越往上影响到的可能性越小，因此总是持 “悲观” 的想法而获取搜索路径上的的非叶子节点的写锁是不合适的。</p>\n<p>反之，可以采用类似乐观锁的思想，假设 leaf node 是安全（更新操作仅会引起 leaf node 的变化）的，在查询路径上一路获取、释放 read latch，到达 leaf node 时，若操作不会引起 split&#x2F;merge 发生，则只需要在 leaf node 上获取 write latch 然后更新数据，释放 write latch 即可；若操作会引起 split&#x2F;merge 发生，则重新执行一遍，此时在查询路径上一路获取、释放 write latch，即 Latch Crabbing 原始方案。</p>\n<h3 id=\"Leaf-Node-Scans\"><a href=\"#Leaf-Node-Scans\" class=\"headerlink\" title=\"Leaf Node Scans\"></a>Leaf Node Scans</h3><p>以上都是从上往下的访问模式，我们介绍了相关锁的控制；而对于水平方向的访问模式，如果两个线程分别从左往右，从右往左获取写锁，就可能陷入死锁，因此部分 DBMS 在进行范围查询时总是从左往右扫描。</p>\n<h1 id=\"Lec10-Sorting-amp-Aggregations-Algorithms\"><a href=\"#Lec10-Sorting-amp-Aggregations-Algorithms\" class=\"headerlink\" title=\"Lec10 Sorting &amp; Aggregations Algorithms\"></a>Lec10 Sorting &amp; Aggregations Algorithms</h1><p>开始进入 Operator Execution 部分。</p>\n<p>SQL 语句被 DBMS 解释成 Query Plan。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/query_plan.jpg\" class=\"\" title=\"query_plan\">\n\n<h2 id=\"Sorting\"><a href=\"#Sorting\" class=\"headerlink\" title=\"Sorting\"></a>Sorting</h2><p>Disk-Oriented DBMS 将数据持久化在磁盘中，而不会假设整张表能够完全写入内存中，这也就可能由于数据量过大而无法在内存中完全一次性的排序操作。然而很多时候排序是需要的，比如 <code>ORDER BY</code>，<code>DISTINCT</code>，<code>GROUP BY</code> 和 <code>JOIN</code> 都将需要排好序的数据以便能够快速地执行。</p>\n<p>对于数据量太大而无法放入内存的数据，外部归并排序是一个很好的选择。它是一种分而治之的排序算法，它将数据集分成多个独立的块，然后对它们分别进行排序，并将排序结果写入磁盘，以便下一个阶段使用。</p>\n<h3 id=\"Two-way-Merge-Sort\"><a href=\"#Two-way-Merge-Sort\" class=\"headerlink\" title=\"Two-way Merge Sort\"></a>Two-way Merge Sort</h3><p>该算法的最基本版本是二路归并排序。该算法排序阶段读取每个页面，对它们单独进行排序，并将排序后的结果写回磁盘。然后，在合并阶段，它使用三个缓冲页，其中将两个已排序的页面读取到缓冲页中，并将它们合并到第三个缓冲页面中，每当第三页填满后，它就会写回磁盘并替换为空页。</p>\n<p>复杂度：</p>\n<h3 id=\"General-K-way-Merge-Sort\"><a href=\"#General-K-way-Merge-Sort\" class=\"headerlink\" title=\"General (K-way) Merge Sort\"></a>General (K-way) Merge Sort</h3><p>归并排序的一般版本可以使用三个以上的缓冲页，从而加快排序速度。</p>\n<h3 id=\"Double-Buffering-Optimization\"><a href=\"#Double-Buffering-Optimization\" class=\"headerlink\" title=\"Double Buffering Optimization\"></a>Double Buffering Optimization</h3><p>外部归并排序的一个优化是在后端预取下一阶段的排序块并将其存储在第二个缓冲区，而系统正在处理当前排序块。这减少了 I&#x2F;O 请求的等待时间。但这种优化需要使用多线程，因为预取应该在当前运行的计算发生时发生。</p>\n<h3 id=\"Using-B-Trees\"><a href=\"#Using-B-Trees\" class=\"headerlink\" title=\"Using B+Trees\"></a>Using B+Trees</h3><p>有时 DBMS 使用现有的 B+ 树索引来帮助排序。特别是，如果索引是聚簇索引，数据会按照正确的顺序存储在磁盘中，I&#x2F;O访问<br>将是顺序的，从而 DBMS 可以只遍历B+ 树的叶子结点，不需要进行排序操作。</p>\n<p>另一方面，如果索引是非聚簇的，遍历树几乎总是更糟，因为每条记录可以存储在任何页面中，几乎所有记录访问都需要进行磁盘读取。</p>\n<h2 id=\"Aggregations\"><a href=\"#Aggregations\" class=\"headerlink\" title=\"Aggregations\"></a>Aggregations</h2><p>查询计划中的聚合运算符将一个或多个元组的值折叠为单个标量值。有两种实现聚合的方法：</p>\n<ol>\n<li>排序；</li>\n<li>散列。</li>\n</ol>\n<h3 id=\"Sorting-Aggregation\"><a href=\"#Sorting-Aggregation\" class=\"headerlink\" title=\"Sorting Aggregation\"></a>Sorting Aggregation</h3><p>DBMS 首先根据 GROUP BY Key(s)  对元组进行排序。在内存足够时，可以使用快速排序，如果内存不够，可以使用外部归并排序算法。然后，DBMS 对排序后的数据执行顺序扫描以计算聚合。</p>\n<p>但是，有时候我们并不需要排好序的数据，比如 <code>GROUP BY</code> 和 <code>DISTINCT</code>。在这种场景下 hashing 将更好，它能有效减少排序所需的额外工作。</p>\n<h3 id=\"Hashing-Aggregation\"><a href=\"#Hashing-Aggregation\" class=\"headerlink\" title=\"Hashing Aggregation\"></a>Hashing Aggregation</h3><p>hash 在计算上比 sort 开销更小。 DBMS 通过扫描表填充临时哈希表，对于每条记录，检查是否已经有一个条目，并根据聚合类型执行适当的修改。如果哈希表的大小太大而无法全部装入内存时，DBMS 可以将它写入磁盘。这需要两部操作：</p>\n<ol>\n<li>Partition： Use a hash function h1 to split tuples into partitions on disk based on target hash key。这会将所有匹配的元组放入同一分区；</li>\n<li>ReHash：对于磁盘上的每个分区，将其页面读入内存并基于第二个哈希函数 h2（h1 !&#x3D; h2） 构建一个内存中的哈希表。然后遍历这个临时哈希表的每个桶，将匹配的元组放在一起计算聚合（这假定每个分区都能装入内存）。</li>\n</ol>\n<p>在 ReHash phase 中，存着 (GroupKey→RunningVal) 的键值对，当我们需要向 hash table 中插入新的 tuple 时：</p>\n<ol>\n<li>如果我们发现相应的 GroupKey 已经在内存中，只需要更新 RunningVal 就可以；</li>\n<li>反之，则插入新的 GroupKey 到 RunningVal 的键值对。</li>\n</ol>\n<p>复杂度：</p>\n<h1 id=\"推荐阅读\"><a href=\"#推荐阅读\" class=\"headerlink\" title=\"推荐阅读\"></a>推荐阅读</h1><p><a href=\"https://15445.courses.cs.cmu.edu/fall2022/notes/\">官方笔记</a></p>\n<p><a href=\"https://zhenghe.gitbook.io/open-courses/cmu-15-445-645-database-systems/relational-data-model\">公开课笔记</a></p>\n","categories":["数据库"],"tags":["CMU15445","公开课","数据库"]}]