[{"title":"C++ primer笔记","url":"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/","content":"<h1 id=\"C-primer笔记\"><a href=\"#C-primer笔记\" class=\"headerlink\" title=\"C++ primer笔记\"></a>C++ primer笔记</h1><h2 id=\"1-开始\"><a href=\"#1-开始\" class=\"headerlink\" title=\"1 开始\"></a>1 开始</h2><h3 id=\"1-2-初识输入输出\"><a href=\"#1-2-初识输入输出\" class=\"headerlink\" title=\"1.2 初识输入输出\"></a>1.2 初识输入输出</h3><ol>\n<li>cout 输出语句时，尽量要以 endl 结束，从而刷新缓存区，否则如果出现程序崩溃，输出可能还留在缓冲区中。</li>\n</ol>\n<h2 id=\"2-变量和基本类型\"><a href=\"#2-变量和基本类型\" class=\"headerlink\" title=\"2 变量和基本类型\"></a>2 变量和基本类型</h2><h3 id=\"2-2-变量\"><a href=\"#2-2-变量\" class=\"headerlink\" title=\"2.2 变量\"></a>2.2 变量</h3><ol>\n<li>extern：<ul>\n<li>只能作用在函数体外。</li>\n<li>extern 声明一个变量，如果赋予变量初值，则表示定义。</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"2-5-处理类型\"><a href=\"#2-5-处理类型\" class=\"headerlink\" title=\"2.5 处理类型\"></a>2.5 处理类型</h3><ol>\n<li><p>类型别名：</p>\n<ul>\n<li>typedef double wages;</li>\n<li>using my_double &#x3D; double;</li>\n<li>在使用包含指针的类型别名的时候，不能简单地将类型别名替换成原类型，见下图。</li>\n</ul>\n <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/typedef%E6%8C%87%E9%92%88%E7%B1%BB%E5%9E%8B%E5%88%AB%E5%90%8D%E4%B8%8Econst%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8.jpg\" class=\"\" title=\"typedef指针类型别名与const同时使用\">\n</li>\n<li><p>auto</p>\n</li>\n<li><p>decltype：<code>decltype(expression)</code> 返回表达式结果类型（可能是左值）。</p>\n</li>\n</ol>\n<h2 id=\"3字符串、向量和数组\"><a href=\"#3字符串、向量和数组\" class=\"headerlink\" title=\"3字符串、向量和数组\"></a>3字符串、向量和数组</h2><h2 id=\"4-表达式\"><a href=\"#4-表达式\" class=\"headerlink\" title=\"4 表达式\"></a>4 表达式</h2><h3 id=\"4-11-类型转换\"><a href=\"#4-11-类型转换\" class=\"headerlink\" title=\"4.11 类型转换\"></a>4.11 类型转换</h3><ol>\n<li>隐式转换</li>\n<li>显示转换<ul>\n<li>强制类型转换：(double)</li>\n<li>命名的强制类型转换：cast-name&lt;type&gt;(expression)<ul>\n<li><p>static_cast：常用于 void* 的具体化，较大的算术类型降级。</p>\n</li>\n<li><p>dynamic_cast：支持运行时类型识别。详细见 19.2 节。</p>\n</li>\n<li><p>const_cast：将 const 对象转换为非 const 对象，常常用于有函数重载的上下文。</p>\n  <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/const_cast%E5%9C%A8%E5%87%BD%E6%95%B0%E9%87%8D%E8%BD%BD%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.jpg\" class=\"\">\n</li>\n<li><p>reinterpret_cast：<strong>最好别用</strong>。</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"5-语句\"><a href=\"#5-语句\" class=\"headerlink\" title=\"5 语句\"></a>5 语句</h2><h2 id=\"6-函数\"><a href=\"#6-函数\" class=\"headerlink\" title=\"6 函数\"></a>6 函数</h2><h2 id=\"7-类\"><a href=\"#7-类\" class=\"headerlink\" title=\"7 类\"></a>7 类</h2><p>一个好的习惯是：将类中的成员函数声明和定义分离，并尽量设置类的成员变量为私有。如此一来，用户代码只能通过声明的接口对类进行操作，类的实现者对类接口的修改不会影响到用户代码。</p>\n<img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/%E5%B0%86%E7%B1%BB%E7%9A%84%E5%A3%B0%E6%98%8E%E5%92%8C%E5%AE%9A%E4%B9%89%E5%88%86%E7%A6%BB.jpg\" class=\"\">\n\n<p>此时，将类以及与该类相关的友元接口声明在 <code>*.h</code> 文件中，并暴露给用户代码，即用户代码只需 <code>include &quot;*.h&quot;</code> 即可；类及相关友元接口的实现则定义在 <code>*.cpp</code> 中。</p>\n<h3 id=\"7-3-类的其它特性\"><a href=\"#7-3-类的其它特性\" class=\"headerlink\" title=\"7.3 类的其它特性\"></a>7.3 类的其它特性</h3><ol>\n<li><p>mutable 数据成员可以被 const 成员函数修改。</p>\n <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/mutable.jpg\" class=\"\" title=\"mutable\"></li>\n</ol>\n<h3 id=\"7-5-再探构造函数\"><a href=\"#7-5-再探构造函数\" class=\"headerlink\" title=\"7.5 再探构造函数\"></a>7.5 再探构造函数</h3><h4 id=\"7-5-3-使用默认构造函数\"><a href=\"#7-5-3-使用默认构造函数\" class=\"headerlink\" title=\"7.5.3 使用默认构造函数\"></a>7.5.3 使用默认构造函数</h4><p><code>Sales_data obj;</code> √</p>\n<p><code>Sales_data obj();</code> ×，这是在声明函数。</p>\n<h4 id=\"7-5-4-隐式的类类型转换\"><a href=\"#7-5-4-隐式的类类型转换\" class=\"headerlink\" title=\"7.5.4 隐式的类类型转换\"></a>7.5.4 隐式的类类型转换</h4><ol>\n<li><p>只接受一个实参的构造函数，也叫做<strong>转换构造函数</strong>。</p>\n<p> <code>Sales_data obj = string(&quot;123&quot;);</code></p>\n</li>\n<li><p>但是，只允许<strong>一步类类型转化</strong>，比如 <code>Sales_data obj = &quot;123&quot;；</code> 就不被允许。</p>\n</li>\n<li><p>抑制构造函数定义的隐式转换：使用 <code>explicit</code> 关键字</p>\n <img src=\"/2023/03/05/C++%20primer%E7%AC%94%E8%AE%B0/explicit.jpg\" class=\"\" title=\"explicit\"></li>\n</ol>\n<h2 id=\"8-IO-库\"><a href=\"#8-IO-库\" class=\"headerlink\" title=\"8 IO 库\"></a>8 IO 库</h2><h2 id=\"9-顺序容器\"><a href=\"#9-顺序容器\" class=\"headerlink\" title=\"9 顺序容器\"></a>9 顺序容器</h2><h2 id=\"10-泛型\"><a href=\"#10-泛型\" class=\"headerlink\" title=\"10 泛型\"></a>10 泛型</h2>","categories":["C/C++"],"tags":["C/C++"]},{"title":"CMU15445-Lab","url":"/2023/03/14/CMU15445-Lab/","content":"<h1 id=\"CMU15445-Fall2022\"><a href=\"#CMU15445-Fall2022\" class=\"headerlink\" title=\"CMU15445 Fall2022\"></a>CMU15445 Fall2022</h1><h2 id=\"HW1-SQL\"><a href=\"#HW1-SQL\" class=\"headerlink\" title=\"HW1 SQL\"></a>HW1 SQL</h2><h3 id=\"sqlite3-基础\"><a href=\"#sqlite3-基础\" class=\"headerlink\" title=\"sqlite3 基础\"></a>sqlite3 基础</h3><ol>\n<li>命令行键入 <code>sqlite3 db_name.db</code> 进入数据库；</li>\n<li><code>.tables</code> 展示检查数据库内容；</li>\n<li><code>.scheme table_name</code> 熟悉表的模式（结构）（包含什么属性、索引，主键和外键是什么）；</li>\n</ol>\n<h3 id=\"实验问题\"><a href=\"#实验问题\" class=\"headerlink\" title=\"实验问题\"></a>实验问题</h3><ol>\n<li>q2：使用 <code>||</code> 拼接两个字符串；</li>\n<li>q3：使用 <code>data(&#39;now&#39;)</code> 返回当前时间，<code>substr()</code> 截取子字符串；</li>\n<li>q6：sql 子查询似乎很慢…</li>\n<li>q9：<a href=\"https://www.sqlitetutorial.net/sqlite-window-functions/sqlite-ntile/\">NTILE()</a>；with 视图；</li>\n<li>q10：<a href=\"https://sqlite.org/lang_with.html\">Recursive CTEs</a>；窗口函数 <code>ROW_NUMBER()</code>；</li>\n</ol>\n<h2 id=\"Project1-Storage-Manger\"><a href=\"#Project1-Storage-Manger\" class=\"headerlink\" title=\"Project1 Storage Manger\"></a>Project1 Storage Manger</h2><h3 id=\"实验问题-1\"><a href=\"#实验问题-1\" class=\"headerlink\" title=\"实验问题\"></a>实验问题</h3><h4 id=\"Extendible-Hash-Table\"><a href=\"#Extendible-Hash-Table\" class=\"headerlink\" title=\"Extendible Hash Table\"></a><a href=\"https://15445.courses.cs.cmu.edu/fall2022/project1/#extendible-hash-table\"><strong>Extendible Hash Table</strong></a></h4><p>这是本实验的第一个任务，实现一个可扩展的哈希表，这里的可扩展只需要考虑根据需求增加哈希表的大小，而不需要考虑缩小或压缩。其中哈希表的 <code>Find(k, v)</code> 和 <code>Remove(k)</code> 接口实现相对比较容易，比较麻烦的是 <code>Insert(k, v)</code> 接口，该接口需要考虑扩展哈希表以及为部分键值对重新分配哈希桶的问题。在此之前需要明确几个概念，</p>\n<ol>\n<li>哈希表的全局深度：整个哈希表用到了多少 bit 将 key 映射到某个哈希桶；</li>\n<li>哈希表中的某个桶的局部深度：该桶用到了多少个 bit 将 key 映射到该桶；</li>\n<li>重排哈希桶：当某个哈希桶满时，将新增哈希桶，并利用更多的 bit 将 key 映射到具体哈希桶中，从而减小哈希冲突（哈希桶的最大大小固定）。</li>\n</ol>\n<p>从全局深度和局部深度的概念中不难得到<code>gloabl_depth &gt;= max(local_depth_1, ...)</code>，因此，当想要插入新数据时，但新数据映射到的哈希桶满了，则</p>\n<ol>\n<li><p>若 <code>gloabl_depth = local_depth</code>，那么很遗憾，即使是使用了 <code>gloabl_depth</code> 个 bit 去映射，这个数据映射到的哈希桶仍然是满的，因此毫无疑问，我们需要利用到更多的 bit，以减弱哈希冲突，即 <code>gloabl_depth++</code>，相应的，哈希桶的数量应该翻倍。在这里引入了一个新问题，那就是我们是否需要立即分配一倍的哈希桶空间？这似乎有点像操作系统中的 <code>COW（Copy On Write）</code> 问题，我们这个时候只是一个哈希桶出现了冲突，而如果直接暴力地分配一倍的哈希桶空间，可能是浪费的，那么，如果不立即分配哈希桶，我们增加的 ”哈希桶“ 该指向哪里呢？试想一下，新增哈希桶是因为现在将利用更多的 bit 来进行映射，那么在没有利用更多 bit 映射之前，这些 key 将映射到哪个地方呢？</p>\n <figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (global_depth_ == <span class=\"built_in\">GetLocalDepthInternal</span>(idx)) &#123;  <span class=\"comment\">// global depth is not enough, need more bit to hash</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; (<span class=\"number\">1</span> &lt;&lt; global_depth_); i++) &#123;</span><br><span class=\"line\">    \tdir_.<span class=\"built_in\">push_back</span>(dir_[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    global_depth_++;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>当前已满的哈希桶局部深度 <code>local_depth</code> 应该 +1，这是因为使用原 <code>local_depth</code> 的 bit 数量映射已经不足以解决该桶的哈希冲突了。</p>\n <figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">dir_[idx]-&gt;<span class=\"built_in\">IncrementDepth</span>();</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>最后则是重排哈希桶，此时冲突桶使用到了更多的 bit 来进行映射，此时即触发了一次 <code>COW</code> 操作，需要真正地进行分配新哈希桶空间了，并对冲突桶中已满的所有数据进行重排，以决定哪些留在原桶，哪些从原桶中删除并加入到新桶中。</p>\n <figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">template</span> &lt;<span class=\"keyword\">typename</span> K, <span class=\"keyword\">typename</span> V&gt;</span><br><span class=\"line\"><span class=\"keyword\">auto</span> ExtendibleHashTable&lt;K, V&gt;::<span class=\"built_in\">RedistributeBucket</span>(std::shared_ptr&lt;Bucket&gt; bucket, <span class=\"type\">size_t</span> idx)</span><br><span class=\"line\">    -&gt; std::shared_ptr&lt;Bucket&gt; &#123;</span><br><span class=\"line\">  std::shared_ptr&lt;Bucket&gt; new_bucket = std::<span class=\"built_in\">make_shared</span>&lt;Bucket&gt;(<span class=\"built_in\">Bucket</span>(bucket_size_, bucket-&gt;<span class=\"built_in\">GetDepth</span>()));</span><br><span class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> it = bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">begin</span>(); it != bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">end</span>();) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"built_in\">IndexOf</span>(it-&gt;first, bucket-&gt;<span class=\"built_in\">GetDepth</span>()) == idx) &#123;</span><br><span class=\"line\">      it++;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      new_bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">push_back</span>(&#123;it-&gt;first, it-&gt;second&#125;);</span><br><span class=\"line\">      it = bucket-&gt;<span class=\"built_in\">GetItems</span>().<span class=\"built_in\">erase</span>(it);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> new_bucket;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p> 再将指向新桶的指针加入到哈希表 <code>directory</code> 中的正确位置。<strong>值得注意的是，新桶指针可能会覆盖 directory 中的多个位置，具体数量与 <code>gloabl_depth_ - local_depth_</code> 的值有关。</strong></p>\n</li>\n<li><p>此时哈希冲突可能解决了，也可能发生巧合，就是原来已满的哈希桶中的所有数据使用多一 bit 映射时仍然映射到同一位置，因此需要递归进行 <code>Insert(k, v)</code>。</p>\n</li>\n</ol>\n<h4 id=\"LRU-K-Replacement-Policy\"><a href=\"#LRU-K-Replacement-Policy\" class=\"headerlink\" title=\"LRU-K Replacement Policy\"></a><a href=\"https://15445.courses.cs.cmu.edu/fall2022/project1/#lru-k-replacer\"><strong>LRU-K Replacement Policy</strong></a></h4><p>LRU-K 缓存替换算法，问题的关键是设计相关的数据结构。<code>time_map_</code> 之所以需要设置成 <code>multimap</code>，是因为缓存中后向k重用距离为 <code> inf</code> 的对象可能会有多个。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// map frame_id_t to the queue of its k-access timeStamps</span></span><br><span class=\"line\">std::unordered_map&lt;<span class=\"type\">frame_id_t</span>, std::queue&lt;<span class=\"type\">size_t</span>&gt;&gt; refs_map_;</span><br><span class=\"line\"><span class=\"comment\">// real cache set, map frame_id to its iterator in time_map_</span></span><br><span class=\"line\">std::unordered_map&lt;<span class=\"type\">frame_id_t</span>, std::multimap&lt;<span class=\"type\">size_t</span>, <span class=\"type\">frame_id_t</span>&gt;::iterator&gt; cache_map_;</span><br><span class=\"line\"><span class=\"comment\">// multimap timeStamp to frame_id_t, its size should equal cache_map_&#x27;s size</span></span><br><span class=\"line\">std::multimap&lt;<span class=\"type\">size_t</span>, <span class=\"type\">frame_id_t</span>&gt; time_map_;</span><br><span class=\"line\"><span class=\"comment\">// the set contain all evictable frame_id</span></span><br><span class=\"line\">std::unordered_set&lt;<span class=\"type\">frame_id_t</span>&gt; evictable_frame_id_set_;</span><br></pre></td></tr></table></figure>\n\n<p><strong>踩坑：<code>SetEvictable(frame_id_t frame_id, bool set_evictable)</code> 时如果 frame_id 不在缓存中时，直接 return 就可以了。</strong></p>\n<h4 id=\"Buffer-Pool-Manager-Instance\"><a href=\"#Buffer-Pool-Manager-Instance\" class=\"headerlink\" title=\"Buffer Pool Manager Instance\"></a><a href=\"https://15445.courses.cs.cmu.edu/fall2022/project1/#buffer-pool-instance\"><strong>Buffer Pool Manager Instance</strong></a></h4><p>使用前两步完成的扩展哈希表和 LRU-K 缓存替换算法，构建一个缓冲池，相关接口的注释都比较完整。</p>\n<ol>\n<li><code>NewPgImp(page_id_t)</code> 和 <code>FetchPgImp(page_id_t)</code> 接口实现时一定要记得显示地进行 <code>pages_[frame_id].pin_count++</code> 操作，而不仅仅是执行 <code>replacer.SetEvictable(frame_id, false)</code>。</li>\n<li><code>UnpinPgImp</code> 中 is_dirty： <code>pages_[frame_id].is_dirty_ |= is_dirty</code>。</li>\n<li><code>DeletePgImp</code> 需要检查删除的页在 buffer pool 中是否是脏的，如若是脏还需记得刷盘。</li>\n</ol>\n<h3 id=\"感受\"><a href=\"#感受\" class=\"headerlink\" title=\"感受\"></a>感受</h3><p>人麻了，刚写完代码本地测完感觉很好，线上测试狂冒红光，为了过线上测试花了一天。。。</p>\n<p>（很多奇怪的错误可能是由于锁的粒度太细了，用大锁就OK）</p>\n<h2 id=\"Project2-B-Tree-Index\"><a href=\"#Project2-B-Tree-Index\" class=\"headerlink\" title=\"Project2 B+Tree Index\"></a>Project2 B+Tree Index</h2><h3 id=\"实验问题-2\"><a href=\"#实验问题-2\" class=\"headerlink\" title=\"实验问题\"></a>实验问题</h3><h4 id=\"task1：B-Tree-Pages\"><a href=\"#task1：B-Tree-Pages\" class=\"headerlink\" title=\"task1：B+Tree Pages\"></a>task1：B+Tree Pages</h4><ol>\n<li>internal node 和 leaf node 能容纳的 K&#x2F;V 键值对数目不同。internal node 中的 K&#x2F;V 对中的 V 是 page_id，而 leaf node 中的 K&#x2F;V 对中的 V 是 rid（{page_id, slot_num}），同时 leaf node 还包含额外元数据（next_page_id_）；</li>\n<li>B+Tree 非叶子节点的 <code>min_size = (max_size + 1)/2</code>，叶子节点的 <code>min_size = max_size / 2</code>；</li>\n<li>叶子节点插入后的键值对数量达到 max_size_ 则拆分；非叶子节点插入前的 child 数量为 max_size 则拆分；</li>\n</ol>\n<h4 id=\"task2：B-Tree-Data-Structure\"><a href=\"#task2：B-Tree-Data-Structure\" class=\"headerlink\" title=\"task2：B+Tree Data Structure\"></a>task2：B+Tree Data Structure</h4><ol>\n<li><p>Search</p>\n <img src=\"/2023/03/14/CMU15445-Lab/B+Tree_search.jpg\" class=\"\" title=\"B+Tree_search\">\n\n<ul>\n<li><p>需要注意图中的 <code>/*C是叶结点*/</code> 应该和 while 循环是同一缩进；</p>\n</li>\n<li><p>原书中的 非叶子结点 K&#x2F;V 键值对是如下组织的，忽略最后一个 K，因此和实验中忽略第一个 K 的组织方式略有不同，最终会导致取子树 index 时出现加一减一之类的细微区别（寻找非叶子节点中的第一个严格大于查询键的 Key 的索引更简洁）。</p>\n  <img src=\"/2023/03/14/CMU15445-Lab/node.jpg\" class=\"\" title=\"node\"></li>\n</ul>\n</li>\n</ol>\n","categories":["数据库"],"tags":["CMU15445","公开课","数据库"]},{"title":"CMU15445-Lecture","url":"/2023/03/14/CMU15445-Lecture/","content":"<h1 id=\"Lec1-关系模型\"><a href=\"#Lec1-关系模型\" class=\"headerlink\" title=\"Lec1 关系模型\"></a>Lec1 关系模型</h1><ol>\n<li><p>每个关系（Relation）都是一个无序集合，也叫数据库表，集合中每个元素都是一个元组（tuple），每个 tuple 由一组属性构成，这些属性在逻辑上通常有内在联系。</p>\n</li>\n<li><p>主键（primary key）在一个关系中唯一确定一个 tuple。</p>\n</li>\n<li><p>外键（foreign key）唯一确定另一个关系中的一个 tuple。</p>\n</li>\n<li><p>Data Manipulation Language（DML），数据操作语言，比如增删改查。</p>\n</li>\n<li><p>Data Definition Language（DDL），对数据结构进行修改的语言，比如加索引，建表等。</p>\n</li>\n<li><p>在关系模型中查找和查询数据的两种方式，使用哪种方式是具体的实现问题，与 Relational Model 本身无关。</p>\n<ul>\n<li>Procedural：查询命令需要指定 DBMS 执行时的具体查询策略，如关系代数（Relational Algebra），会因操作步骤影响执行效率；</li>\n<li>Non-Procedural：查询命令只需要指定想要查询哪些数据，无需关心幕后的故事，如 SQL。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Lec2-SQL-进阶\"><a href=\"#Lec2-SQL-进阶\" class=\"headerlink\" title=\"Lec2 SQL 进阶\"></a>Lec2 SQL 进阶</h1><h2 id=\"SQL-历史\"><a href=\"#SQL-历史\" class=\"headerlink\" title=\"SQL 历史\"></a>SQL 历史</h2><p>Structured Query Language，结构化查询语言。</p>\n<h2 id=\"SQL-特性\"><a href=\"#SQL-特性\" class=\"headerlink\" title=\"SQL 特性\"></a>SQL 特性</h2><ol>\n<li>Aggregates：聚类函数，通常可以将查询结果聚合成一个值。<ul>\n<li>AVG(<distinct> col)</li>\n<li>MIN(col)</li>\n<li>MAX(col)</li>\n<li>SUM(<distinct> col)</li>\n<li>COUNT(<distinct> col)</li>\n</ul>\n</li>\n<li>Group By：group by 就是把记录按某种方式分成多组，对每组记录分别做 aggregates 操作。<strong>所有非 aggregates 操作的字段，都必须出现在 group by 语句</strong>。</li>\n<li>Having：基于 aggregation 结果的过滤条件不能写在 WHERE 中，而应放在 HAVING 中，从而对 group by 结果进行进一步筛选。</li>\n<li>Output Control：<ul>\n<li><code>ORDER BY &lt;column*&gt; [ASC|DESC]</code></li>\n<li><code>LIMIT &lt;count&gt; [offset]</code></li>\n</ul>\n</li>\n<li>Common Table Expressions：with 视图（虚拟中间表）。</li>\n<li>Nested Queries：子查询。</li>\n<li>String Operations<ul>\n<li>string match</li>\n<li>string functions</li>\n</ul>\n</li>\n<li>Window Functions：<code>ROW_NUMBER() OVER(&lt;PARTITION BY ...&gt;)</code></li>\n<li>Date&#x2F;Time Operations</li>\n</ol>\n<h1 id=\"Lec3-amp-4-Database-Storage\"><a href=\"#Lec3-amp-4-Database-Storage\" class=\"headerlink\" title=\"Lec3&amp;4 Database Storage\"></a>Lec3&amp;4 Database Storage</h1><h2 id=\"Disk-Manager\"><a href=\"#Disk-Manager\" class=\"headerlink\" title=\"Disk Manager\"></a>Disk Manager</h2><ol>\n<li>为什么不适用 OS 磁盘管理的轮子？主要原因在于，OS 的磁盘管理模块并没有、也不可能会有 DBMS 中的领域知识，因此 DBMS 比 OS 拥有更多、更充分的知识来决定数据移动的时机和数量（OS 只是提供一种普适化的接口，但现在既然具体化到了数据库的场景，那么我们在拥有这种特定场景下的负载特征，自然就可以特制化一些更优秀的策略），具体包括：<ul>\n<li>Flushing dirty pages to disk in the correct order</li>\n<li>Specialized prefetching</li>\n<li>Buffer replacement policy</li>\n<li>Thread&#x2F;process scheduling</li>\n</ul>\n</li>\n<li>DBMS 的磁盘模块主要解决两个问题：<ul>\n<li>How the DBMS represents the database in files on disk（如何使用磁盘文件来表示数据库的数据，比如元数据、索引、数据表等）</li>\n<li>How the DBMS manages its memory and moves data back-and-forth from disk（如何管理数据在内存与磁盘之间的移动）</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"在磁盘文件中表示数据库\"><a href=\"#在磁盘文件中表示数据库\" class=\"headerlink\" title=\"在磁盘文件中表示数据库\"></a>在磁盘文件中表示数据库</h2><h3 id=\"File-Storage\"><a href=\"#File-Storage\" class=\"headerlink\" title=\"File Storage\"></a>File Storage</h3><p>DBMS 通常将自己的所有数据作为一个或多个文件存储在磁盘中，而 OS 只当它们是普通文件，并不知道如何解读这些文件。</p>\n<ol>\n<li>Storage Manager：存储管理器负责维护数据库文件，为读写操作安排合适的调度，从而改善页面的时间和空间局部性</li>\n<li>Database Pages：一个页是固定大小的数据块，每个 page 内部可能存储着 tuples、meta-data、indexes 以及 logs 等等，大多数 DBMS 不会把不同类型数据存储在同一个 page 上。每个 page 带着一个唯一的 id，DBMS 使用一个 indirection layer 将 page id 与数据实际存储的物理位置关联起来。</li>\n<li>Database Heap：有几种方法可以找到 DBMS 想要的页面在磁盘和堆文件中的位置，heap 就是其中一种方式。堆文件是页面的无序集合，以随机顺序存储元组，也就是说我们保存的数据无须按照我们插入时的顺序进行保存。（Linked List 和 Page Directory）。</li>\n</ol>\n<h3 id=\"Page-Layout\"><a href=\"#Page-Layout\" class=\"headerlink\" title=\"Page Layout\"></a>Page Layout</h3><img src=\"/2023/03/14/CMU15445-Lecture/page_layout.jpg\" class=\"\" title=\"page_layout\">\n\n<p>每个 page 被分为两个部分：header 和 data。</p>\n<p>header 中通常包含以下信息：</p>\n<ul>\n<li>Page Size</li>\n<li>Checksum</li>\n<li>DBMS Version</li>\n<li>Transaction Visibility</li>\n<li>Compression Information</li>\n</ul>\n<p>data 中存储真正的数据，包含数据本身和数据的操作日志：Tuple-oriented 和 Log-structured</p>\n<ol>\n<li><p>Tuple-oriented：</p>\n<ul>\n<li><p>Strawman Idea：在 header 中记录 tuple 的个数，然后不断的往下 append 即可。这种方法有两种明显问题，（1）一旦出现删除操作，每次插入就需要遍历一遍，寻找空位，否则就会出现碎片；（2）无法处理变长的数据记录（tuple）</p>\n  <img src=\"/2023/03/14/CMU15445-Lecture/Strawman.jpg\" class=\"\" title=\"Strawman\">\n</li>\n<li><p>Slotted Pages：常用。</p>\n  <img src=\"/2023/03/14/CMU15445-Lecture/slot_page.jpg\" class=\"\" title=\"slot_page\"></li>\n</ul>\n</li>\n<li><p>Log-structured：KV 数据库中好用，因为 KV 数据库中每个键只有一个值，因此只需从日志栈顶往下找第一个 update 就行了，而关系数据库有多个字段，只找到一个 update 可能是不够的。</p>\n<ul>\n<li><p>Slotted-Page Design 存在一些问题：（1）碎片化：删除元组会在页面中留下空白（即时能够迁移也需要额外开销）；（2）无用的磁盘 I&#x2F;O：由于非易失性存储的面向块的特性，整个块需要被读取以获取元组；（3）随机磁盘 I&#x2F;O：磁盘读取器可能必须跳转到 20 个不同的位置才能更新 20 个不同的元组，这非常慢。因此日志结构存储模型仅允许创建新数据而不允许覆盖，解决了上面列出的一些问题。</p>\n</li>\n<li><p>写入速度快，读取速度可能较慢。磁盘写入是顺序的，现有页面是不可变的，导致随机磁盘 I&#x2F;O 减少。</p>\n</li>\n<li><p>为了加快读取，可以创建索引以跳转到日志中的一些特定位置。</p>\n</li>\n<li><p>必要时需要压缩日志。</p>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"Tuple-Layout\"><a href=\"#Tuple-Layout\" class=\"headerlink\" title=\"Tuple Layout\"></a>Tuple Layout</h3><p>tuple-oriented 中的数据本身也有别的元信息，比如：</p>\n<ul>\n<li>Visibility information for the DBMS’s concurrency control protocol (i.e., information about which transaction created&#x2F;modified that tuple).</li>\n<li>Bit Map for NULL values. </li>\n<li>Note that the DBMS does not need to store meta-data about the schema of the database here.</li>\n</ul>\n<p>tuple data 中的属性通常按照创建表时指定的顺序存储。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/tuple_data.jpg\" class=\"\" title=\"tuple_data\">\n\n<p>数据库中的每个元组都分配有一个唯一标识符，最常见的是 <code>page id + (offset or slot)</code>，DBMS 可以创建一个 map，将不同的 tuple 映射到 <code>page id + (offset or slot)</code>。</p>\n<h3 id=\"Tuple-Storage\"><a href=\"#Tuple-Storage\" class=\"headerlink\" title=\"Tuple Storage\"></a>Tuple Storage</h3><p>元组中的数据本质上只是字节数组。由 DBMS 决定如何解释这些字节以导出属性的值。数据表示方案是 DBMS 如何存储这些字节。有五种高级数据类型可以存储在元组中：integers, variable-precision numbers, fixedpoint precision numbers, variable length values, and dates&#x2F;times。</p>\n<p>为了让 DBMS 能够破译元组的内容，它维护了一个内部目录（catelogs）来描述数据库的元数据。这些元数据包括数据库包含的表和列，以及列中的属性的类型以及列的顺序。大多数 DBMS 同样以表的格式将它们的 catelogs 存储在自身内部。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ol>\n<li>database is organized in pages;</li>\n<li>different ways to track pages：Heap File Organization…;</li>\n<li>different ways to store pages：Linked List 和 Page Directory;</li>\n<li>different ways to store tuples：Strawman，slotted 和 Log-structured;</li>\n</ol>\n<h1 id=\"Lec5-Storage-Models-amp-Compression\"><a href=\"#Lec5-Storage-Models-amp-Compression\" class=\"headerlink\" title=\"Lec5 Storage Models &amp; Compression\"></a>Lec5 Storage Models &amp; Compression</h1><h2 id=\"Database-Workload\"><a href=\"#Database-Workload\" class=\"headerlink\" title=\"Database Workload\"></a>Database Workload</h2><ol>\n<li>OLTP：Online Transaction Processing，运行速度快、运行时间短、对单个实体进行简单查询。OLTP 工作负载通常会处理更多的写入操作。</li>\n<li>OLAP：Online Analytical Processing，长时间运行、复杂查询、读取数据库中的大部分数据。在 OLAP 工作负载中，数据库系统通过分析大量现有数据，并期望从中获得结论。</li>\n<li>HTAP：Hybrid Transaction + Analytical Processing，尝试在同一个数据库上同时执行 OLTP 和 OLAP 的组合。</li>\n</ol>\n<img src=\"/2023/03/14/CMU15445-Lecture/workloads.jpg\" class=\"\" title=\"workloads\">\n\n<h2 id=\"Storage-Models\"><a href=\"#Storage-Models\" class=\"headerlink\" title=\"Storage Models\"></a>Storage Models</h2><h3 id=\"N-Ary-Storage-Model-NSM\"><a href=\"#N-Ary-Storage-Model-NSM\" class=\"headerlink\" title=\"N-Ary Storage Model (NSM)\"></a>N-Ary Storage Model (NSM)</h3><p>将单个元组的所有属性连续存储在单个页中，这种方法非常适合 OLTP 工作负载，其中请求插入量大且事务倾向于操作单个实体。这是理想的，因为只需要一次读取就可以获取单个元组的所有属性。</p>\n<ol>\n<li>优点<ul>\n<li>快速插入、更新和删除。</li>\n<li>适用于需要整个元组的查询。</li>\n</ul>\n</li>\n<li>缺点：不适合检索表的大部分元组，或者只需要查找小部分的属性。</li>\n</ol>\n<h3 id=\"Decomposition-Storage-Model-DSM\"><a href=\"#Decomposition-Storage-Model-DSM\" class=\"headerlink\" title=\"Decomposition Storage Model (DSM)\"></a>Decomposition Storage Model (DSM)</h3><p>将所有元组的单个属性连续地存储在一个 page 中，这种存储方式特别适用于 OLAP 场景。</p>\n<ol>\n<li>优点<ul>\n<li>减少了浪费的 I&#x2F;O 量，因为 DBMS 只读取该查询所需的数据。</li>\n<li>更好的查询处理和数据压缩支持。</li>\n</ul>\n</li>\n<li>缺点<ul>\n<li>由于元组拆分&#x2F;拼接，点查询、插入、更新和删除速度较慢。</li>\n<li>引入新问题：如何跟踪每个元组的不同属性？（1）Fixed-length Offsets：每个属性都是定长的，直接靠 offset 来跟踪（常用），但是不灵活；（2）Embedded Tuple Ids：在每个属性前面都加上 tupleID，但是需要额外的存储开销。</li>\n</ul>\n</li>\n</ol>\n<p><strong>chose right storage model for the ：OLAP（column store） 和 OLTP（row store）。</strong></p>\n<h2 id=\"Database-Compression\"><a href=\"#Database-Compression\" class=\"headerlink\" title=\"Database Compression\"></a>Database Compression</h2><p>压缩对于基于磁盘的 DBMS 来说非常重要，因为磁盘 I&#x2F;O 几乎总是主要的瓶颈；DBMS 可以通过压缩页面以提高每个 I&#x2F;O 操作能够移动的数据有效量，尤其是在只读分析工作负载中。</p>\n<p>如果数据集是完全随机的，那么将没有办法进行压缩。但是，真实世界中的数据集往往存在 key insight，比如：</p>\n<ol>\n<li>数据集往往具有高度偏斜的属性值分布（例如，齐夫分布）</li>\n<li>同一元组的属性之间具有高度相关性（例如，邮政编码和城市，订购日期和发货日期）。</li>\n</ol>\n<p>基于此，我们希望数据库压缩方案具有如下性质：</p>\n<ol>\n<li>必须产生固定长度的值。唯一的例外是存储在单独池中的变长数据，这是因为 DBMS 应该遵循字对齐并能够使用偏移量访问数据。</li>\n<li>允许 DBMS 在查询执行期间尽可能长时间地推迟解压缩（延迟实现）。</li>\n<li>必须是无损方案，因为人们不喜欢丢失数据。任何一种有损压缩都必须在应用程序层级执行。</li>\n</ol>\n<p>在向 DBMS 添加压缩功能之前，我们需要确定要压缩的数据类型，这决定了压缩方案是否可用。压缩粒度有四个级别：</p>\n<ol>\n<li>块级：压缩一个块，这个块中的所有元组隶属于同一张表。</li>\n<li>元组级别：压缩整个元组的内容（仅限 NSM）。</li>\n<li>属性级别：在一个元组中压缩单个属性值。可以针对同一个元组的多个属性。</li>\n<li>列级：为多个元组压缩一个或多个属性值（仅限 DSM）。这允许更复杂的压缩方案。</li>\n</ol>\n<h2 id=\"Naive-Compression\"><a href=\"#Naive-Compression\" class=\"headerlink\" title=\"Naive Compression\"></a>Naive Compression</h2><p>DBMS 使用通用算法（例如 gzip、LZO、LZ4、Snappy、Brotli、Oracle OZIP，Zstd）。尽管 DBMS 可以使用多种压缩算法，但工程师经常选择具有较低压缩率的压缩算法以换取更快的压缩&#x2F;解压缩速度。</p>\n<p>MySQL InnoDB 中有一个使用简单压缩的示例。DBMS 压缩磁盘页、往里面填充 2 的幂次方 KB 并将它们存储到缓冲池中。然而，每次 DBMS 尝试读取数据时，缓冲池中的压缩数据需要解压。</p>\n<p>由于访问数据需要对压缩数据进行解压，这就限制了压缩机制的范围。如果目标是将整个表压缩成一个巨大的块，使用简单的压缩方案<br>这是不可能的，因为每次访问都需要压缩&#x2F;解压缩整个表。因此，对于 MySQL，由于压缩范围有限，它将表分成更小的块。</p>\n<p>另一个问题是这些简单的方案也没有考虑数据的高级特征或语义。这些算法既不关心数据的结构，也不关心查询计划如何访问数据。因此，这放弃了利用 <code>late materialization</code> 的机会，因为这样的话 DBMS 无法知道什么时候可以延迟数据的解压缩。</p>\n<h2 id=\"Columnar-Compression\"><a href=\"#Columnar-Compression\" class=\"headerlink\" title=\"Columnar Compression\"></a>Columnar Compression</h2><h3 id=\"Run-length-Encoding\"><a href=\"#Run-length-Encoding\" class=\"headerlink\" title=\"Run-length Encoding\"></a>Run-length Encoding</h3><h3 id=\"Bit-Packing-Encoding\"><a href=\"#Bit-Packing-Encoding\" class=\"headerlink\" title=\"Bit-Packing Encoding\"></a>Bit-Packing Encoding</h3><h3 id=\"Bitmap-Encoding\"><a href=\"#Bitmap-Encoding\" class=\"headerlink\" title=\"Bitmap Encoding\"></a>Bitmap Encoding</h3><h3 id=\"Delta-Encoding\"><a href=\"#Delta-Encoding\" class=\"headerlink\" title=\"Delta Encoding\"></a>Delta Encoding</h3><h3 id=\"Incremental-Encoding\"><a href=\"#Incremental-Encoding\" class=\"headerlink\" title=\"Incremental Encoding\"></a>Incremental Encoding</h3><h3 id=\"Dictionary-Encoding\"><a href=\"#Dictionary-Encoding\" class=\"headerlink\" title=\"Dictionary Encoding\"></a>Dictionary Encoding</h3><h1 id=\"Lec6-Memory-Management：Buffer-Pools\"><a href=\"#Lec6-Memory-Management：Buffer-Pools\" class=\"headerlink\" title=\"Lec6 Memory Management：Buffer Pools\"></a>Lec6 Memory Management：Buffer Pools</h1><p>DBMS 的磁盘管理模块主要解决两个问题，一个是在 Lec3 &amp; Lec4 中提到的<strong>如何使用磁盘文件来表示数据库的数据（元数据、索引、数据表等）</strong>，另一个就是本节将介绍的<strong>如何管理数据在内存与磁盘之间的移动</strong>。</p>\n<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>DBMS 负责管理其内存并从磁盘来回移动数据。因为，对于大多数情况，数据不能直接在磁盘上操作，任何数据库都必须能够高效地移动以文件形式表示在磁盘上的数据，将它们装入内存，以便可以使用。</p>\n<p>DBMS 面临的问题是将移动数据的延迟降到最低。理想情况下，延迟应该是0，也就是用户使用数据之前，数据已经在内存中了（预取）。而执行引擎不必考虑数据是如何被预取到内存的（内存管理器考虑）。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/data_move.jpg\" class=\"\" title=\"data_move\">\n\n<p>考虑这个问题的另一种方法是划分为空间和时间控制。</p>\n<ol>\n<li>空间控制策略通过决定将 pages 写到磁盘的哪个位置，使得常常一起使用的 pages 在磁盘的物理距离更近，从而提高 I&#x2F;O 效率。</li>\n<li>时间控制策略通过决定何时将 pages 读入内存，写回磁盘，使得磁盘读写的次数最少，从而提高 I&#x2F;O 效率。</li>\n</ol>\n<h2 id=\"Locks-vs-Latches\"><a href=\"#Locks-vs-Latches\" class=\"headerlink\" title=\"Locks vs. Latches\"></a>Locks vs. Latches</h2><p>在讨论 DBMS 如何保护其内部元素时，我们需要分清 <code>lock</code> 和 <code>latch</code>。</p>\n<p><strong>lock</strong>：lock 是一种更高层级别的逻辑原语，用于保护一个事务中数据库的内容（例如，元组、表、数据库）。事务将在其整个持续时间内持有锁。数据库系统可以向用户公开运行查询时持有哪些 lock。此外，lock 需要能够支持回滚修改。</p>\n<p><strong>latch</strong>：latch 是一种低层级的保护原语，DBMS 在其内部数据结构（例如哈希表、内存区域）中使用以保护临界区。latch 仅在执行操作期间保持。latch 不需要支持回滚修改。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/lock_latch.png\" class=\"\" title=\"lock_latch\">\n\n<h2 id=\"Buffer-Pool\"><a href=\"#Buffer-Pool\" class=\"headerlink\" title=\"Buffer Pool\"></a>Buffer Pool</h2><p>缓冲池本质上是数据库内部分配的一个大内存区域，用于存储从磁盘获取的页面。</p>\n<p>DBMS 启动时会从 OS 申请一片内存区域，即 Buffer Pool，并将这块区域划分成大小相同的 pages，为了与 disk pages 区别，通常称为 frames。当用户请求一个页面时，DBMS 会先查询缓冲池，如果没有找到页面，则 DBMS 会进一步请求原始的 disk page ，并将该 disk page 复制到 Buffer Pool 的一个 frame 中。</p>\n<h3 id=\"Buffer-Pool-元数据\"><a href=\"#Buffer-Pool-元数据\" class=\"headerlink\" title=\"Buffer Pool 元数据\"></a>Buffer Pool 元数据</h3><p>缓冲池必须维护一定的元数据才能被高效且正确地使用。</p>\n<p>首先，<strong>page table</strong> 是一个内存中的哈希表，用于跟踪当前内存中的页面。它将 page id 映射到缓冲池中的帧位置。</p>\n<p>注意：不要将 page table 和 page directory 混淆，page directory 是将 page id 映射到该页在数据库文件中的位置。对页面目录的所有更改都必须记录在磁盘上，以便 DBMS 可以在重启后找到。</p>\n<p>page table 还维护每个页面的附加元数据、dirty flag 和引用计数器。</p>\n<ol>\n<li>dirty flag 由线程在修改页面时设置。这向存储管理器表明该页面已被修改，必须写回磁盘以持久化。</li>\n<li>引用计数器跟踪当前访问该页面（读取或修改它）的线程数。线程必须在访问页面之前递增计数器，如果页面的计数大于零，则<br> 不允许存储管理器从内存中逐出该页面。</li>\n</ol>\n<h3 id=\"Memory-Allocation-Policies\"><a href=\"#Memory-Allocation-Policies\" class=\"headerlink\" title=\"Memory Allocation Policies\"></a>Memory Allocation Policies</h3><p>缓冲池中的缓存有两种分配策略。</p>\n<p>全局策略：考虑所有活动事务以找到分配内存的最佳决策，以使正在执行的整个工作负载受益（比如最小化全局的磁盘读写次数，类似缓存分配&#x2F;分区策略中的 UCP，相对于动态地为缓冲池分区了）。</p>\n<p>局部策略：它做出的决策将使单个查询或事务运行得更快，即使这对整个工作负载不利。局部策略将帧分配给特定事务而不考虑正在并发进行的其它事务行为（first-in-first-out，对每个事务 ”屡求屡给“，也就相当于没有对缓冲池分区）。</p>\n<p>大多数系统结合使用全局视图和局部视图。</p>\n<h2 id=\"Buffer-Pool-优化\"><a href=\"#Buffer-Pool-优化\" class=\"headerlink\" title=\"Buffer Pool 优化\"></a>Buffer Pool 优化</h2><h3 id=\"Multiple-Buffer-Pools\"><a href=\"#Multiple-Buffer-Pools\" class=\"headerlink\" title=\"Multiple Buffer Pools\"></a>Multiple Buffer Pools</h3><p>DBMS 可以拥有多个缓冲池，比如为每个数据库设置一个缓冲池，为每种页面类型（data page、directory page…）设置一个缓冲池，从而每个缓冲池可以为特定的数据库、页面类型设置局部的缓存策略。与此同时，在处理并发事务时，也可以减少 latch 的争用。</p>\n<h3 id=\"Prefetching\"><a href=\"#Prefetching\" class=\"headerlink\" title=\"Prefetching\"></a>Prefetching</h3><p>缓存预取策略。比如在全局扫描查询中，可以预先将该表接下来将扫描的 page 预取到缓冲池中；将 B+ 树的根节点预取到缓冲池中。</p>\n<h3 id=\"Scan-Sharing-Synchronized-Scans\"><a href=\"#Scan-Sharing-Synchronized-Scans\" class=\"headerlink\" title=\"Scan Sharing (Synchronized Scans)\"></a>Scan Sharing (Synchronized Scans)</h3><p>Scan Sharing 技术主要用在多个查询存在数据共用的情况。当两个查询 A, B 先后发生，B 发现自己有一部分数据与 A 共用，于是先共用 A 的 cursor，等 A 扫完后，再扫描自己还需要的其它数据。</p>\n<h3 id=\"Buffer-Pool-Bypass\"><a href=\"#Buffer-Pool-Bypass\" class=\"headerlink\" title=\"Buffer Pool Bypass\"></a>Buffer Pool Bypass</h3><p>缓存准入策略。全表扫描的 page 可能无需加入缓冲池中，因为它们的时间局部性较差，加入缓存可能会驱逐缓冲池中其它一些局部性较好的页面。因此，DBMS 可能单独分配一块局部内存（Bypass，旁路），在该内存中的写入和处理不影响缓冲池中的其它数据。</p>\n<h2 id=\"OS-Page-Cache\"><a href=\"#OS-Page-Cache\" class=\"headerlink\" title=\"OS Page Cache\"></a>OS Page Cache</h2><p>大部分 disk operations 都是通过系统调用完成，通常系统会维护自身的数据缓存，这会导致一份数据分别在操作系统和 DMBS 中被缓存两次。大多数 DBMS 都会使用 (O_DIRECT) 来告诉 OS 不要缓存这些数据，除了 Postgres。</p>\n<h2 id=\"Buffer-Replacement-Policies\"><a href=\"#Buffer-Replacement-Policies\" class=\"headerlink\" title=\"Buffer Replacement Policies\"></a>Buffer Replacement Policies</h2><p>缓存替换策略。</p>\n<h3 id=\"localization\"><a href=\"#localization\" class=\"headerlink\" title=\"localization\"></a>localization</h3><p>每次事务查询只能驱逐有限的缓冲池页面，或者是拥有一份自己独享的缓冲池区域（缓存分区）。</p>\n<h3 id=\"priority-hints\"><a href=\"#priority-hints\" class=\"headerlink\" title=\"priority hints\"></a>priority hints</h3><p>允许事务告知缓冲池哪些页面非常重要，比如根据页面的具体内容，或者是类似于 B+ 树的根节点。</p>\n<h3 id=\"Dirty-Pages\"><a href=\"#Dirty-Pages\" class=\"headerlink\" title=\"Dirty Pages\"></a>Dirty Pages</h3><p>驱逐一个 dirty page 的成本要高于驱逐一般 page，因为前者需要写 disk，后者可以直接 drop，因此 DBMS 在驱逐缓冲池中的 page 时，需要权衡页面局部性和写回的开销。</p>\n<p>除了直接在 Replacement Policies 中考虑，有的 DBMS 使用 Background Writing 的方式来处理。它们定期扫描 page table，发现 dirty page 就写入 disk，在 Replacement 发生时就无需考虑脏数据带来的问题。</p>\n<h2 id=\"Other-Memory-Pools\"><a href=\"#Other-Memory-Pools\" class=\"headerlink\" title=\"Other Memory Pools\"></a>Other Memory Pools</h2><p>除了存储 tuples 和 indexes，DBMS 还需要 Memory Pools 来存储其它数据，如：</p>\n<ul>\n<li>Sorting + Join Buffers</li>\n<li>Query Caches</li>\n<li>Maintenance Buffers </li>\n<li>Log Buffers</li>\n<li>Dictionary Caches</li>\n</ul>\n<h1 id=\"Lec7-Hash-Tabels\"><a href=\"#Lec7-Hash-Tabels\" class=\"headerlink\" title=\"Lec7 Hash Tabels\"></a>Lec7 Hash Tabels</h1><h2 id=\"Data-Structures\"><a href=\"#Data-Structures\" class=\"headerlink\" title=\"Data Structures\"></a>Data Structures</h2><p>DBMS 为系统内部得不同部分设置了相应的数据结构，比如：</p>\n<ol>\n<li><strong>Internal Meta-Data</strong>：用于跟踪数据库和系统状态的数据，比如 page tables，page directories；</li>\n<li><strong>Core Data Storage</strong>：数据库中元组的基本数据结构；</li>\n<li><strong>Temporary Data Structures</strong>：DBMS 可以在处理查询的过程中临时构建数据结构从而加快执行速度（例如，用于连接的哈希表）；</li>\n<li><strong>Table Indexes</strong>：辅助数据结构，可以用来更容易地找到特定的元组。</li>\n</ol>\n<p>在为 DBMS 实现数据结构时，需要考虑两个主要的设计问题：</p>\n<ol>\n<li>数据组织：我们需要弄清楚内存如何布局，以及支持高效访问，里面存放什么信息。</li>\n<li>并发性：我们还需要考虑如何让多个线程访问数据结构，而不会引起问题。</li>\n</ol>\n<h2 id=\"Hash-Table\"><a href=\"#Hash-Table\" class=\"headerlink\" title=\"Hash Table\"></a>Hash Table</h2><p>哈希表实现了 associative array ADT（Abstract Data Type），将键映射到值。</p>\n<p>Hash Table 主要分为两部分：</p>\n<ol>\n<li><p>Hash Function：</p>\n<ul>\n<li>How to map a large key space into a smaller domain</li>\n<li>Trade-off between <strong>being fast</strong> vs <strong>collision rate</strong></li>\n</ul>\n</li>\n<li><p>Hashing Scheme：</p>\n<ul>\n<li>How to handle key collisions after hashing</li>\n<li>Trade-off between <strong>allocating a large hash table</strong> vs <strong>additional instructions to find&#x2F;insert keys</strong></li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Hash-Functions\"><a href=\"#Hash-Functions\" class=\"headerlink\" title=\"Hash Functions\"></a>Hash Functions</h2><p>由于 DBMS 内使用的 Hash Function  并不会暴露在外，因此没必要使用加密哈希函数（例如，SHA-256），我们希望它速度越快，碰撞率越低越好。</p>\n<p>当前最先进的哈希函数是 Facebook XXHash3。</p>\n<h2 id=\"Static-Hashing-Schemes\"><a href=\"#Static-Hashing-Schemes\" class=\"headerlink\" title=\"Static Hashing Schemes\"></a>Static Hashing Schemes</h2><p>静态哈希方案中的哈希表大小固定，这意味着如果哈希表的存储空间用完，就得重建一个更大的哈希表，这样的代价非常昂贵。通常，新哈希表的大小是原始哈希表大小的 2 倍。同时，为了减少发生碰撞时比较的次数，哈希表的槽数会设置成预期元素书的 2 倍，但可惜的是，（1）元素的数量很难提前知道；（2）不同数据通过哈希函数计算得到的键可能会相等；（3）没有一个完美的哈希函数。</p>\n<h3 id=\"Linear-Probe-Hashing\"><a href=\"#Linear-Probe-Hashing\" class=\"headerlink\" title=\"Linear Probe Hashing\"></a>Linear Probe Hashing</h3><h3 id=\"Robin-Hood-Hashing\"><a href=\"#Robin-Hood-Hashing\" class=\"headerlink\" title=\"Robin Hood Hashing\"></a>Robin Hood Hashing</h3><h3 id=\"Cuckoo-Hashing\"><a href=\"#Cuckoo-Hashing\" class=\"headerlink\" title=\"Cuckoo Hashing\"></a>Cuckoo Hashing</h3><h2 id=\"Dynamic-Hashing-Schemes\"><a href=\"#Dynamic-Hashing-Schemes\" class=\"headerlink\" title=\"Dynamic Hashing Schemes\"></a>Dynamic Hashing Schemes</h2><p>动态哈希方案可以按需扩容缩容。</p>\n<h3 id=\"Chained-Hashing\"><a href=\"#Chained-Hashing\" class=\"headerlink\" title=\"Chained Hashing\"></a>Chained Hashing</h3><p>和 Linear Probe Hashing 有点像，只不过 Linear Probe Hashing 中的哈希槽大小是固定的，而 Chained Hashing 的哈希槽大小是可变的，当新插入数据时，创建一个指针指向新数据，并将指针加入到哈希槽的末尾。</p>\n<h3 id=\"Extendible-Hashing\"><a href=\"#Extendible-Hashing\" class=\"headerlink\" title=\"Extendible Hashing\"></a>Extendible Hashing</h3><p>是 project 1 将要实现的一部分，它的基本思想是哈希值的位数慢慢用，一开始数据少时，只是用哈希值的末尾几位，当数据逐渐增多时，哈希冲突越来越剧烈，就逐渐开放使用更多的哈希值位数，同时分配更多的哈希桶来存储数据，进行 rehash 操作，达到扩容的目的。</p>\n<h3 id=\"Linear-Hashing\"><a href=\"#Linear-Hashing\" class=\"headerlink\" title=\"Linear Hashing\"></a>Linear Hashing</h3><h2 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>哈希表提供了 O(1) 的访问效率，因此被大量地应用于 DBMS 的内部实现中。即便如此，她并不适合作为 table index 的数据结构，而 table index 的首选时下节将介绍的 B+ Tree（aka “The Greatest Data Structure of All Time”）。</p>\n<h1 id=\"Lec8-Trees-Indexes\"><a href=\"#Lec8-Trees-Indexes\" class=\"headerlink\" title=\"Lec8 Trees Indexes\"></a>Lec8 Trees Indexes</h1><h2 id=\"Table-Indexes\"><a href=\"#Table-Indexes\" class=\"headerlink\" title=\"Table Indexes\"></a>Table Indexes</h2><p>表索引作为数据库系统内部的数据结构，常常涉及范围扫描查询。有了表索引后，DBMS 在范围查询时，就不需要进行全表扫描，而是可以选择已有的最佳表索引，更快地找到范围内的元组。</p>\n<p>表的内容和索引在逻辑上是同步的。更多的索引可以加快查询速度，但也意味着更大的存储和维护开销。</p>\n<h2 id=\"B-Tree\"><a href=\"#B-Tree\" class=\"headerlink\" title=\"B+Tree\"></a>B+Tree</h2><p>B+Tree 是一种自平衡树，它将数据有序地存储，且在 search、sequential access、insertions 以及 deletions 操作的复杂度上都满足 O(logn)。</p>\n<p>B+Tree 可以看作是 BST (Binary Search Tree) 的衍生结构，它的每个 node 可以有多个孩子，这特别契合 disk-oriented database 的数据存储方式，每个 page 存储一个 node，使得树的结构扁平化，减少获取索引给查询带来的 I&#x2F;O 成本。其基本结构如下图所示：</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/B+Tree.jpg\" class=\"\" title=\"B+Tree\">\n\n<p>通常来说，B+Tree 是一个 M-路 搜索树（M 代表它的最大孩子数量），它有如下性质：</p>\n<ol>\n<li>每个节点最多存储 M-1 个 key，有 M 个 孩子；</li>\n<li>B+Tree 是完全平衡的，即每个叶子节点的深度都一样；</li>\n<li>除了 root 节点，所有其它节点至少处于半满状态，即 M&#x2F;2−1≤ #keys ≤ M−1； </li>\n<li>假设每个非叶子节点中包含 k 个 keys，那么它必然有 k+1 个孩子；</li>\n<li>B+Tree 的叶子节点通过双向链表串联，访问更高效。</li>\n</ol>\n<p>与 B-Tree 相比，B+Tree 仅在叶子节点上存储数据，而 B-Tree 在非叶子节点上也存储数据。</p>\n<h3 id=\"B-Tree-Node\"><a href=\"#B-Tree-Node\" class=\"headerlink\" title=\"B+ Tree Node\"></a>B+ Tree Node</h3><p>B+ Tree 的每个节点包含一个 K&#x2F;V 键值对数组，K 是从表的 attrubute(s) 中提取出来的，即 K 的类型为表的某列的类型；V 值类型取决于节点是否是叶子节点，对于非叶子节点，V 值类型为指向节点的指针，对于叶子节点，V 值类型通常有两种做法：</p>\n<ol>\n<li>Record&#x2F;Tuple Ids：存储指向最终 tuple 的指针；</li>\n<li>Tuple Data：直接将 tuple data 存在 leaf node 中，但这种方式对于 <a href=\"https://docs.oracle.com/cd/E17275_01/html/programmer_reference/am_second.html\">Secondary Indexes</a> 不适用，因为 DBMS 只能将 tuple 数据存储到一个 index 中，否则数据的存储就会出现冗余，同时带来额外的维护成本。</li>\n</ol>\n<p>B+Tree 节点中的 K&#x2F;V 键值对数组基本是按照 K 值排序的，尽管对于 B+Tree 的定义来说不是必需的；从概念上讲，非叶子节点上的 K 只作为标记使用，指导快速查询，但并不意味着这些 K 一定存在于叶子节点上，但是传统上，非叶子节点上的 K 都出现在叶子节点上。</p>\n<h3 id=\"Insertion\"><a href=\"#Insertion\" class=\"headerlink\" title=\"Insertion\"></a>Insertion</h3><ol>\n<li>根据非叶子节点的 K 从树的根部往下找到新 K&#x2F;V 键值对对应的 leaf node，L；</li>\n<li>如果 L 还有空间，则将 K&#x2F;V 键值对插入到 L 中的合适位置，保证 L 中的 K&#x2F;V 键值对仍是有序；否则，需要将 L 均匀分裂成两个节点，同时在 parent node 上新增 entry，新增 entry 的 K 应该是 L 中的中间 K 值，若 parent node 也空间不足，则递归地分裂，直到 root node 为止。</li>\n</ol>\n<h3 id=\"Deletion\"><a href=\"#Deletion\" class=\"headerlink\" title=\"Deletion\"></a>Deletion</h3><p>和在插入后节点满了需要拆分一样，当删除键值对后如果一个节点的大小小于半满时，需要进行合并操作。</p>\n<ol>\n<li>从 root 开始，找到目标 entry 所处的 leaf node, L；</li>\n<li>删除该 entry；</li>\n<li>如果 L 仍然至少处于半满状态，则操作结束；否则先尝试从 siblings 那里拆借 entries，如果失败，则将 L 与相应的 sibling 合并；</li>\n<li>如果合并发生了，则可能需要递归地删除 parent node 中的 entry。</li>\n</ol>\n<p>B+Tree 的 Insert、Delete 过程，可参考<a href=\"https://dichchankinh.com/~galles/visualization/BPlusTree.html\">这里</a>。</p>\n<h3 id=\"Selection-条件\"><a href=\"#Selection-条件\" class=\"headerlink\" title=\"Selection 条件\"></a>Selection 条件</h3><p>因为 B+Tree 是有序的，所以查找很快并且不需要整个键。如果查询条件中提供了键的任何“特征”，DBMS 可以使用 B+Tree 索引。而散列索引需要具体且完整的键，因此无法进行范围查询。</p>\n<h3 id=\"Non-Unique-Indexes\"><a href=\"#Non-Unique-Indexes\" class=\"headerlink\" title=\"Non-Unique Indexes\"></a>Non-Unique Indexes</h3><p>与哈希表一样，B+Tree 也可以处理重复的键。比如存储有相同键的 K&#x2F;V 对，或者是使用关联链表。</p>\n<h3 id=\"Duplicate-Keys\"><a href=\"#Duplicate-Keys\" class=\"headerlink\" title=\"Duplicate Keys\"></a>Duplicate Keys</h3><p>（有点分不清和 Non-Unique Indexes 的关系）</p>\n<h3 id=\"Clustered-Indexes\"><a href=\"#Clustered-Indexes\" class=\"headerlink\" title=\"Clustered Indexes\"></a>Clustered Indexes</h3><p>Clustered Indexes（聚簇索引）规定了 table 本身的物理存储方式，通常即按 primary key 排序存储，因此一个 table 只能建立一个 clustered index。有些 DBMS 对每个 table 的主键都添加聚簇索引，如果该 table 没有 primary key，则 DBMS 会为其自动生成一个。</p>\n<p>table 本身的物理存储按照聚簇索引排序后，通过聚簇索引进行条件查询时，将减少磁盘的读取次数。</p>\n<h3 id=\"Heap-Clustering\"><a href=\"#Heap-Clustering\" class=\"headerlink\" title=\"Heap Clustering\"></a>Heap Clustering</h3><p>有了聚簇索引后，元组在页面中以及页面间按照聚簇索引的规则变得有序。此时访问元组时如若使用了聚类索引（主键），DBMS 就可以直接选择正确的页面，这也就解释了上面说的为什么可以减少磁盘的读取次数。</p>\n<h3 id=\"Index-Scan-Page-Sorting\"><a href=\"#Index-Scan-Page-Sorting\" class=\"headerlink\" title=\"Index Scan Page Sorting\"></a>Index Scan Page Sorting</h3><p>由于直接从非聚集索引中检索元组效率低下（从磁盘中重复读取相同页面），因此 DBMS 可以首先找出它需要的所有元组，然后根据它们的页面 ID 对它们进行排序，再一次从磁盘中读取对应的元组，从而不会重复读取相同的磁盘页面。</p>\n<h2 id=\"B-Tree-Design-Choices\"><a href=\"#B-Tree-Design-Choices\" class=\"headerlink\" title=\"B+Tree Design Choices\"></a>B+Tree Design Choices</h2><h3 id=\"Node-Size\"><a href=\"#Node-Size\" class=\"headerlink\" title=\"Node Size\"></a>Node Size</h3><p>根据存储介质的不同，Node Size 的选择不一样。例如，存储在 HDD 上的节点大小通常在 MB 数量级，以减少查找数据所需的磁盘读取次数，而内存节点大小可能小至 512 字节，以便将整个页面放入 CPU 缓存并减少数据碎片化。这种选择也可以取决于工作负载的类型，因为点查询更喜欢尽可能小的页面以减少不必要的额外信息加载量，而大的顺序 scan 可能更喜欢大页面以减少它需要执行的读取次数。</p>\n<h3 id=\"Merge-Threshold\"><a href=\"#Merge-Threshold\" class=\"headerlink\" title=\"Merge Threshold\"></a>Merge Threshold</h3><p>由于 merge 操作引起的修改较大，有些 DBMS 选择延迟 merge 操作的发生时间，甚至可以利用其它进程来负责周期性地重建 table index。</p>\n<h3 id=\"Variable-Length-Keys\"><a href=\"#Variable-Length-Keys\" class=\"headerlink\" title=\"Variable Length Keys\"></a>Variable Length Keys</h3><p>B+ Tree 中存储的 key 经常是变长的，通常有三种手段来应对：</p>\n<ol>\n<li>Pointers：存储指向 key 的指针；</li>\n<li>Variable-Length Nodes：Node 的大小可以不一致，但这需要精细化的内存管理（几乎没有人这么做）；</li>\n<li>Padding：对 key 的末尾进行 pad 操作，直至 key 最大长度；</li>\n<li>Key Map&#x2F;Indirection：内嵌一个指针数组，数组中的每个元素指向 K&#x2F;V list。</li>\n</ol>\n<h3 id=\"Intra-node-Search\"><a href=\"#Intra-node-Search\" class=\"headerlink\" title=\"Intra-node Search\"></a>Intra-node Search</h3><p>在节点内部搜索，就是在排好序的序列中检索元素，手段通常有：</p>\n<ol>\n<li>Linear Scan：从节点头部向尾部线性搜索；</li>\n<li>Binary Search：二分查找；</li>\n<li>Interpolation：通过 keys 的分布统计信息来估计大概位置进行检索</li>\n</ol>\n<h2 id=\"Optimizations\"><a href=\"#Optimizations\" class=\"headerlink\" title=\"Optimizations\"></a>Optimizations</h2><h3 id=\"Pointer-Swizzling\"><a href=\"#Pointer-Swizzling\" class=\"headerlink\" title=\"Pointer Swizzling\"></a>Pointer Swizzling</h3><p>Node 中的 V 常常使用 page id 来指向其它 Node，这样的话 DBMS 每次需要首先从 page table 中获取对应 page 的 frame_id，然后才能从 buffer pool 获取相应的 node 本身，而如果 page 已经在 buffer pool 中，我们可以直接存储其在 buffer pool 的位置，从而避免查询 page table（常常还需要使用 latch 避免竞争），提高访问效率。</p>\n<h3 id=\"Bulk-Insert\"><a href=\"#Bulk-Insert\" class=\"headerlink\" title=\"Bulk Insert\"></a>Bulk Insert</h3><p>建 B+ Tree 的最快方式是先将 keys 排好序后，再从下往上建树。因此如果有大量插入操作，可以利用这种方式提高效率。</p>\n<h3 id=\"Prefix-Compression\"><a href=\"#Prefix-Compression\" class=\"headerlink\" title=\"Prefix Compression\"></a>Prefix Compression</h3><p>同一个 leaf node 中的 keys 通常有相同的 prefix，为了节省空间，可以只存所有 keys 的不同的 suffix。</p>\n<h3 id=\"Suffix-Truncation\"><a href=\"#Suffix-Truncation\" class=\"headerlink\" title=\"Suffix Truncation\"></a>Suffix Truncation</h3><p>由于非叶子节点只用于引导搜索，因此没有必要在非叶子节点中储存完整的 key，我们可以只存储足够的 prefix 并保证和原来的搜索语义一样即可。</p>\n<h3 id=\"Deduplication\"><a href=\"#Deduplication\" class=\"headerlink\" title=\"Deduplication\"></a>Deduplication</h3><p>Non-unique indexes 可能在节点中存储了多个相同的 K 副本，为了减少冗余，可以替换只存储唯一 K，其中 V 则使用关联列表代替。</p>\n<h1 id=\"Lec9-Index-Concurrency-Control\"><a href=\"#Lec9-Index-Concurrency-Control\" class=\"headerlink\" title=\"Lec9 Index Concurrency Control\"></a>Lec9 Index Concurrency Control</h1><h2 id=\"Latch-Implementations\"><a href=\"#Latch-Implementations\" class=\"headerlink\" title=\"Latch Implementations\"></a>Latch Implementations</h2><ol>\n<li>Blocking OS Mutex：睡眠锁，当该锁被其它线程占用时，当前线程进入睡眠状态，CPU 将调度其它线程。<ul>\n<li>示例：<code>std::mutex</code>；</li>\n<li>优点：使用简单，不需要 DBMS 进行额外编码；</li>\n<li>缺点：由于是操作系统进行调度，代价昂贵（每次锁定&#x2F;解锁大约需要 25 ns）且不可扩展。</li>\n</ul>\n</li>\n<li>Test-and-Set Spin Latch (TAS)：自旋锁，当该锁被其它线程占用时，用户可以自行决定接下来的操作，比如可以选择重试（while 循环）或允许操作系统将当前线程睡眠，因此，该方法给了 DBMS 更多的控制权。<ul>\n<li>示例：<code>std::atomic&lt;T&gt;</code>；</li>\n<li>优点：锁定&#x2F;解锁操作是高效的（因为很多事情交给了用户来做了）；</li>\n<li>缺点：不可扩展且缓存不友好，因为 CAS（compare-and-set）指令将在不同的线程中执行多次。</li>\n</ul>\n</li>\n<li>Reader-Writer Latches：读写锁。<ul>\n<li>示例：<code>std::shared mutex</code>；</li>\n<li>优点：允许并发读；</li>\n<li>缺点：DBMS 需要管理读写队列，避免大量读请求淹没写请求，同时由于额外的元数据，开销比自旋锁搞。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Hash-Table-Latching\"><a href=\"#Hash-Table-Latching\" class=\"headerlink\" title=\"Hash Table Latching\"></a>Hash Table Latching</h2><p>对于静态哈希表，所有线程在遇到哈希冲突时都是自顶向下查看 slot，因此很容易加锁，比如为每个 page 或者 slot 加锁，这样也不会出现死锁的情况（加锁方向一致），当需要重新调整哈希表的大小时，则获取整个哈希表的全局锁即可；</p>\n<ol>\n<li>page 锁：每个页面都有自己的读写锁来保护其全部内容。线程在访问页面之前获取读或写锁。由于一个页面中有多个 slot，这会降低一定的并行度。</li>\n<li>slot 锁：每个 slot 都有一把锁，此时两个线程可以访问同一页中的不同 slot，这增加了存储（每个 slot 有一个锁变量）和计算（一次查找可能涉及很多次加锁&#x2F;解锁操作）开销。</li>\n</ol>\n<p>对于动态哈希表，加锁比较麻烦，但大致思路和上面一样。</p>\n<h2 id=\"B-Tree-Latching\"><a href=\"#B-Tree-Latching\" class=\"headerlink\" title=\"B+Tree Latching\"></a>B+Tree Latching</h2><p>目标：</p>\n<ol>\n<li>允许多线程同时读取和更新 B+Tree；</li>\n<li>同时需要注意两种情形：（1）多个线程同时修改一个节点的内容；（2）一个线程正在遍历树然而另外一个线程正在 split&#x2F;merge 节点；</li>\n</ol>\n<p>举例：T1 想要删除 44，T2 想要查询 41。删除 44 后，DBMS 需要 rebalance，将 H 节点拆分成两个节点。若在拆分前，T2 读取到 D 节点，发现 41 在 H 节点，此时时间片轮转到了 T1，T1 把 D 节点拆分成 H、I 两个节点，同时把 41 转移到 I 节点，之后 CPU 交还给 T2，T2 到 H 节点就找不到 41，如下图所示。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/B+delete.jpg\" class=\"\" title=\"B+delete\">\n\n<img src=\"/2023/03/14/CMU15445-Lecture/B+rebalance.jpg\" class=\"\" title=\"B+rebalance\">\n\n<img src=\"/2023/03/14/CMU15445-Lecture/B+find.jpg\" class=\"\" title=\"B+find\">\n\n<p>由于 B+Tree 是树形结构，有明确的搜索顺序，因此沿着搜索路径加锁是不错的选择。</p>\n<h3 id=\"Latch-Crabbing-x2F-Coupling\"><a href=\"#Latch-Crabbing-x2F-Coupling\" class=\"headerlink\" title=\"Latch Crabbing&#x2F;Coupling\"></a>Latch Crabbing&#x2F;Coupling</h3><p>它的基本思想是</p>\n<ol>\n<li>获取 parent 的 latch</li>\n<li>获取 child 的 latch</li>\n<li>如果 child “安全”（不会出现 split&#x2F;merge 的情况，它的 parent 节点也就不需要修改），则可以释放 parent 的 latch。</li>\n</ol>\n<h3 id=\"Better-Latching-Algorithm\"><a href=\"#Better-Latching-Algorithm\" class=\"headerlink\" title=\"Better Latching Algorithm\"></a>Better Latching Algorithm</h3><p>在实际应用中，每次更新 B+Tree 都需要获取非叶子节点的写锁是不好的，因为实际的更新只是发生在叶子节点上，而叶子节点大多数情况下是不会影响到上层节点的，尤其是越往上影响到的可能性越小，因此总是持 “悲观” 的想法而获取搜索路径上的的非叶子节点的写锁是不合适的。</p>\n<p>反之，可以采用类似乐观锁的思想，假设 leaf node 是安全（更新操作仅会引起 leaf node 的变化）的，在查询路径上一路获取、释放 read latch，到达 leaf node 时，若操作不会引起 split&#x2F;merge 发生，则只需要在 leaf node 上获取 write latch 然后更新数据，释放 write latch 即可；若操作会引起 split&#x2F;merge 发生，则重新执行一遍，此时在查询路径上一路获取、释放 write latch，即 Latch Crabbing 原始方案。</p>\n<h3 id=\"Leaf-Node-Scans\"><a href=\"#Leaf-Node-Scans\" class=\"headerlink\" title=\"Leaf Node Scans\"></a>Leaf Node Scans</h3><p>以上都是从上往下的访问模式，我们介绍了相关锁的控制；而对于水平方向的访问模式，如果两个线程分别从左往右，从右往左获取写锁，就可能陷入死锁，因此部分 DBMS 在进行范围查询时总是从左往右扫描。</p>\n<h1 id=\"Lec10-Sorting-amp-Aggregations-Algorithms\"><a href=\"#Lec10-Sorting-amp-Aggregations-Algorithms\" class=\"headerlink\" title=\"Lec10 Sorting &amp; Aggregations Algorithms\"></a>Lec10 Sorting &amp; Aggregations Algorithms</h1><p>开始进入 Operator Execution 部分。</p>\n<p>SQL 语句被 DBMS 解释成 Query Plan。</p>\n<img src=\"/2023/03/14/CMU15445-Lecture/query_plan.jpg\" class=\"\" title=\"query_plan\">\n\n<h2 id=\"Sorting\"><a href=\"#Sorting\" class=\"headerlink\" title=\"Sorting\"></a>Sorting</h2><p>Disk-Oriented DBMS 将数据持久化在磁盘中，而不会假设整张表能够完全写入内存中，这也就可能由于数据量过大而无法在内存中完全一次性的排序操作。然而很多时候排序是需要的，比如 <code>ORDER BY</code>，<code>DISTINCT</code>，<code>GROUP BY</code> 和 <code>JOIN</code> 都将需要排好序的数据以便能够快速地执行。</p>\n<p>对于数据量太大而无法放入内存的数据，外部归并排序是一个很好的选择。它是一种分而治之的排序算法，它将数据集分成多个独立的块，然后对它们分别进行排序，并将排序结果写入磁盘，以便下一个阶段使用。</p>\n<h3 id=\"Two-way-Merge-Sort\"><a href=\"#Two-way-Merge-Sort\" class=\"headerlink\" title=\"Two-way Merge Sort\"></a>Two-way Merge Sort</h3><p>该算法的最基本版本是二路归并排序。该算法排序阶段读取每个页面，对它们单独进行排序，并将排序后的结果写回磁盘。然后，在合并阶段，它使用三个缓冲页，其中将两个已排序的页面读取到缓冲页中，并将它们合并到第三个缓冲页面中，每当第三页填满后，它就会写回磁盘并替换为空页。</p>\n<p>复杂度：</p>\n<h3 id=\"General-K-way-Merge-Sort\"><a href=\"#General-K-way-Merge-Sort\" class=\"headerlink\" title=\"General (K-way) Merge Sort\"></a>General (K-way) Merge Sort</h3><p>归并排序的一般版本可以使用三个以上的缓冲页，从而加快排序速度。</p>\n<h3 id=\"Double-Buffering-Optimization\"><a href=\"#Double-Buffering-Optimization\" class=\"headerlink\" title=\"Double Buffering Optimization\"></a>Double Buffering Optimization</h3><p>外部归并排序的一个优化是在后端预取下一阶段的排序块并将其存储在第二个缓冲区，而系统正在处理当前排序块。这减少了 I&#x2F;O 请求的等待时间。但这种优化需要使用多线程，因为预取应该在当前运行的计算发生时发生。</p>\n<h3 id=\"Using-B-Trees\"><a href=\"#Using-B-Trees\" class=\"headerlink\" title=\"Using B+Trees\"></a>Using B+Trees</h3><p>有时 DBMS 使用现有的 B+ 树索引来帮助排序。特别是，如果索引是聚簇索引，数据会按照正确的顺序存储在磁盘中，I&#x2F;O访问<br>将是顺序的，从而 DBMS 可以只遍历B+ 树的叶子结点，不需要进行排序操作。</p>\n<p>另一方面，如果索引是非聚簇的，遍历树几乎总是更糟，因为每条记录可以存储在任何页面中，几乎所有记录访问都需要进行磁盘读取。</p>\n<h2 id=\"Aggregations\"><a href=\"#Aggregations\" class=\"headerlink\" title=\"Aggregations\"></a>Aggregations</h2><p>查询计划中的聚合运算符将一个或多个元组的值折叠为单个标量值。有两种实现聚合的方法：</p>\n<ol>\n<li>排序；</li>\n<li>散列。</li>\n</ol>\n<h3 id=\"Sorting-Aggregation\"><a href=\"#Sorting-Aggregation\" class=\"headerlink\" title=\"Sorting Aggregation\"></a>Sorting Aggregation</h3><p>DBMS 首先根据 GROUP BY Key(s)  对元组进行排序。在内存足够时，可以使用快速排序，如果内存不够，可以使用外部归并排序算法。然后，DBMS 对排序后的数据执行顺序扫描以计算聚合。</p>\n<p>但是，有时候我们并不需要排好序的数据，比如 <code>GROUP BY</code> 和 <code>DISTINCT</code>。在这种场景下 hashing 将更好，它能有效减少排序所需的额外工作。</p>\n<h3 id=\"Hashing-Aggregation\"><a href=\"#Hashing-Aggregation\" class=\"headerlink\" title=\"Hashing Aggregation\"></a>Hashing Aggregation</h3><p>hash 在计算上比 sort 开销更小。 DBMS 通过扫描表填充临时哈希表，对于每条记录，检查是否已经有一个条目，并根据聚合类型执行适当的修改。如果哈希表的大小太大而无法全部装入内存时，DBMS 可以将它写入磁盘。这需要两部操作：</p>\n<ol>\n<li>Partition： Use a hash function h1 to split tuples into partitions on disk based on target hash key。这会将所有匹配的元组放入同一分区；</li>\n<li>ReHash：对于磁盘上的每个分区，将其页面读入内存并基于第二个哈希函数 h2（h1 !&#x3D; h2） 构建一个内存中的哈希表。然后遍历这个临时哈希表的每个桶，将匹配的元组放在一起计算聚合（这假定每个分区都能装入内存）。</li>\n</ol>\n<p>在 ReHash phase 中，存着 (GroupKey→RunningVal) 的键值对，当我们需要向 hash table 中插入新的 tuple 时：</p>\n<ol>\n<li>如果我们发现相应的 GroupKey 已经在内存中，只需要更新 RunningVal 就可以；</li>\n<li>反之，则插入新的 GroupKey 到 RunningVal 的键值对。</li>\n</ol>\n<p>复杂度：</p>\n<h1 id=\"推荐阅读\"><a href=\"#推荐阅读\" class=\"headerlink\" title=\"推荐阅读\"></a>推荐阅读</h1><p><a href=\"https://15445.courses.cs.cmu.edu/fall2022/notes/\">官方笔记</a></p>\n<p><a href=\"https://zhenghe.gitbook.io/open-courses/cmu-15-445-645-database-systems/relational-data-model\">公开课笔记</a></p>\n","categories":["数据库"],"tags":["CMU15445","公开课","数据库"]},{"title":"CacheDedup（FAST 16）","url":"/2023/05/21/CacheDedup%EF%BC%88FAST%2016%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>使用闪存作为较慢主存的缓存层可以解决存储系统的扩展性问题，但是，闪存容量有限且耐用性较差；</li>\n<li>本文提出一种新的架构，将数据缓存和重删元数据（source addresses and fingerprints of the data）结合起来，有效管理两个组件；</li>\n<li>进一步，提出重复感知的 D-LRU 和 D-ARC 缓存替换算法，优化缓存性能和提高闪存寿命。</li>\n</ol>\n<p><strong>创新性：本文强调，虽然已有将重删和闪存结合起来的研究，但本文谈到还可以将压缩感知结合起来，同时重点介绍了整合重删和缓存过程中的挑战和解决办法。</strong></p>\n<h2 id=\"key-idea\"><a href=\"#key-idea\" class=\"headerlink\" title=\"key idea\"></a>key idea</h2><p>将闪存划分成两个部分：数据缓存和元数据缓存。</p>\n<p>元数据缓存中包含两个重要数据结构：</p>\n<ol>\n<li>source address index map：将后端存储的源地址映射到元数据缓存保存的某一指纹。由于 chunk 内容重复的存在，多个源地址可能映射到同一指纹。（多对一）</li>\n<li>fingerprint store：将指纹映射到数据缓存中的块地址。（一对一）</li>\n</ol>\n<p>因此，对于相同内容的 chunk，它们在数据缓存中只会有一份副本。</p>\n","categories":["Cache"],"tags":["Cache","论文阅读","Deduplication"]},{"title":"CXL-SSD（ATC 23）","url":"/2023/12/26/CXL-SSD%EF%BC%88ATC%2023%EF%BC%89/","content":"<p>基于 CXL，探索使用廉价的闪存克服内存墙（<strong>在这里应该是指对内存的需求增长远远大于内存容量增长的速度</strong>）的可行性。</p>\n<p>作者在本文中分析了现有技术的一些限制，并建议进行一些系统级的改变以使得闪存实现近似 DRAM 的性能。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>数据库，数据分析和机器学习等业务对内存的需求增长远远大于内存容量的增长速度。是否可以使用闪存克服内存墙呢？新兴的互联技术比如 CXL，Gen-Z，CCIX 和 OpenCAPI 允许 PCIe 设备被 CPU 使用 load&#x2F;store 指令直接访问，并且它们的可扩展性更好（更多的 PCIe 设备可以跨交换机连接，这与用于 DRAM 的 DIMM 不同）。</p>\n<p>但是，使用闪存作为 CPU 直接访问的主存仍然有三个挑战。</p>\n<ol>\n<li>内存请求粒度和闪存粒度不匹配；这可能导致流量放大（比如 64B 的缓存行刷新到 CXL-SSD 中，可能需要 16KB 的闪存读，64B 的更新和 16 KB 的写入，但其实后文主要讲的是在请求 64KB 的过程中，LLC 可能会发送接下来的 64B 请求，导致延时命中）。</li>\n<li>闪存比 DRAM 慢几个数量级；</li>\n<li>闪存的耐用性有限；</li>\n</ol>\n<p>作者探索一些设计选项，特别是使用缓存和预取来克服以上挑战，以使得 CXL-SSD 可以作为主存来克服内存墙。作者谈到，先前有一些技术已经证明了 CXL-SSD 的可行性，但本文是第一篇开源的对 CXL-SSD 设计选项和使用现有技术进行优化的深度研究（既然如此的话，那么找一个场景、动机，使用 CXL 可以一定程度上解决这个问题，并讲讲如何高效地使用 CXL 解决这个问题，这种行文逻辑就更可行了，而不用太担心创新型）。</p>\n<p>进一步，由于虚拟地址到物理地址的转换，影响了现有预取策略的效果，作者对其进行了进一步优化。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><h2 id=\"CXL-带来的机会\"><a href=\"#CXL-带来的机会\" class=\"headerlink\" title=\"CXL 带来的机会\"></a>CXL 带来的机会</h2><ol>\n<li>CXL 允许 CPU 和 PCIe 设备之间的一致性内存访问，减少了 CPU 和设备之间传输所需的同步开销；</li>\n<li>很容易扩展 CXL 设备的数量，通过一个 CXL 交换机，可以将一组 CXL 设备连接到 CPU（这一组 CXL 设备应该可以被连接到该交换机的多台服务器访问吧？）</li>\n</ol>\n<p>除了 DRAM 和 PMEM 之外，SSD 也可以作为内存扩展设备，并且由于其高容量以及更好的扩展性，SSD 可以有效解决现代内存敏感型应用程序面临的内存墙问题。</p>\n<h2 id=\"CXL-SSD-作为主存的挑战\"><a href=\"#CXL-SSD-作为主存的挑战\" class=\"headerlink\" title=\"CXL-SSD 作为主存的挑战\"></a>CXL-SSD 作为主存的挑战</h2><ol>\n<li><p>内存请求粒度和 SSD 粒度不匹配；</p>\n<p> 闪存无法随机访问，数据总是以页面粒度（KB 级）进行读写，（如果是 DRAM 作为内存扩展设备，它的粒度是多少呢？），而又由于 SSD 无法覆盖写，SSD 需要以块粒度读出原有数据，再修改块中的对应页面，然后再将它们重新写入，一个 64B 的缓存行刷新将经历读-改-写过程，导致了流量放大；</p>\n</li>\n<li><p>微秒级时延；</p>\n<p> 闪存比 DRAM 慢了几个数量级，当闪存作为 CPU 直接访问的内存设备时，微秒级的时延成为了一个挑战；</p>\n</li>\n<li><p>有限的寿命；</p>\n<p> 闪存的擦除次数有限。</p>\n</li>\n</ol>\n<h1 id=\"Design\"><a href=\"#Design\" class=\"headerlink\" title=\"Design\"></a>Design</h1><h2 id=\"为-CXL-SSD-增加-DRAM-Cache\"><a href=\"#为-CXL-SSD-增加-DRAM-Cache\" class=\"headerlink\" title=\"为 CXL-SSD 增加 DRAM Cache\"></a>为 CXL-SSD 增加 DRAM Cache</h2><p>在闪存前面增加一个 DRAM Cache，可以有效降低平均访问延迟，减少闪存后端的流量。</p>\n<p>但是，即使有足够大的缓存，平均时延仍然远远大于 DRAM 的时延，这是由于请求到达的时间间隔较短，导致闪存后端无法获取数据（写穿排队？）。</p>\n<h2 id=\"预取\"><a href=\"#预取\" class=\"headerlink\" title=\"预取\"></a>预取</h2><p>预取可以进一步降低时延。作者进一步实验讨论了预取程度和预取偏移量对时延降低程度的影响。</p>\n<h2 id=\"减少闪存流量\"><a href=\"#减少闪存流量\" class=\"headerlink\" title=\"减少闪存流量\"></a>减少闪存流量</h2><p>内存访问粒度是 64B，然而闪存后端却是 4KB，因此，DRAM Cache 的 64B 缺失，会导致从闪存提取 4KB 数据，并且即使闪存正在进行中，属于同一 4KB 的后续 64B 缓存缺失也将生成额外的闪存读取请求。</p>\n<p>存储领域可能没有这个问题，因为软件栈会将具有重叠地址的块 I&#x2F;O 合并，然而，对于 CXL-SSD 来说没有软件层，它直接从接收 LLC 的内存事务。作者效仿 CPU Cache 设计，增加一组 MSHRs，记录正在进行的闪存请求，通过单次闪存读取服务多个 64B 内存访问。</p>\n<p>这一做法显著减少了尾延时，减少了闪存后端的访问流量。</p>\n<h2 id=\"闪存科技和并行\"><a href=\"#闪存科技和并行\" class=\"headerlink\" title=\"闪存科技和并行\"></a>闪存科技和并行</h2><p>不同闪存科技的延时性能和寿命不同，ULL 和 SLC 科技，再加上 DRAM Cache，CXL-SSD 可以实现超 4 年的寿命，增加 DRAM Cache 的大小也可以进一步提高寿命。</p>\n<p>对于大缓存来说，CXL-SSD 的并行性（channels*ways）不重要，但对于小缓存，闪存的并行性很重要。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>单独使用缓存并不足以掩盖闪存的较长时延；此时进一步增加预取策略可以提高缓存命中率，提高时延；</p>\n<p>由于内存访问粒度和闪存访问粒度不一致，会导致闪存的重叠访问，放大了闪存后端的流量，此时使用 MSHRs 技术可以通过单次闪存读取服务多个 64B 内存访问；</p>\n<p>ULL 和 SLC 闪存科技的性能差异很小，而如果使用 MLC 和 TLC 闪存，即使有足够大的 DRAM Cache，它们的性能和寿命仍然较差。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><h2 id=\"不同缓存替换策略\"><a href=\"#不同缓存替换策略\" class=\"headerlink\" title=\"不同缓存替换策略\"></a>不同缓存替换策略</h2><p>测试了不同缓存替换算法，以及不同关联度的情况下，CXL-SSD 的请求延时和写入量。</p>\n<h2 id=\"预取策略\"><a href=\"#预取策略\" class=\"headerlink\" title=\"预取策略\"></a>预取策略</h2><p>测试了几个预取算法，发现在某些工作负载下，预取器反而会降低性能；</p>\n<p>负载强度，读写率，局部性会影响 CXL-SSD 的寿命；</p>\n<p>CXL-SSD 的 performance-per-cost 比纯 DRAM 高（加了 小DRAM 做分层…）；</p>\n<p>预取器的准确度对性能提升很重要；</p>\n<p>预取器的准确度低时，造成的缓存污染是性能降低的主要原因；</p>\n<p>虚拟地址-物理地址的转换，使得 CXL-SSD 的预取变得困难，具体来说，基于虚拟地址预取的效果比基于物理地址的预取效果好；</p>\n<p>内核为设备提供内存访问提示（大致意思是将一些频繁访问的地址发送给 CXL-SSD，由它根据这些地址触发预取操作），通过将延迟命中转换为缓存命中，可以进一步降低时延。</p>\n","categories":["CXL"],"tags":["Cache","论文阅读","CXL"]},{"title":"CacheSifter（FAST 22）","url":"/2023/04/12/CacheSifter%EF%BC%88FAST%2022%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>移动应用程序经常将下载文件作为缓存文件维护在本地存储中，从而获得更好的用户体验。但是这些缓存文件占据了移动闪存中的一大部分，对移动设备的性能和寿命有很大影响；</li>\n<li>本文提出了 CacheSifter 通用框架，并基于重用行为和内存&#x2F;存储使用量区分缓存文件；在线将缓存文件分为三类，通过移除大概率不会被重用的文件来减少闪存的写入量；</li>\n<li>作者在实际安卓设备上实现了 CacheSifter，并在具有代表性的应用中进行了评估。实验结果表示，依靠 ML 模型，CacheSifter 平均减少了 60% 的缓存文件对闪存的写入量，I&#x2F;O 密集型写性能改善了 20% 左右。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>由于应用的动态性质和整体系统的优化，几乎所有移动应用需要从网络下载文件或数据。而移动设备上的缓存文件可以避免从网络中重新下载数据，从而满足执行延迟的需求。</p>\n<p>当前移动设备首先将缓存文件存储在主存中，然后写回闪存（默认在内存中保存 30s）。但是，近年来由于应用程序对数据量的需求增长，缓存文件的数量和大小按指数级增长，降低了移动设备的性能，同时大量缓存文件写回闪存，减少了移动设备的寿命。</p>\n<p>值得注意的是，一些缓存文件在其生命周期内只被使用一次，而其它缓存文件在被删除之前可能会被重新访问多次，但是目前的实际系统中，这些缓存文件被同等对待。</p>\n<p>现有的一些技术只在 DRAM 上存储缓存文件，减少对闪存的写回，但是，随着应用程序对于缓存文件的需求增长，这会有两个问题：（1）缓存数据在访问频率、生命周期和大小上有很大的区别；（2）在移动设备可用内存不足的情况下，维持无用的缓存文件会因为内存竞争而降低系统的整体性能。本文则根据缓存文件的重用行为管理缓存文件，从而提高系统性能和山存的使用寿命。</p>\n<p>本文提出的通用缓存文件管理框架，CacheSifter，使用轻量级机器学习算法将缓存文件动态分类成不同类别，并基于它们的数据访问模式动态地将不同类别的缓存文件放置在 DRAM 或闪存中。基于重用概率，缓存文件被分成三类：Burn-After-Reading (BAR)  文件，Transient 文件和 Long-living 文件。 </p>\n<h1 id=\"Cache-Files-in-Mobile-Systems\"><a href=\"#Cache-Files-in-Mobile-Systems\" class=\"headerlink\" title=\"Cache Files in Mobile Systems\"></a>Cache Files in Mobile Systems</h1><p><strong>为什么能够对不同缓存文件进行分类？</strong></p>\n<ol>\n<li>实验展示了不同厂商的智能手机、应用程序的<strong>缓存文件平均大小</strong>差异很大；</li>\n<li>实验展示了<strong>缓存文件对闪存的写入量</strong>占总写入量的 64%；</li>\n<li>实验展示<strong>不同应用程序的写入模式</strong>差异很大（社交软件的缓存文件写入量很大，单机游戏的缓存文件写入量很小）；</li>\n</ol>\n<p><strong>如何对不同缓存文件进行分类？</strong></p>\n<ol>\n<li>Burn-After-Reading (BAR)：该类缓存文件只在其生命周期开始阶段有些微重用，没有必要将这类文件写入闪存；</li>\n<li>Transient：只在创建之后的短期内活跃，有较多重用；</li>\n<li>Long-living：其它的缓存文件，表现出长期内持续有较多的重用。</li>\n</ol>\n<p><strong>管理缓存文件的挑战</strong></p>\n<ol>\n<li>缓存文件的重用行为可能会随时间变化，因此，在管理时需要实时地适应缓存文件的行为变化；</li>\n<li>现有系统都是将所有缓存文件同等对待，然而，对于不同类别的缓存文件，需要使用不同的策略进行管理。</li>\n</ol>\n<p>本文的主要目标是改善系统性能和存储寿命；探索应用程序缓存文件的访问模式并考虑基于 DRAM 的内存和基于闪存的移动设备存储在性能和寿命上的特点。</p>\n<h1 id=\"CacheSifter-Design\"><a href=\"#CacheSifter-Design\" class=\"headerlink\" title=\"CacheSifter Design\"></a>CacheSifter Design</h1><h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><h3 id=\"设计原则\"><a href=\"#设计原则\" class=\"headerlink\" title=\"设计原则\"></a>设计原则</h3><ol>\n<li>对应用程序透明：CacheSifter 不能对用户体验造成较大影响，同时，也不能对用户应用程序进行任何更改；</li>\n<li>在线分类：分类器需要适应系统状态的动态变化和用户的配置；</li>\n<li>自适应内存管理：虽然使用内存保存缓存文件可以实现高效访问，但是使用太多的内存会导致系统性能下降。因此，CacheSifter 应该根据不同的活跃程序自适应调整内存使用量；</li>\n<li>适应用户行为的改变：缓存文件的类别可能会发生变化，比如该文件过去是 BAR 文件，但用户行为发生变化后，该文件可能成为 Long-living 文件；</li>\n<li>确保删除缓存文件时的安全：在删除 BAR 和 Transient 文件时，应用程序可能正在使用这些文件执行任务，因此，CacheSifter 在删除数据时不能造成应用程序 crash 或用户数据缺失。</li>\n</ol>\n<h3 id=\"框架\"><a href=\"#框架\" class=\"headerlink\" title=\"框架\"></a>框架</h3><p>CacheSifter 处于内核层，能够直接在页缓存上分类缓存文件而不会带来额外的内存开销和数据复制。所有新下载的文件被保存在主存并等待分类。</p>\n<img src=\"/2023/04/12/CacheSifter%EF%BC%88FAST%2022%EF%BC%89/CacheSifter%E6%A1%86%E6%9E%B6.jpg\" class=\"\" title=\"CacheSifter框架\">\n\n<ol>\n<li>使用一个轻量级机器学习分类引擎在线将缓存文件分类成上述三类中的一类；</li>\n<li>为了更好地利用内存和闪存，丢弃所有 BAR 文件；</li>\n<li>将 Transient 文件保存在内存的一个 <strong>LRU-like</strong> 链表中，它包含一个活跃链表和一个不活跃链表。该类文件加入缓存时，插入活跃链表的头部，当离开活跃链表时，如若在活跃链表的存活时间大于阈值，将降级到不活跃链表的头部，否则直接丢弃；当命中不活跃链表的文件时，将提升到活跃链表的头部。当文件从链表中驱逐后，直接丢弃；</li>\n<li>将 Long-living 文件保存在内存中的另一个 LRU 链表中，当文件从链表中驱逐且 dirty，则加入闪存；只有在应用程序将该类文件标记为无效后，才从系统中删除。</li>\n<li>BAR 文件的重新下载，将升级为 “TR，LL” 文件；TR 文件的重新下载，将升级为 LL 文件。</li>\n</ol>\n<p><strong>CacheSifter 带来了三个好处</strong></p>\n<ol>\n<li>避免将 BAR 和 Transient 文件写入闪存，减少了写竞争，延长了闪存寿命，改善了系统性能，节约了存储空间；</li>\n<li>Transient 文件直接从 DRAM 中访问，改善了该类文件的访问延迟；</li>\n<li>在内核层使用轻量级机器学习引擎优化缓存文件管理，仅带来微小开销且对用户程序透明。</li>\n</ol>\n<h2 id=\"Feature-based-Cache-Files-Management\"><a href=\"#Feature-based-Cache-Files-Management\" class=\"headerlink\" title=\"Feature-based Cache Files Management\"></a>Feature-based Cache Files Management</h2><p>CacheSifter 的有效性很大程度上依赖分类引擎的准确性，同时，开销也应该尽可能小。</p>\n<h3 id=\"Lightweight-Categorization-of-Cache-Files\"><a href=\"#Lightweight-Categorization-of-Cache-Files\" class=\"headerlink\" title=\"Lightweight Categorization of Cache Files\"></a>Lightweight Categorization of Cache Files</h3><p><strong>机器学习分类</strong></p>\n<p>本文选择轻量级神经网络 MLP 作为分类模型，而为了进一步降低开销，分类过程分为两步：首先区分是否是 BAR，如若不是，再区分 Transient&#x2F;Long-living（因为 Long-living 文件无法使用短期信息识别出来）。</p>\n<p>选择 K+6 个特征：（1）该文件创建后的前 K 个单位时间点（比如每个时间点 30s）访问的 I&#x2F;O 大小；（2）读取量、读取次数、写入量、写入次数；（3）文件大小和活跃时间（上次访问时间 - 第一次访问时间）。特别地，为权衡准确率和开销，第一个 MLP 模型需要 K 值小于第二个 MLP 模型。</p>\n<p>离线训练两个 MLP 模型。</p>\n<p><strong>模型评价指标</strong></p>\n<p>准确率、召回率、准确率-召回率曲线。</p>\n<p><strong>性能评价指标</strong></p>\n<p>缓存文件对闪存的写入减少量、闪存寿命、读写性能。</p>\n<p><strong>开销分析</strong></p>\n<p>网络开销、内存开销、CPU时间开销。</p>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读","Multi-level"]},{"title":"Demystifying CXL Memory（MICRO 23）","url":"/2023/12/27/Demystifying_CXL_Memory%EF%BC%88MICRO%2023%EF%BC%89/","content":"<p>对容量更大，带宽更高的内存需求推动了基于 CXL 的内存扩展和分离的创新。基于 CXL 的内存扩展，不仅能够扩展内存容量和带宽，而且能够将内存从与 CPU 的紧耦合中分离出来。然而，由于 CXL 设备并没有大规模生产，因此大多数研究者都是使用远程 NUMA 节点进行模拟。</p>\n<p>作者则是首次在一个真正的 CXL 系统中，使用来自不同制造商的三个 CXL 内存设备进行评估，并比较了真实 CXL 内存和模拟 CXL 内存的性能，此外，还分析了 CPU 和 CXL 内存之间的复杂相互作用，解释了模拟的 CXL 内存和真实 CXL 内存的一些差异；接下来，作者探讨了内存带宽密集型应用程序从 CXL 内存中获益的机会；最后，提出一个 CXL 内存感知的动态页面分配机制.</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>新兴应用程序对内存有着更大容量和更高带宽的需求，然而内存容量和带宽是每个 CPU 通道数，每个通道的 DIMM 数以及每个通道的比特传输率的函数，因此 DDR 的带宽和容量扩展能力有限，此时需要替代的内存接口技术和内存子系统架构。</p>\n<p>CXL 构建在 PCIe 之上，相比 DDR4 有着更高的 bit 传输速度，更低的 bit 传输能耗，但连接时延相对较高；同时，CXL 内存设备相比于 DDR5，减少了 3 倍引脚数量的消耗，可以经济高效地扩展系统的存储容量和带宽；此外，通过 CPU 和内存设备之间的 CXL 控制器，CXL 将内存技术与 CPU 支持的特定内存接口技术解耦。最后，通过采用交换机，支持 CXL 的 CPU 可以轻松访问远程节点中的内存，并且比 RDMA 等网络接口技术的延迟更低，从而有效促进内存分解。</p>\n<p>现有对 CXL 的模拟主要是使用远程 NUMA 节点，但这种仿真方式可能带来误导性的性能表征结果和次优设计决策。于是作者在本文比较了真实 CXL 和模拟 CXL 的性能，并对 CPU 和 CXL 内存之间的复杂相互作用进行了深入分析，基于这些分析，做出了以下贡献：</p>\n<ol>\n<li>CXL 内存 !&#x3D; 远程 NUMA 内存。<ul>\n<li>不同的 CXL 控制器设计或内存技术，将使得真正的 CXL 访问延迟和带宽具有很大不同；</li>\n<li>与模拟的 CXL 内存相比，真正的 CXL 内存可降低高达 26% 的延迟，并提高 3%-66% 的带宽效率；</li>\n<li>使用真正的 CXL 内存后，多个节点上的 CPU 可以将它们的 L2 缓存行驱逐到所有的 LLC 缓存，因此，CPU 访问 CXL 内存时，可以从超大容量的 LLC 缓存中受益，特别是缓存友好型工作负载。</li>\n</ul>\n</li>\n<li>盲目使用 CXL 内存可能是有害的。<ul>\n<li>要求 us 级延迟的简单应用程序（比如内存键值存储）对内存访问延迟高度敏感，与本地 DDR 内存相比，将页面分配给 CXL 内存会使得尾延迟增加 1082%；</li>\n<li>现有的 CXL 内存感知的多级页面放置策略 TPP，相比于在 DDR 内存和 CXL 内存之间静态分区页面，可能会增加尾延迟，这是因为存在页面迁移开销；</li>\n<li>……</li>\n</ul>\n</li>\n<li>CXL 内存感知的动态页面分配策略 Caption。<ul>\n<li>Caption 首先确定制造商特定的 CXL 内存设备的带宽，随后，定期监控各种 CPU 计数器（比如内存访问延迟），并评估它们在运行时消耗的带宽；</li>\n<li>根据上一步监控的计数器值，估计一段时间内的内存子系统性能。当给定应用程序需要分配新页面时，Caption 考虑内存子系统性能的历史记录以及过去分配给 CXL 内存的页面百分比；</li>\n<li>然后，使用简单的贪心算法调整分配给 CXL 内存的页面百分比，以提高整体系统吞吐量。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"内存延迟和带宽特征\"><a href=\"#内存延迟和带宽特征\" class=\"headerlink\" title=\"内存延迟和带宽特征\"></a>内存延迟和带宽特征</h1><p>本节评估不同 CXL 内存设备、基于 NUMA 节点模拟的 CXL 内存的延迟和带宽，并研究 CPU 和 CXL 内存设备之间的交互。</p>\n<h2 id=\"延迟\"><a href=\"#延迟\" class=\"headerlink\" title=\"延迟\"></a>延迟</h2><ol>\n<li><p>全双工 CXL 和 UPI 接口可减少内存访问延迟；</p>\n</li>\n<li><p>访问真实 CXL 内存设备的延迟高度依赖给定的 CXL 控制器设计；</p>\n</li>\n<li><p>模拟 CXL 内存的延迟更高；</p>\n<p> 这是因为向模拟 CXL 内存发出请求时，本地 CPU 必须首先检查通过芯片间 UPI 接口连接的远程 CPU 的缓存一致性；此外，内存请求必须通过远程 CPU 内的长芯片内互联才能到达其内存控制器，这些开销也随着 CPU 内核的增加而增加。</p>\n</li>\n</ol>\n<h2 id=\"带宽\"><a href=\"#带宽\" class=\"headerlink\" title=\"带宽\"></a>带宽</h2><p>以带宽效率（实际最大吞吐量&#x2F;理论最大带宽）为标准。</p>\n<ol>\n<li>带宽效率很大程度上取决于 CXL 控制器的效率；</li>\n<li>真实的 CXL 内存写入操作带宽效率比模拟 CXL 内存高。</li>\n</ol>\n<h2 id=\"缓存层次交互\"><a href=\"#缓存层次交互\" class=\"headerlink\" title=\"缓存层次交互\"></a>缓存层次交互</h2><p>与访问本地内存相比，访问 CXL 内存时，更大的有效 LLC 容量会暴漏给 CPU 内核（可以利用其它 SNC 节点的 LLC 缓存）。</p>\n<h1 id=\"使用-CXL-对应用程序的性能影响\"><a href=\"#使用-CXL-对应用程序的性能影响\" class=\"headerlink\" title=\"使用 CXL 对应用程序的性能影响\"></a>使用 CXL 对应用程序的性能影响</h1><h2 id=\"延迟-1\"><a href=\"#延迟-1\" class=\"headerlink\" title=\"延迟\"></a>延迟</h2><p>内存密集型应用程序分配给页面不同比例的 CXL 内存时，应用程序的 P99 时延。看起来，CXL 内存时延大致是本地 DRAM 的 2 倍。</p>\n<p>对于 CXL 内存的分配方式，由于现有 SOTA 策略 TPP 的页面迁移开销，导致使用此页面分配方法时，延迟比静态分配（固定以 75% 概率分配给本地 DRAM，25% 概率分配给 CXL）还高。</p>\n<p>此外，端到端应用程序存在很多前端逻辑开销，因此访问 CXL 内存的长延迟对此类应用程序的影响较小。  </p>\n<h2 id=\"吞吐量\"><a href=\"#吞吐量\" class=\"headerlink\" title=\"吞吐量\"></a>吞吐量</h2><p><strong>将 100% 的页面分配给 DDR 内存时，当线程数到达 20 个，吞吐量达到饱和，而如果此时适当地分配一些页面给 CXL 内存，吞吐量可以进一步提高。</strong></p>\n<p>然而，如果本地 DDR 内存带宽没有跑满，此时将一些页面分配给 CXL，虽然总带宽增加了，但吞吐量会下降，因此，作者也就提出了一个动态页面分配策略，<strong>根据给定 CXL 内存设备的带宽能力和当前所有应用程序消耗的带宽，自动配置运行时分配给 CXL 内存的页面百分比。</strong></p>\n","categories":["CXL"],"tags":["论文阅读","CXL","Memory"]},{"title":"Ditto（SOSP 23）","url":"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/","content":"<p>内存缓存系统是云服务的重要组成部分。然而，由于服务器上 CPU 和内存的耦合，现有缓存系统无法以资源有效和快速的方式调整资源。</p>\n<p>作者建议将内存缓存系统移植到分离内存（DM）架构中以应对上述问题。但是，在分离内存上构建弹性缓存系统具有挑战：对远程内存的访问绕过了远程缓存服务器上的 CPU，会阻碍缓存算法（promotion 和 demotion）的执行；此外，计算和内存资源的弹性变化（并发客户端数量的改变，缓存容量的改变）会影响数据的访问模式，从而影响缓存算法的命中率。</p>\n<p>作者设计了 Ditto，第一个分离内存上的缓存系统，（1）提出以客户端为中心的缓存框架，仅依赖于远程内存访问，就可以在分离内存的计算池中高效地执行不同的缓存算法；（2）采用分布式自适应缓存机制（类 Cacheus）动态选择合适的缓存算法。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>内存缓存系统广泛应用于云服务中，以提高吞吐量和降低延迟。然而，由于云服务请求负载的动态变化和突发性，弹性（根据负载变化调整计算和内存资源的能力）对于内存缓存系统至关重要。</p>\n<p>然而现有的缓存系统构建和部署在将 CPU 和内存紧耦合的单体服务器上，这将导致动态资源调整上存在两个问题：（1）由于 CPU 和内存必须作为大规模集群上的固定大小虚拟机一起添加或减少，资源利用率收到影响（服务可能只想添加更多内存或 CPU 核心来增加缓存容量或请求吞吐量）；（2）调整资源需要进行长时间的数据迁移，导致资源调整的速度太慢，无法应对请求量的爆发；（<strong>现有问题：资源利用率低，资源分配调整速度慢</strong>）</p>\n<p>DM 有机会解决上述问题。它将单台服务器上的 CPU 和内存解耦为独立的计算和内存池，并使用高速互连技术（RDMA 和 CXL）将计算池和内存池互相连接。因此，CPU 和内存可以根据应用需求独立调整，从而提高资源利用率；此外，由于数据被计算池中的所有 CPU 核心共享，因此可以大大降低数据迁移的频率（只有在特殊情况才需要迁移）。</p>\n<p>但要在 DM 上实现缓存系统，必须解决两个挑战：</p>\n<ol>\n<li>绕过远程缓存服务器上的 CPU 后会阻碍缓存算法的执行。缓存算法通过各自的考量指标维护缓存对象的驱逐优先级，一旦数据访问改变了对象的热度，现有缓存算法<strong>依赖缓存服务器上的 CPU 监视对象热度并维护缓存队列数据结构。</strong>然而，在 DM 上的缓存系统中，计算池里的应用程序客户端在访问对象时将绕过内存池中的 CPU，导致在数据路径上无法更新请求对象的热度；此外如何高效地选择驱逐对象也是一个问题，缓存数据结构只能通过额外的高开销 RDMA 操作进行对象提升和降级，而这些数据结构在高并发场景下又会存在大量锁竞争。</li>\n<li>调整资源会影响缓存命中率。缓存命中率和工作负载的访问模式、缓存大小有关。在 DM 上，这两个属性都会随着动态资源的调整而变化。数据访问模式随着并发客户端数量（即计算资源）的变化而变化，缓存大小随着分配的内存空间（即内存资源）的变化而变化。因此，最大化命中率的最佳缓存算法会随着资源设置而动态变化。具有固定缓存算法的缓存系统无法适应 DM 的这些动态特征，并可能导致命中率较低。</li>\n</ol>\n<p>作者首先提出带有分布式热度监控、基于采样驱逐功能的以客户端为中心的缓存框架…使用 MAB 动态选择驱逐算法…</p>\n<p>贡献：</p>\n<ol>\n<li>讨论在 DM 上构建缓存系统的优势和挑战；</li>\n<li>提出以客户端为中心的缓存框架，不同的缓存算法可以被灵活地集成并高效地在 DM 上执行；提出一个采样友好地哈希表和频率计数缓存以提高框架效率；</li>\n<li>提出分布式自适应缓存，根据动态资源变化和数据访问模式改变选择最佳缓存算法；并通过轻量级驱逐历史和懒惰权重更新机制使得策略在 DM 上高效执行；</li>\n<li>合成+真实工作负载测试 Ditto 的性能。</li>\n</ol>\n<h1 id=\"Background-and-Motivation\"><a href=\"#Background-and-Motivation\" class=\"headerlink\" title=\"Background and Motivation\"></a>Background and Motivation</h1><h2 id=\"单体服务器上的缓存系统问题\"><a href=\"#单体服务器上的缓存系统问题\" class=\"headerlink\" title=\"单体服务器上的缓存系统问题\"></a>单体服务器上的缓存系统问题</h2><ol>\n<li>资源利用率低。单体服务器上现有的缓存服务的资源（例如 AWS 的 ElastiCache）被以固定大小的虚拟机粒度（1个 CPU，2GB DRAM）进行分配。然而，应用程序有时可能只需要 CPU 或者内存，导致资源利用率低。</li>\n<li>资源调整速度慢。现有内存缓存系统将数据分片到多个虚拟机，以利用更多的 CPU 和内存资源。然而，当新虚拟机添加到缓存集群后，缓存数据必须重新分片和迁移，将造成一定程度的性能下降；而在虚拟机回收时，又因为迁移将导致资源回收延迟。</li>\n</ol>\n<h2 id=\"分离内存\"><a href=\"#分离内存\" class=\"headerlink\" title=\"分离内存\"></a>分离内存</h2><p>分离内存被提出以降低总成本和改善云数据中心应用程序的弹性。它将单体服务器的计算和内存资源解耦为独立的计算和内存池，计算池中包含计算节点，具有丰富的 CPU 核心和少量用作运行时缓存的 DRAM，而内存池则具有足够的内存节点和计算能力较弱的控制器（1-2 个 CPU 核心）来执行管理任务（网络连接和内存管理）。<strong>计算节点和内存节点通过高带宽、微秒级延迟的 CPU 旁路互连（RDMA 和 CXL）连接，</strong>保证内存访问的性能要求。计算节点可以通过控制器提供的 ALLOC 和 FREE 接口来分配和释放内存池中可变大小的内存块。</p>\n<p>DM 解耦了计算和内存资源后，它们就可以以更细的粒度单独分配，并根据应用程序需求分配准确数量的资源，实现高资源利用率的目标；其次，因为内存池中的缓存数据可以被计算池中的所有计算节点访问，所以它可以大大降低数据迁移的频率，无需在扩容或缩容内存时进行数据迁，大大降低数据迁移成本，使得资源调整迅速生效。</p>\n<h1 id=\"Challenges\"><a href=\"#Challenges\" class=\"headerlink\" title=\"Challenges\"></a>Challenges</h1><h2 id=\"在-DM-上执行缓存算法\"><a href=\"#在-DM-上执行缓存算法\" class=\"headerlink\" title=\"在 DM 上执行缓存算法\"></a>在 DM 上执行缓存算法</h2><p>现有缓存算法都是为以服务器为中心的缓存系统设计的，所有数据都是被服务端的 CPU 访问和驱逐，这种设计模式无法应用于 DM，因为（1）DM 上的缓存系统是以客户端为中心的，所有客户端都是以 CPU 旁路的形式直接访问和驱逐数据的；（2）内存池上的计算能力太弱，难以在数据路径上执行缓存算法。这将导致两个问题。</p>\n<p>首先是如何在客户端为中心的环境下评估缓存对象的热度。现有缓存算法通过监视和统计所有数据访问来评估对象热度。由于单体缓存服务器上的 CPU 访问所有数据，所以可以在以服务器为中心的缓存系统上轻松实现监控。但是对于 DM，无法在内存池或客户端上监控缓存对象的访问，因为（1）RDMA 绕过了内存池中的 CPU，（2）<strong>计算池中的各个客户端不知道全局数据访问。</strong></p>\n<p>其次是如何在客户端高效地选择驱逐对象。DM 上的缓存数据结构必须由计算池中的客户端进行维护，导致需要多个 RTT 完成数据获取和缓存数据结构更新操作，导致维护缓存数据结构效率低，并且还可能存在大量的锁冲突。微秒级的锁延迟和多次锁失败导致的网络争用将严重成为缓存系统吞吐量的瓶颈。</p>\n<p>左图说明，由于额外的 RDMA 操作，导致 KVC 和 KVC-S 的吞吐率和延迟性能较差，右图说明由于锁竞争，它们的吞吐量差。</p>\n<img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/execute_cache_algo_on_dm_challenge.jpg\" class=\"\" title=\"execute_cache_algo_on_dm_challenge\">\n\n<h2 id=\"动态资源变化影响命中率\"><a href=\"#动态资源变化影响命中率\" class=\"headerlink\" title=\"动态资源变化影响命中率\"></a>动态资源变化影响命中率</h2><p>缓存命中率与数据访问模式、缓存大小密切相关。当动态调整计算和内存资源时，这两者都会受到影响。此时，具有固定缓存算法的缓存系统无法适应，可能导致命中率较低。</p>\n<ol>\n<li><p>改变计算资源影响命中率；</p>\n<p> 在 DM 的缓存系统上，应用程序在计算池中执行多个客户端线程去并发访问内存池中的缓存数据，缓存对象的访问模式将是所有应用程序访问模式的混合。</p>\n<p> 当计算资源变化时，一个应用程序的客户端线程数量也将变化，会改变访问模式的整体组合，并以两种方式影响单个缓存算法的命中率。</p>\n<ul>\n<li><p>首先，一个应用程序的数据访问占比随着其客户端线程的数量而变化；下图显示，随着 LRU 友好应用程序的客户端线程数占比增加，其命中率增高。（占比低时命中率低，应该是由于其它应用程序挤占了命中机会）</p>\n  <img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/different_application_client_affect_hit.jpg\" class=\"\" title=\"different_application_client_affect_hit\">\n</li>\n<li><p>其次，客户端的并发执行改变了工作负载的原始访问模式。左图坐标（0.25, 0.5）表示，使用 LRU 算法，在 50% 的工作负载上，它们相对命中率变化高于 0.25，即并发执行存在不确定性，同一工作负载命中率可能不稳定；右图显示，对于同一个应用程序，计算资源的变化将导致最优缓存算法改变。</p>\n  <img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/concurrent_number_affect_hit.jpg\" class=\"\" title=\"concurrent_number_affect_hit\"></li>\n</ul>\n</li>\n<li><p>改变内存资源影响命中率；</p>\n<p> 对于同一工作负载，改变内存资源会影响命中率，甚至于改变其访问模式（比如从搅拌变成 LFU 友好型）。</p>\n <img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/memory_resize_challenge.jpg\" class=\"\" title=\"memory_resize_challenge\"></li>\n</ol>\n<h1 id=\"Design\"><a href=\"#Design\" class=\"headerlink\" title=\"Design\"></a>Design</h1><h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><p>每个应用程序拥有一个本地 Ditto 客户端作为子进程，而每个 Ditto 客户端有多个线程，可以通过增加或删除分配给 Ditto 的线程数和 CPU 核心数来自由扩展计算资源。</p>\n<p>Ditto 客户端通过使用单边 RDMA 动作执行 get 或 set 操作。get 操作将使用一次 RDMA_READ 获取缓存对象在哈希表中的地址，再使用一次 RDMA_READ 从这个地址中获取对象的实际数据；set 操作需要通过一个 RDMA_READ 获取哈希表中的一个合适槽，一个 RDMA_WRITE 将新对象写入到内存池，一个 RDMA_CAS 原子修改槽中的指针。</p>\n<img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/overview.jpg\" class=\"\" title=\"overview\">\n\n<h2 id=\"以客户端为中心的缓存框架\"><a href=\"#以客户端为中心的缓存框架\" class=\"headerlink\" title=\"以客户端为中心的缓存框架\"></a>以客户端为中心的缓存框架</h2><p>以客户端为中心的缓存框架可以解决在 DM 上执行缓存算法时评估对象热度和选择驱逐对象的挑战。</p>\n<p>首先，Ditto 将每个对象与记录其全局访问信息（例如访问时间戳、频率等）的元数据相关联，在每次客户端执行 get 或 set 操作后，元数据同样由具有单边 RDMA 动作的客户端进行更新。在客户端，Ditto 提供了两个接口来继承缓存算法，分别是优先级接口（<code>double priority(Metadata)</code>）和元数据更新接口（<code>void update(Metadata &amp;)</code>）。优先级函数将对象的元数据映射到指示其热度的实际值，元数据更新函数在数据重用时更新元数据信息（频率、新近度等等）。通过实现这两个接口可以轻松集成多种缓存算法。</p>\n<p>其次，为了有效选择驱逐对象，Ditto 客户端对内存池中的缓存对象进行采样，并对采样对象应用优先级函数进行评估，驱逐优先级最低的对象。如此一来，就避免了维护缓存数据结构的昂贵开销。（<strong>一般的缓存算法往往需要一个哈希表和一个链表，MAB 使得需要 N 个哈希表和 N 个链表，而此时降低精确度，使用采样技术，又变成只需一个哈希表了，缓存对象的优先级顺序不再需要维护，只需通过 RDMA 更新其访问信息即可</strong>）</p>\n<p>为了有效在 DM 上执行此框架，<strong>Ditto 提出一个采样友好的哈希表和一个频率计数器缓存，以减少在 DM 上采样和记录访问信息的开销</strong>。</p>\n<h3 id=\"采样友好哈希表\"><a href=\"#采样友好哈希表\" class=\"headerlink\" title=\"采样友好哈希表\"></a>采样友好哈希表</h3><p>采样友好哈希表有助于减少 DM 上的采样和更新访问信息的开销，这是因为在 DM 上采样需要进行多次 RDMA_READ 操作，延迟很高；此外，更新访问信息也会影响整体吞吐量，因为额外的 RDMA 操作会消耗内存池中 RNIC  的有限消息速率。</p>\n<p>Ditto 将广泛使用的元数据与哈希索引中的槽存储在一起，同时保留其它不广泛的元数据（高级缓存算法需要）扩展功能。如此一来，通过直接获取哈希表中具有随机偏移量的连续槽，仅用一次 RDMA_READ 便可以进行采样（？）；其次，根据对象的更新频率组织它们的访问信息，从而减少更新对象元数据的 RDMA 操作数量，组织良好的访问信息可以使用单个 RDMA_WRITE 更新多个访问信息。</p>\n<ol>\n<li><p>哈希表结构</p>\n<p> 哈希表有多个桶，每个桶有多个槽，每个槽由两个部分组成，即原子部分和元数据部分。原子部分长度为 8 个字节，包含指向对象的指针，加速对象搜索的指纹和对象大小信息。在插入、更新或删除对象时使用 RDMA_CAS 进行原子修改；元数据部分记录了大多数缓存算法所需的访问信息，此外，还为分布式自适应缓存方案记录了一个附加的哈希字段（后续介绍）。</p>\n <img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/sample_friendly_hash_table.jpg\" class=\"\" title=\"sample_friendly_hash_table\">\n</li>\n<li><p>访问信息组织</p>\n<p> 首先，Ditto 通过区分本地信息和全局信息来减少必须包含在元数据中的访问信息数量。全局信息需要由所有客户端协作维护，因此必须包含在元数据中；而本地信息（比如延迟和成本）可以由分布式客户端在本地决定，无需包含在内。全局信息又可以分为有状态信息和无状态信息，无状态信息（比如插入时间和新近度）通过覆盖旧值进行更新，而有状态信息（比如频率）根据其旧值进行更新。</p>\n<p> Ditto 通过将无状态信息组织在一起，使用单个 RDMA_WRITE 进行更新；使用 RDMA_FAA 更新有状态信息。</p>\n</li>\n</ol>\n<h3 id=\"频率计数器缓存\"><a href=\"#频率计数器缓存\" class=\"headerlink\" title=\"频率计数器缓存\"></a>频率计数器缓存</h3><p>使用客户端频率计数器（FC）缓存进一步减少更新元数据的开销。对于采样友好的哈希表，更新元数据仍然需要两个 RDMA 操作，即更新无状态信息的 RDMA_WRITE 和更新有状态信息的 RDMA_FAA，这些 RDMA 操作会消耗 RNIC 的效率速率，从而限制 Ditto 的正太吞吐量；此外，由于 RNIC 内部锁的争用，在 DM 上执行 RDMA_FAA 的成本很高。FC 缓存旨在减少元数据更新时的 RDMA_FAA 数量。</p>\n<p>短时间内多个写入指令可能会针对同一内存区域，FC 缓存则是作为写 buffer，吸收短时间内对同一区域的写操作，将其转换为单个内存写操作，以节省内存带宽。具体来说，<strong>FC 缓存条目包含对象 ID，哈希表中槽的地址以及计数器增量值</strong>。每次访问对象时，其对频率计数器的更新都会缓冲在 FC 缓存中，对象在内存池哈希表中实际的频率计数器更新会被推迟，直到缓存条目被驱逐。</p>\n<p>缓存条目在两种情况会被驱逐：缓存容量不足，FIFO 驱逐；计数器增量大于阈值 t。在条目被驱逐时，根据记录的槽地址将缓冲的计数器值增加相应的值。如此一来，RDMA_FAA 的操作数最多减少至了 1&#x2F;t。</p>\n<h2 id=\"分布式自适应缓存\"><a href=\"#分布式自适应缓存\" class=\"headerlink\" title=\"分布式自适应缓存\"></a>分布式自适应缓存</h2><p>Ditto 提出一个分布式自适应缓存机制来适应工作负载变化和动态资源配置。</p>\n<p>在 DM 上实现自适应缓存，必须解决两个挑战。首先，由于访问远程数据结构开销较高，维护全局 FIFO 逐出历史的成本很高；其次，由于客户端需要同步获取更新的权重，在分布式客户端上管理专家权重开销很大。</p>\n<p>为了解决上述挑战，Ditto 将驱逐历史条目嵌入到具有轻量级逐出历史的哈希表中，以避免在 DM 上维护额外的 FIFO 队列；Ditto 提出一种懒惰权重更新方案，以避免客户端之间昂贵的同步开销。</p>\n<h3 id=\"轻量级驱逐历史\"><a href=\"#轻量级驱逐历史\" class=\"headerlink\" title=\"轻量级驱逐历史\"></a>轻量级驱逐历史</h3><p><strong>重用</strong>采样友好哈希表的槽来存储和索引历史条目，从而无需分配额外的空间和为历史条目构建额外的哈希索引；提出一种具有懒惰驱逐方案的逻辑 FIFO 队列，以有效地实现对历史条目的 FIFO 替换，从而无需维护额外的 FIFO 队列。</p>\n<ol>\n<li><p>嵌入历史条目</p>\n<p> size 域 0xFF 指代这是个驱逐历史条目，pointer 域不再存储对象的地址，而是一个历史 ID；insert_ts 域则是改为存储专家位图，指示先前是哪些专家决定驱逐该对象；hash 域记录被驱逐对象 ID 的哈希值，以检查驱逐历史中是否包含缓存缺失的对象。</p>\n <img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/lightweight_history.jpg\" class=\"\" title=\"lightweight_history\">\n</li>\n<li><p>逻辑 FIFO 队列</p>\n<p> 有一个 6 字节的全局循环计数器为新的历史条目生成历史 ID，所有客户端都知道它的地址。</p>\n <img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/logic_fifo.jpg\" class=\"\" title=\"logic_fifo\">\n\n<ul>\n<li>驱逐历史条目插入：当客户端决定从缓存中逐出对象时，客户端将首先通过在全局历史计数器上执行 RDMA_FAA 来获取历史 ID，并使计数器自动加一，然后发送 RDMA_CAS 以原子方式将驱逐对象的大小和指针分别修改为 0XFF 和获取的历史 ID，再使用 RDMA_WRITE 将专家位图异步写入槽元数据部分的 insert_ts 字段；</li>\n<li>懒惰历史条目驱逐：在客户端实行过期检查，如果全局历史计数器值为 v1，历史 ID 为 v2，并且 FIFO 历史大小为 L。当 v1 &gt; v2，则当 (v1 + 2^48^ - v2) mod 2^48^ &gt; L 时条目将无效。然而，实际的历史条目驱逐发生在有新的缓存对象（还是新的历史条目？）将要插入时，过期的槽被认为是空的，从而做到透明地驱逐历史条目。</li>\n<li>遗憾收集：遗憾被定义为客户端发现缓存中缺失的对象包含在驱逐历史中。客户端搜索对象时，会计算对象 ID 的哈希值，根据哈希值定位到一个桶，并迭代匹配桶中的槽，看看指向的对象是否与目标具有相同的对象 ID。在此过程中，客户但还会匹配桶中遇到的历史条目的哈希值，如果尚未找到对象但历史条目具有匹配的哈希值，则可以收集遗憾。</li>\n</ul>\n <img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/history_insert_evict.jpg\" class=\"\" title=\"history_insert_evict\"></li>\n</ol>\n<h3 id=\"懒惰专家权重更新\"><a href=\"#懒惰专家权重更新\" class=\"headerlink\" title=\"懒惰专家权重更新\"></a>懒惰专家权重更新</h3><p>当客户端发现遗憾时，需要降低决定驱逐该对象的专家的权重。然而，由于 DM 上高同步开销，更新和使用分布式客户端的专家权重会产生不可忽略的开销。懒惰权重更新方案的思想是让客户端在本地批处理遗憾并将权重更新懒惰地卸载到内存节点地控制器上，这样旧减少了更新权重的频率，避免了同步的开销，同时，由于更新不频繁，内存节点的弱控制器也不会成为瓶颈。</p>\n<img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/lazy_weight_update.jpg\" class=\"\" title=\"lazy_weight_update\">\n\n<p>上图展示了懒惰权重更新方案的流程。每个客户端在本地维护专家权重以做出驱逐决策，当客户端发现遗憾时，它根据历史条目中的专家位图将惩罚应用于本地专家权重，惩罚被记录在惩罚 buffer 中。当缓冲的惩罚超过阈值时，客户端通过基于 RDMA 的 RPC 请求将所有惩罚发送到保存专家权重的内存节点的控制器。在受到客户端的惩罚后，内存节点的控制器首先将惩罚应用于全局专家权重，然后将更新后的全局权重回复给所有客户端。</p>\n<p>为了减少通过网络传输惩罚的带宽消耗，Ditto 利用指数函数的属性（指数相加）压缩惩罚，从而使得惩罚列表变成单个值。</p>\n<p>尽管本地客户端的专家权重并不总和全局权重同步，但是作者说这个并没有影响 Ditto 的适应性。</p>\n<h2 id=\"Discussions\"><a href=\"#Discussions\" class=\"headerlink\" title=\"Discussions\"></a>Discussions</h2><p><strong>元数据扩展。</strong>如果有额外元数据（没有存储在哈希表中），执行 Get 和 Set 操作后，需要额外的 RDMA_WRITE 来异步更新额外元数据；在缓存驱逐时，需要额外的 RDMA_READ 获取额外元数据一同输入到优先级函数中。</p>\n<p><strong>元数据开销。</strong>每个历史条目占 40 字节，历史条目总数和缓存对象的最大数量相等；对于每个缓存对象，在哈希表中需要占据 40 字节；对于每个专家，需要一个 4 字节浮点数。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><p><strong>mark：如何将 trace 分发到多个客户端？</strong></p>\n<p>缓存驱逐时的采样大小：5（和 Redis 一样）；</p>\n<p>历史记录大小：Cache Capacity（和 LeCaR 一样）；</p>\n<p>FC Cache 的阈值：10，FC Cache 的容量：10 MB（实验结果评估）；</p>\n<p>MAB 学习率：0.1，本地 100 次的权重更新再传输给全局权重。</p>\n<h2 id=\"弹性\"><a href=\"#弹性\" class=\"headerlink\" title=\"弹性\"></a>弹性</h2><p>基于 DM 的缓存系统 Ditto 相比于基于传统单体服务器的 Redis 来说，资源利用率以及资源切换时的调整速度都更好。</p>\n<img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/elasticity.jpg\" class=\"\" title=\"elasticity\">\n\n<h2 id=\"效率\"><a href=\"#效率\" class=\"headerlink\" title=\"效率\"></a>效率</h2><p>为了评估 Ditto 在 DM 上可以高效执行缓存算法，作者在 YCSB 基准上评估了其与 Shard-LRU，CliqueMap 的吞吐量和尾延迟。Ditto 的性能瓶颈仅在 RNIC 的传输速率上；而 Shard-LRU 会随着客户端数量的增加，远程锁争用成为瓶颈；CliqueMap 的瓶颈则在于内存池的计算能力</p>\n<img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/efficiency.jpg\" class=\"\" title=\"efficiency\">\n\n<img src=\"/2023/12/29/Ditto%EF%BC%88SOSP%2023%EF%BC%89/more_cores_on_MN.jpg\" class=\"\" title=\"more_cores_on_MN\">\n\n<h2 id=\"适应性\"><a href=\"#适应性\" class=\"headerlink\" title=\"适应性\"></a>适应性</h2><h3 id=\"适应不同的真实工作负载\"><a href=\"#适应不同的真实工作负载\" class=\"headerlink\" title=\"适应不同的真实工作负载\"></a>适应不同的真实工作负载</h3><p>MAB 集成多种缓存算法，使得可以根据不同的访问模式切换最合适的缓存算法，能够适应大部分工作负载。</p>\n<h3 id=\"适应动态资源调整\"><a href=\"#适应动态资源调整\" class=\"headerlink\" title=\"适应动态资源调整\"></a>适应动态资源调整</h3><p>计算资源变化（客户端数量改变）和内存资源变化（缓存大小改变），Ditto 都能够很好地适应。</p>\n<h2 id=\"灵活性\"><a href=\"#灵活性\" class=\"headerlink\" title=\"灵活性\"></a>灵活性</h2><p>只需少量的额外代码，就可以集成多种缓存算法。</p>\n<h2 id=\"消融实验\"><a href=\"#消融实验\" class=\"headerlink\" title=\"消融实验\"></a>消融实验</h2><p>Design 部分各自的技术对于吞吐量的提升。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本文提出了 Ditto，第一个基于内存分离架构的缓存系统，可以实现更好的弹性。</p>\n<p>Ditto 解决了在 DM 上构建缓存系统的挑战，即<strong>执行现有以服务器为中心的缓存算法并处理动态变化的资源和数据访问模式导致的低命中率</strong>。具体地说，它通过提出一种以客户端为中心的缓存框架，以有效地在 DM 上执行缓存算法；提出一种分布式自适应缓存方案来适应资源和工作负载的变化。</p>\n<p>实验结果表明，Ditto 有效地适应了 DM 上的资源和工作负载变化，并且在 YCSB 合成工作负载上优于单片服务器上最先进的缓存系统高达 9 倍，在现实世界的键值跟踪上优于最先进的缓存系统 3.6 倍。</p>\n","categories":["Cache"],"tags":["Cache","论文阅读","内存分离"]},{"title":"GL-Cache（FAST 23）","url":"/2023/04/05/GL-Cache%EF%BC%88FAST%2023%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<p>Web 应用程序严重依赖软件缓存从而实现低延迟和高带宽。为适应工作负载的变化，近年提出了三种学习缓存（学习驱逐）：object-level learning，learning-from-distribution 和 learning-from-simple-experts。但是，这三种方法要么粒度太细（object-level），计算和存储开销比较大，要么就像其它两种一样粒度太粗，无法捕捉对象之间的差异而留下较大的效率提升空间。</p>\n<p>本文提出将相似对象集群到同一组并以组为单位执行学习和驱逐，<code>Learning at the group level accumulates more signals for learning, leverages more features with adaptive weights, and amortizes overheads over objects, thereby achieving both high efficiency and high throughput.</code></p>\n<p>与 SOTA 策略比较，GL-Cache 实现了高带宽和高命中率。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li><p>Large-scale cache deployments enable the success of today’s Internet.</p>\n</li>\n<li><p>The main driving force of cache deployments is the cache’s ability to serve data with high throughput and low latency. Retrieving data from a cache is thousands of times faster than retrieving it from the backend.</p>\n<ul>\n<li>但是缓存总是部署在价格昂贵、容量有限的介质中，缓存容量通常比数据集大小小得多，因此<strong>决定哪些数据存储在缓存中</strong>格外重要。</li>\n<li>高效缓存能够存储更有用得数据从而无需请求后端数据系统就能服务更多的请求。缓存有效性通常被命中率衡量。</li>\n<li>当缓存满时，需要使用驱逐算法决定哪些数据被保存，哪些数据被驱逐。因此，<strong>驱逐策略对于缓存效率非常重要</strong>。</li>\n</ul>\n</li>\n<li><p>这些年许多驱逐算法利用不同的对象特征做驱逐决策。比如一些 LRU 的变体使用新近度的不同角度选择驱逐对象；频率+新近度；频率+对象大小；</p>\n<ul>\n<li>不同特征对于不同的工作负载具有不同程度的重要性，因此只是使用特定方式组合一个或者两个对象特征通常也就只能在一些工作负载中实现高效率。</li>\n<li><strong>learned caches</strong>: employed  machine learning to improve cache evictions.</li>\n</ul>\n</li>\n<li><p>将 learned caches 分为三类</p>\n<ul>\n<li><p>object-level learning（LRB）：learns the next access time for each object using dozens of object features and evicts the object with the furthest predicted request time.</p>\n<ul>\n<li><strong>Advantage</strong>：leverages more object features, learns the relative feature importance, and performs fine-grained learning on each cached object, it has the highest potential for achieving high efficiency.</li>\n<li><strong>Disadvantage</strong>：predicting and ranking objects at each eviction incurs significant computation and storage overheads as we observe LRB suffers from a 775× slow down compared to LRU.</li>\n</ul>\n</li>\n<li><p>learning-from-distribution（LHD）：models request probability distributions to inform eviction decisions. For example, LHD measures object hit density using age and size, and evicts the object with the lowest hit density.</p>\n<ul>\n<li><strong>Advantage</strong>：a lower computation and storage overhead because it models request probability using fewer features at a coarser granularity.</li>\n<li><strong>Disadvantage</strong>：still has a lower throughput compared to simple heuristics (e.g., LRU) because it has to <strong>randomly sample and compare many objects at each eviction</strong>. Moreover, the existing design (e.g., LHD [7]) <strong>does not leverage object features other than age and size, limiting its potential for high efficiency</strong>.</li>\n</ul>\n</li>\n<li><p>learning-from-simple-experts（LeCaR, Cacheus）：performs evictions by choosing eviction candidates recommended by experts (e.g., LRU and LFU), and updates experts’ weights based on their past performance on the workload.</p>\n<ul>\n<li><strong>Disadvantage</strong>：受强化学习中的 “delayed rewards” 限制，在不佳驱逐和该对象再次访问之间，专家权重没有得到更新；性能取决于专家的选择，现有系统使用简单的专家，无法利用到专家未考虑的特征。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>为了克服现有 learned caches 的缺点，提出以对象组为粒度进行学习。Group-level learning leverages multiple group-level features to learn object-group utility for evictions.</p>\n<ul>\n<li>It reduces the computation and storage overheads of learning by hundreds of times through amortization compared to learning at the object level.</li>\n<li>object groups accumulate more “signals” for learning and can leverage a variety of features for prediction, enabling better eviction decisions.</li>\n</ul>\n</li>\n<li><p>组级别学习需要回答以下几个问题</p>\n<ul>\n<li><p>How to group objects and perform evictions efficiently? </p>\n<p>  答：GL-Cache clusters similar objects into groups using write time (§3.3) and evicts the least useful groups using a merge-based eviction(§3.6).</p>\n</li>\n<li><p>How to measure the usefulness of object groups (termed “utility”) to determine the best eviction candidate? </p>\n<p>  答：GL-Cache introduces a group utility function (§3.4) to rank groups, which enables group-based eviction to achieve similar efficiency as object-based eviction (§4.2).</p>\n</li>\n<li><p>How to learn and predict the object-group utility online?</p>\n<p>  答：(§3.5).</p>\n</li>\n</ul>\n</li>\n<li><p>本文贡献</p>\n<ul>\n<li>基于学习粒度将 learned caches 分为三类，并提出一个新的缓存学习方法—— group-level learning。</li>\n<li>设计并实现了 GL-Cache，解决了上面提到的组级别学习的挑战。同时也是第一次定义了 group-level utility function 用于缓存驱逐。</li>\n<li>使用生产跟踪评估 GL-Cache，展现了组级别学习的高效率和高吞吐量。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Background-and-motivation\"><a href=\"#Background-and-motivation\" class=\"headerlink\" title=\"Background and motivation\"></a>Background and motivation</h1><p>两个重要的缓存指标：<strong>efficiency</strong> measured using hit ratio, <strong>performance</strong> measured using throughput.</p>\n<h1 id=\"GL-Cache-Group-level-learned-cache\"><a href=\"#GL-Cache-Group-level-learned-cache\" class=\"headerlink\" title=\"GL-Cache: Group-level learned cache\"></a>GL-Cache: Group-level learned cache</h1><p>The key idea behind group-level learning is to learn the usefulness of groups of objects (called “utility”).</p>\n<h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><ol>\n<li>objects are clustered into fixed-size groups when writing to cache (§3.3).</li>\n<li>The training module in GL-Cache collects training data online and periodically trains a model to learn the utility of object groups (§3.5).</li>\n<li>The inference module predicts object-group utility and ranks object groups for eviction. When the cache is full, object groups are evicted using a merge-based eviction which merges multiple groups into one, evicts most objects, and retains a small portion of popular objects (§3.6).</li>\n</ol>\n<h2 id=\"Group-level-learning\"><a href=\"#Group-level-learning\" class=\"headerlink\" title=\"Group-level learning\"></a>Group-level learning</h2><ol>\n<li>优势<ul>\n<li><strong>Grouping amortizes overheads</strong>：these overheads are amortized over multiple objects in the group. The metadata overhead is only added for each group and the cost of inference computation is also amortized over objects.</li>\n<li><strong>Grouping accumulates more signal</strong>：大多缓存工作负载遵守 Zipf 分布，也就是说大部分的对象只会收到少量的请求，对象级的学习每个对象只能收到很少的信号；而将大量对象集群在一起，该集群就能收获更多的请求，从而更好地学习和预测。</li>\n</ul>\n</li>\n<li>挑战<ul>\n<li>How to cluster objects into groups (§3.3)?</li>\n<li>How to compare the usefulness of object groups (§3.4)?</li>\n<li>How to learn the utility of object groups (§3.5)?</li>\n<li>How to perform evictions at group level (§3.6)?</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Object-groups\"><a href=\"#Object-groups\" class=\"headerlink\" title=\"Object groups\"></a>Object groups</h2><ol>\n<li>当对象进入缓存时，对象的分组就被它的简单静态对象特征决定了，如时间、租户 ID、内容类型、对象大小等。本项工作专注于基于写入时间的分组，这在所有系统中都可用，具有通用性。</li>\n<li>相似时间写入的对象展现出相似的行为，它们的重用时间更接近。</li>\n</ol>\n<h2 id=\"Utility-of-object-groups\"><a href=\"#Utility-of-object-groups\" class=\"headerlink\" title=\"Utility of object groups\"></a>Utility of object groups</h2><p>定义一个函数用于评估 group 的实用性。</p>\n<ol>\n<li>大对象占据更多空间，因此越大的对象有更低的实用性；（从另一个角度看，大对象驱逐后可以留下更多的空间存放更多的对象，因此更“值得”驱逐）</li>\n<li>前向重用距离越大的对象实用性更低；</li>\n</ol>\n<p>但是对象的前向重用距离是无法实时获得的，因此作者提出使用 GL-Cache 学习一个模型从而基于特征来预测对象组的实用性。</p>\n<h2 id=\"Learning-object-group-utility-in-GL-Cache\"><a href=\"#Learning-object-group-utility-in-GL-Cache\" class=\"headerlink\" title=\"Learning object-group utility in GL-Cache\"></a>Learning object-group utility in GL-Cache</h2><ol>\n<li>静态特征：request rete, write rate, miss ratio and mean object size；</li>\n<li>动态特征：age, #requests and #requested objects；</li>\n<li>学习模型：<ul>\n<li>gradient boosting machines (GBM) because tree models do not require feature normalization.</li>\n<li>将学习任务指定为一个回归问题，最小化对象组效用的均方损失。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Evictions-of-object-groups\"><a href=\"#Evictions-of-object-groups\" class=\"headerlink\" title=\"Evictions of object groups\"></a>Evictions of object groups</h2><p>GL-Cache 在组级别使用重量级学习（均摊开销）从而识别出最佳驱逐组，再在驱逐组内充分利用轻量级对象指标（新近度、大小）保留一些非常有用的对象。两层驱逐使得 GL-Cache 能够在学习开销和缓存效率之间实现卓越的平衡。</p>\n<p><strong>key insight：组级别驱逐的命中率竟接近 Belady，没有带来命中率的大幅下降，而这种一次性驱逐多个对象可以分摊推理开销</strong>。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><ol>\n<li><p>Will group-based eviction limit the efficiency upper bound when compared to object-based eviction ? </p>\n<p> <strong>组驱逐并不会成为实现高效率的瓶颈</strong></p>\n <img src=\"/2023/04/05/GL-Cache%EF%BC%88FAST%2023%EF%BC%89/similar_hit_with_min.jpg\" class=\"\" title=\"similar_hit_with_min\">\n</li>\n<li><p>Can GL-Cache improve hit ratio and efficiency over other learned caches ?</p>\n</li>\n<li><p>Can GL-Cache meet production-level throughput requirements and how much overhead does GL-Cache add ? </p>\n</li>\n<li><p>How does GL-Cache improve efficiency without compromising throughput ?</p>\n</li>\n</ol>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"HALP（NSDI 23）","url":"/2023/04/25/HALP%EF%BC%88NSDI%2023%EF%BC%89/","content":"<p>本文的主要想法是：通过启发式算法选择驱逐候选集，再使用学习策略从中选择最佳驱逐对象。这样做可以减少 ML 预测的计算开销。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>learned caches 部署在大规模生产环境中的问题：</p>\n<ol>\n<li>对比于启发式算法，learned caches 训练和预测的计算开销太大了；</li>\n<li>大规模的 CDN 生产环境，即使是小部分区域的缓存性能下降也是不能接受的，因此，在 learned caches 效果不佳时，需要做好兜底操作；</li>\n<li>生产环境中即使是同一机架的机器缓存缺失率也不一样，如何准确评估一个新算法在 production noise 上的效果。</li>\n</ol>\n<h1 id=\"Design\"><a href=\"#Design\" class=\"headerlink\" title=\"Design\"></a>Design</h1><img src=\"/2023/04/25/HALP%EF%BC%88NSDI%2023%EF%BC%89/design.jpg\" class=\"\" title=\"design\">\n\n<p>利用启发式算法从缓存尾部选取 4 个候选集，再以锦标赛方式进行 3 对比较选择最佳驱逐对象。</p>\n<p>1% 的机器使用原有启发式算法，其余机器使用 HALP，因此，可以监控 HALP 对性能的影响，从而及时切换为原有启发式算法。</p>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"Hawkeye (ISCA 16)","url":"/2023/09/10/Hawkeye-ISCA-16/","content":"<p>在片上缓存场景下，将 OPT 缓存替换算法应用于过去的访问请求，从中学习以指导未来的缓存替换决策。</p>\n<p>“<strong>If past behavior is a good predictor of future behavior, then our replacement policy will approach the behavior of Belady’s algorithm.</strong>“</p>\n<h1 id=\"Hawkeye\"><a href=\"#Hawkeye\" class=\"headerlink\" title=\"Hawkeye\"></a>Hawkeye</h1><h2 id=\"overview\"><a href=\"#overview\" class=\"headerlink\" title=\"overview\"></a>overview</h2><img src=\"/2023/09/10/Hawkeye-ISCA-16/overview.jpg\" class=\"\" title=\"overview\">\n\n<p>OPTgen 使用过去的访问记录，模拟 OPT 行为以产生训练数据；</p>\n<p>Hawkeye Predictor 对新插入对象进行分类：缓存友好和缓存厌恶。</p>\n<h2 id=\"OPTgen\"><a href=\"#OPTgen\" class=\"headerlink\" title=\"OPTgen\"></a>OPTgen</h2><p>三个定义</p>\n<ol>\n<li>usage interval：从对某数据的引用 X 开始，到下次对相同数据的引用 X’ 的长度；</li>\n<li>liveness interval：在 OPT 策略下，缓存行在缓存中的驻留时间段；</li>\n<li>occupancy vector：记录每个时刻使用的缓存容量。</li>\n</ol>\n<p>如果在 X 的 usage interval 过程中，任何时刻其它对象的 liveness intervals 重叠数目达到了缓存容量，则在下一次的引用 X’ 时，该数据缺失；否则，该数据命中。（对于 Belady 的理解，最直观的是在缓存驱逐决策过程中，驱逐从此时开始，缓存空间中前向重用距离最大的对象，<strong>而这里给了一个全新的视角，某个对象引用的 usage interval 过程中，其它对象的 liveness intervals 最大重叠数达到了缓存容量，这意味着在此最大重叠时刻，该对象的前向重用距离就已经是最大的了，这从缓存驱逐决策的视角转化为了对象下次重用时是否命中的判断</strong>）。</p>\n<p>对于下图所示，X 第一次引用时的 usage interval 为 6，重用距离为 3，但是，在 liveness interval 过程中，其它对象的 liveness intervals 最大重叠数仅为 1，因此，在第一次引用时，OPT 的判断为 X 下次引用将命中。</p>\n<img src=\"/2023/09/10/Hawkeye-ISCA-16/OPTgen_example.jpg\" class=\"\">\n\n<p>对于下面的访问 trace，OPT 策略使得缓存命中数为 4（<strong>实际上这是带准入的 OPT——当缓存容量满时，如若当前缺失对象的前向重用距离大于缓存中的所有对象，则不会准入，这比不关心准入的一般 OPT 命中率更高</strong>），其中，occupancy vector 的使用方式如下</p>\n<ol>\n<li>最近引用的条目对应值为 0；（如果不考虑准入，则条目初始值为 1）</li>\n<li>某对象的首次引用不会对 occupancy vector 的其他值造成影响，这反映出 OPT 只能基于对象的下次重用做出判断；</li>\n<li>如果对象不是首次引用，OPTgen 将检查该对象对应 usage interval 过程中是否每个引用在 occupancy vector 中的值（等于 liveness intervals  重叠数）都小于缓存容量。如果是，则 OPT 在该对象上次引用时会将其加入缓存，此次访问命中，并且将 usage interval 过程中每个引用在 occupancy vector 中的值加 1；否则，此次访问缺失，无需更新其它引用在 occupancy vector 的值。</li>\n</ol>\n<img src=\"/2023/09/10/Hawkeye-ISCA-16/OPTgen_illustrate.jpg\" class=\"\" title=\"OPTgen_illustrate\">\n\n<h2 id=\"Hawkeye-Predictor\"><a href=\"#Hawkeye-Predictor\" class=\"headerlink\" title=\"Hawkeye Predictor\"></a>Hawkeye Predictor</h2><p>“为何可以做预测” 基于这样一个事实：给定执行 load 命令的 PC，OPT 所做出的决策是相似的。文中提到，90.4% 的 load PC，在 OPT 看来有相同的缓存行为（缓存友好和缓存厌恶）。</p>\n<p>文中提到，预测器有 8K 个条目，并使用 13-bit 的 hashed PC 进行索引，每个条目有一个 3-bit 计数器。如果 OPTgen 确定 X 在 OPT 策略下将是缓存命中，则增大上次访问 X 的 PC 对应条目的计数器，否则，减小计数器。</p>\n<p>对于每次访问，预测器都通过当前的 load 指令 PC 进行索引，对应条目的 3-bit 计数器的高位标识其加载的数据是否缓存友好。</p>\n<h2 id=\"Replacement\"><a href=\"#Replacement\" class=\"headerlink\" title=\"Replacement\"></a>Replacement</h2><p>在缓存替换时，Hawkeye 总体目标是先驱逐缓存厌恶对象，如果没有，则驱逐最久没有访问的缓存友好行（LRU），并减小其在预测器中的对应计数器。</p>\n<p>进一步，作者将 Hawkeye 与 RRIP 结合，使用一个 3-bit 的计数器来标志驱逐优先级，具体做法是</p>\n<ol>\n<li>RRIP 计数器值越大，驱逐优先级越高；</li>\n<li>在每次缓存访问时，Hawkeye 也测其生成一个二进制预测来指示该对象是否缓存友好，如果缓存友好，则赋予其 RRIP&#x3D;0，同时，如果该对象不在缓存，还会增加其它 RRIP&lt;6 的缓存对象的 RRIP 计数器（有点像 CLOCK）；如果缓存厌恶，则赋予其 RRIP&#x3D;7；</li>\n<li>在缓存替换时，任何 RRIP&#x3D;7（缓存厌恶）对象都会被选为驱逐候选者，而如果没有，则驱逐 RRIP 值最大的对象（LRU）。</li>\n</ol>\n<p>以上确保了缓存厌恶对象会首先被驱逐（因为缓存友好对象的 RRIP 计数器最多增长到6）。</p>\n<h1 id=\"与-LRB-的异同\"><a href=\"#与-LRB-的异同\" class=\"headerlink\" title=\"与 LRB 的异同\"></a>与 LRB 的异同</h1><p>相同点</p>\n<ol>\n<li>Hawkeye 使用了 8X 缓存容量的时间窗口，LRB 使用了 2X 缓存容量的时间窗口，它们本质上都是希望从过去的时间窗口内探寻对象的缓存行为；</li>\n<li>在进行驱逐决策时，Hawkeye 驱逐 RRIP 值最大的对象，该值是否为7标识了是否为缓存厌恶，小于7时，其大小则反映了对象的新近度；LRB 驱逐缓存对象中重用距离预测值最大的对象。</li>\n</ol>\n<p>不同点</p>\n<ol>\n<li>Hawkeye 将 OPT 的决策看成一个二分类问题，并使用精彩的 liveness intervals 重叠数来非常准确地模拟出了 OPT 的行为，考虑了前向重用距离和缓存的需求（重叠数）；LRB 将 OPT 决策看成一个回归问题，在时间窗口中简单地使用引用距离来作为标签进行训练；</li>\n<li>Hawkeye 应用于片上缓存，特征仅为加载指令的 PC 值，标签为在 OPT 策略下是否缓存命中；LRB 应用于 CDN 缓存，特征非常多，包括指数衰减新近度、频率、对象大小等等，标签为引用距离，而对于引用距离大于时间窗口（2X 缓存大小）的对象，引用距离即 2X 缓存大小。</li>\n</ol>\n<h1 id=\"收获\"><a href=\"#收获\" class=\"headerlink\" title=\"收获\"></a>收获</h1><p>对于学习缓存，Hawkeye 和 LRB 这两篇不同应用场景下的缓存替换经典工作揭示了学习缓存过程中非常重要的几个问题</p>\n<ol>\n<li>如何建模缓存问题？（这两个工作都是缓存替换，因此这里就是建模缓存替换问题，它们都是将问题建模成从 OPT 算法中学习驱逐的对应关系，Hawkeye 驱逐过去在 OPT 策略下不会缓存命中的对象，LRB 驱逐过去在 OPT 策略下引用距离过大的对象）</li>\n<li>如何获取训练数据？（使用和缓存大小长度相关的时间窗口捕获历史 trace）</li>\n<li>特征和标签是什么？（PC，OPT 策略下是否缓存命中）以及（新近度、频率、对象大小等特征，重用距离）</li>\n<li>如何决定驱逐对象？（使用预测器对缓存对象进行预测，驱逐优先级最高的对象，<strong>对于缓存替换，这一步开销太大了</strong>）</li>\n</ol>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"LHD（NSDI 18）","url":"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/","content":"<h1 id=\"动机\"><a href=\"#动机\" class=\"headerlink\" title=\"动机\"></a>动机</h1><ol>\n<li>启发式缓存替换算法对工作负载行为做了强烈的隐式假设，无法适应不同场景以及负载的动态变化，严重影响了命中率性能。</li>\n<li>prior policies rely on implementation primitives that unnecessarily limit their design.（复杂缓存策略需要复杂的数据结构，例如 GDSF 使用 heap，缓存插入和驱逐的时间复杂度较高；LRU 则是因为需要进行同步所以吞吐量也很低）</li>\n</ol>\n<h1 id=\"Least-Hit-Density-LHD\"><a href=\"#Least-Hit-Density-LHD\" class=\"headerlink\" title=\"Least Hit Density (LHD)\"></a>Least Hit Density (LHD)</h1><img src=\"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/%E5%AE%9A%E4%B9%89.jpg\" class=\"\" title=\"定义\">\n\n<p>从式子来看，命中概率越大，对象越应该保留在缓存；对象越大，占用更大的空间，生命周期越长，占用更久的空间，给缓存总体的 OHR 带来负面影响。特别地，当缓存对象命中或驱逐时，它的 “生命周期” 都算作结束。而 LHD 做的事情，就是动态预测每个对象的 expected hits-per-space-consumed（hit density），驱逐密度最低的对象。</p>\n<p><strong>挑战：在不知道对象未来的命中次数、生命周期的情况下，如何预测对象的命中密度。</strong></p>\n<img src=\"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/%E5%91%BD%E4%B8%AD%E5%AF%86%E5%BA%A6.jpg\" class=\"\" title=\"命中密度\">\n\n<p>上图中的公式(5)表示：当对象已经在缓存存活 a，它的命中密度。其中分子是对象在 [1, ∞] 时间段的命中概率之和，也就是<strong>期望命中概率</strong>，分母则是对象大小和<strong>期望剩余生命周期</strong>的乘积。</p>\n<p>因此，LHD 可以在线监控所有对象的 H 和 L 的分布，分布中的每个点表示，在生命周期 x 时命中以及在生命周期 x 时命中或驱逐的概率。有了这两个分布，就可以通过上图中的公式(5)计算所有缓存对象的命中密度（所有缓存对象已经在缓存中待的时间作为元数据保留）。</p>\n<p>更进一步，如果有额外信息，可以对额外信息中表示的每个类别计算一组 H 和 L 分布，如对象的访问次数。</p>\n<img src=\"/2023/05/18/LHD%EF%BC%88NSDI-18%EF%BC%89/%E9%99%84%E5%B8%A6%E9%A2%9D%E5%A4%96%E4%BF%A1%E6%81%AF%E7%9A%84%E5%91%BD%E4%B8%AD%E5%AF%86%E5%BA%A6.jpg\" class=\"\" title=\"附带额外信息的命中密度\">\n\n","categories":["Cache"],"tags":["Cache","论文阅读"]},{"title":"Large-scale Analysis at Twitter Cache Clusters（TOS 21）","url":"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>现代 Web 服务广泛使用内存缓存来提高吞吐量并减少访问延迟；</li>\n<li>对生产系统进行工作负载分析，可以推动提高内存缓存系统有效性的研究；</li>\n<li>本文对 Twitter 的多个内存缓存集群进行分析，比如生命周期、流行度分布、大小分布等等。</li>\n</ol>\n<h1 id=\"Production-Stats-and-Workload-Analysis\"><a href=\"#Production-Stats-and-Workload-Analysis\" class=\"headerlink\" title=\"Production Stats and Workload Analysis\"></a>Production Stats and Workload Analysis</h1><h2 id=\"Miss-Ratio\"><a href=\"#Miss-Ratio\" class=\"headerlink\" title=\"Miss Ratio\"></a>Miss Ratio</h2><img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E7%BC%BA%E5%A4%B1%E7%8E%87%E5%8F%98%E5%8C%96.jpg\" class=\"\" title=\"缺失率变化\">\n\n<h2 id=\"Request-Rate\"><a href=\"#Request-Rate\" class=\"headerlink\" title=\"Request Rate\"></a>Request Rate</h2><p>请求率呈现昼夜特征，同时，请求率的尖峰可能是由于热键，也可能是请求对象增多了。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E8%AF%B7%E6%B1%82%E7%8E%87%E5%8F%98%E5%8C%96.jpg\" class=\"\" title=\"请求率变化\">\n\n<h2 id=\"Types-of-Operations\"><a href=\"#Types-of-Operations\" class=\"headerlink\" title=\"Types of Operations\"></a>Types of Operations</h2><p>对于多 trace 的数据集分析和评估，图 a 挺棒的。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E6%93%8D%E4%BD%9C%E7%B1%BB%E5%9E%8B.jpg\" class=\"\" title=\"操作类型\">\n\n<h2 id=\"Time-To-Living（TTL）\"><a href=\"#Time-To-Living（TTL）\" class=\"headerlink\" title=\"Time-To-Living（TTL）\"></a>Time-To-Living（TTL）</h2><p>TTL 相关的累计分布曲线。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/TTL%E7%9A%84%E7%B4%AF%E8%AE%A1%E5%88%86%E5%B8%83.jpg\" class=\"\" title=\"TTL的累计分布\">\n\n<p>不考虑 TTL，工作集大小是当前时间之前所有不同对象大小之和，对于 Web 场景，是无限递增的；但若是考虑了 TTL，工作集大小是当前时间之前所有未过期的不同对象大小之和，随着时间的推移，会趋向于平稳，因此，无限大的缓存容量是没有意义的。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/TTL%E5%AF%B9%E5%B7%A5%E4%BD%9C%E9%9B%86%E5%A4%A7%E5%B0%8F%E7%9A%84%E5%BD%B1%E5%93%8D.jpg\" class=\"\" title=\"TTL对工作集大小的影响\">\n\n<h2 id=\"Popularity-Distribution\"><a href=\"#Popularity-Distribution\" class=\"headerlink\" title=\"Popularity Distribution\"></a>Popularity Distribution</h2><p>一些负载在最流行部分和最不流行部分展现出和齐夫分布的差异。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E6%B5%81%E8%A1%8C%E5%BA%A6.jpg\" class=\"\" title=\"流行度\">\n\n<h2 id=\"Object-Size\"><a href=\"#Object-Size\" class=\"headerlink\" title=\"Object Size\"></a>Object Size</h2><p>推特数据集中的对象大多很小。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E5%AF%B9%E8%B1%A1%E5%A4%A7%E5%B0%8F.jpg\" class=\"\" title=\"对象大小\">\n\n<p>请求大小随时间变化的热力图。（相比将几个 CDF 曲线画在一个图里，用热力图可以看到更细粒度的差距）</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E8%AF%B7%E6%B1%82%E5%A4%A7%E5%B0%8F%E5%88%86%E5%B8%83%E9%9A%8F%E6%97%B6%E9%97%B4%E5%8F%98%E5%8C%96.jpg\" class=\"\" title=\"请求大小分布随时间变化\">\n\n<h2 id=\"Reuse-Distance\"><a href=\"#Reuse-Distance\" class=\"headerlink\" title=\"Reuse Distance\"></a>Reuse Distance</h2><p>对于图 a，是一个经典适用 LRU 替换算法的重用距离累计分布，展现出最近访问的对象，有更高的概率被再次访问；而对于图 b，10^6^ 附近的重用距离占比很高，小于这附近值的缓存大小使用 LRU 效果会很差，因为 LRU 会驱逐重用距离为 10^6^ 附近的对象，而选择保留重用距离为 10^2^ ~ 10^5^ 的对象，FIFO 则平等对待所有对象，不会牺牲全部的较大重用距离对象而选择保留全部重用距离较小的对象。</p>\n<img src=\"/2023/04/17/Large-scale%20Analysis%20at%20Twitter%20Cache%20Clusters%EF%BC%88TOS%2021%EF%BC%89/%E9%87%8D%E7%94%A8%E8%B7%9D%E7%A6%BB%E7%9A%84CDF.jpg\" class=\"\" title=\"重用距离的CDF\">","categories":["Cache"],"tags":["Cache","论文阅读","Plot","Cache Trace Analysis"]},{"title":"LRB（NSDI 20）","url":"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/","content":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li><p>内容分发网络承担了绝大部分的网络流量；</p>\n</li>\n<li><p><strong>字节缺失率</strong>是内容分发网络的关键性能指标；</p>\n</li>\n<li><p>CDN 场景下，缓存策略对于实现低缺失率起着关键作用，但是，CDN 场景下的缓存策略大多使用启发式算法，随着工作负载的变化，缓存策略的效果可能会很差。最关键的是，作者在很多 trace 中观察到：<strong>SOTA缓存替换算法和 MIN 算法之间存在巨大差距</strong>；</p>\n</li>\n<li><p>作者提出 LRB 算法，使用机器学习近似 MIN 算法。但是，如果<strong>完全模仿 MIN 算法驱逐前向重用距离最大的对象，那么计算成本就太大了</strong>。退而求其次，<strong>只要某个对象的前向重用距离超过某一阈值，它就可以被驱逐</strong>。</p>\n</li>\n<li><p>阈值—— Belady 边界，定义为从开始到现在，MIN 算法驱逐对象中前向重用距离最小的值。Relaxed Belady 带来了两个好处：</p>\n<ul>\n<li>允许系统在小的采样集中进行预测，比如大小为64，减少计算开销；</li>\n<li>允许系统实时获取训练数据（只需在后面实时跟踪驱逐对象的重用距离是否大于阈值，而无需跟踪驱逐对象是否前向重用距离最大，大大节省了内存开销）从而快速适应工作负载变化；</li>\n</ul>\n</li>\n<li><p><strong>端到端的评估指标，例如字节缺失率，模拟起来非常花费时间</strong>。因此，作者提出使用 <strong>good decision ratio</strong> 指标来评价 ML 架构（包括特征、模型、预测目标和损失函数）的好坏，定义为驱逐对象的前向重用距离实际上是否超过了 Belady 边界。使用该指标，就可以在一次模拟中，收集训练数据、预测数据、学习 Belady 边界。</p>\n</li>\n<li><p>作者抛出的设计问题：</p>\n<ul>\n<li><p>控制 ML 训练和预测的计算开销；</p>\n</li>\n<li><p>限制训练和预测过程中的内存开销；</p>\n</li>\n</ul>\n<ul>\n<li>如何在线获取训练数据；</li>\n</ul>\n<ul>\n<li>如何选择替换候选集；</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Background-and-Motivation\"><a href=\"#Background-and-Motivation\" class=\"headerlink\" title=\"Background and Motivation\"></a>Background and Motivation</h1><ol>\n<li>CDN 缓存缺失，将花费高额带宽代价去后端存储中获取原数据，因此需要最小化缓存的字节命中率；</li>\n<li>CDN 缓存的工作集比容量大很多；</li>\n<li>作者观察到现有 SOTA 缓存替换算法和 MIN 算法之间仍存在巨大差距（这点不存在的话，就没有做的必要了）；</li>\n<li>启发式算法难以适应工作负载的变化；</li>\n<li>实际 CDN 服务器的 CPU 占用率很低，</li>\n</ol>\n<h1 id=\"Approximating-Belady’s-MIN-Algorithm\"><a href=\"#Approximating-Belady’s-MIN-Algorithm\" class=\"headerlink\" title=\"Approximating Belady’s MIN Algorithm\"></a>Approximating Belady’s MIN Algorithm</h1><h2 id=\"Relaxed-Belady-Algorithm\"><a href=\"#Relaxed-Belady-Algorithm\" class=\"headerlink\" title=\"Relaxed Belady Algorithm\"></a>Relaxed Belady Algorithm</h2><p>完全模仿 MIN 算法存在以下问题：</p>\n<ol>\n<li>需要对所有缓存对象进行预测；</li>\n<li>ML 预测器需要准确地预测每个对象的前向重用距离；</li>\n</ol>\n<p>近似 Belady 则是对上面两个问题的开销进行了妥协，从驱逐候选集前向重用距离预测结果大于 Belady 边界的对象中随机驱逐，如果没有，则继续对其余所有对象进行预测，变成完全模拟 Belady。</p>\n<p>对于 Belady 边界来说，越小则跟踪的开销越小；越大则使 relaxed belady 的字节缺失率越接近 MIN。</p>\n<h2 id=\"Belady-Boundary\"><a href=\"#Belady-Boundary\" class=\"headerlink\" title=\"Belady Boundary\"></a>Belady Boundary</h2><p>预热阶段更新 Belady 边界，而在实际运行中，假设边界近似静止。</p>\n<h2 id=\"Good-Decision-Ratio\"><a href=\"#Good-Decision-Ratio\" class=\"headerlink\" title=\"Good Decision Ratio\"></a>Good Decision Ratio</h2><p>为了使算法做出更好的决策，需要一个指标来衡量单个决策的好坏，而端到端的指标，例如字节缺失率，只能反映大量决策的聚合好坏。</p>\n<p>作者发现，# good decisions &#x2F; # total eviction decisions 与字节缺失率密切相关。</p>\n<h1 id=\"Design-of-Learning-Relaxed-Belady-Cache\"><a href=\"#Design-of-Learning-Relaxed-Belady-Cache\" class=\"headerlink\" title=\"Design of Learning Relaxed Belady Cache\"></a>Design of Learning Relaxed Belady Cache</h1><h2 id=\"Past-information\"><a href=\"#Past-information\" class=\"headerlink\" title=\"Past information\"></a>Past information</h2><p>使用更多数据可以提高训练质量，但会带来更多的内存开销。</p>\n<p>对于小缓存，使用尽可能长的滑动窗口包含足够多的最近请求以做出更好的决策；对于大缓存，使用最小二乘法回归线拟合小缓存大小与其最佳滑动窗口大小的关系，从而得到大缓存对应的滑动窗口大小。（在论文后面有说，滑动窗口的长度近似于 Belady 边界，在实现中也是这样做的）</p>\n<h2 id=\"Training-Data\"><a href=\"#Training-Data\" class=\"headerlink\" title=\"Training Data\"></a>Training Data</h2><p>获取训练数据这一部分细节挺多。</p>\n<ol>\n<li>数据的特征在第一次访问时就可以得到，但是数据的标签只有在重用时才能知道；</li>\n<li>需要从请求滑动窗口中随机采样不同对象获取训练数据；<ul>\n<li>不能随机采样请求，否则会导致偏向流行对象；</li>\n<li>不能采样缓存中的对象，否则也会导致偏向流行对象；</li>\n</ul>\n</li>\n<li>只需后向重用距离大于 Belady 边界就可以安全地打标签了。有些对象重用距离太长，持续跟踪会导致过多的内存开销，甚至有些对象永远不会被重用，然而这些对象对于训练的质量起着非常重要的作用，将它们的重用距离设置为 2*Belady边界 的值。</li>\n</ol>\n<h2 id=\"ML-Architecture\"><a href=\"#ML-Architecture\" class=\"headerlink\" title=\"ML Architecture\"></a>ML Architecture</h2><p>三种类型的特征：</p>\n<ol>\n<li>Deltas：delta1 表示对象的后向重用距离，delta2 表示对象最近两次重用之间的距离，… 。Deltas 在启发式中广泛应用，例如 LRU，LRU-K 和 S4LRU；（新近度）</li>\n<li>Exponentially decayed counters (EDCs)：每个 EDC 计数器近似该对象一段时间内的流行度。（频率）</li>\n<li>Static features：对象大小、对象类型</li>\n</ol>\n<p>模型：对比了逻辑回归，线性回归，SVM，轻量级神经网络，发现 GBM 最好，且无需特征归一化，能够有效处理缺失值（Deltas）以及训练和测试效率都很高。</p>\n<p>预测目标：log (time-to-next-request)</p>\n<p>损失函数：L2</p>\n<p>训练集大小：128K。一旦积累了 128K 带标签的数据，就会训练一个新的模型。128K 是决策率向训练时间和开销的权衡。</p>\n<p><strong>以上指标的确定，基本都是实验佐证再加以启发式的直觉解释。</strong></p>\n<h2 id=\"Eviction-Candidate-Selection\"><a href=\"#Eviction-Candidate-Selection\" class=\"headerlink\" title=\"Eviction Candidate Selection\"></a>Eviction Candidate Selection</h2><p>候选集大小为 64，从缓存中随机选择 64 个对象进行批量预测，如果最大值超过 Belady 边界，驱逐它；否则，变成对所有缓存对象进行预测并驱逐预测最大值的对象。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><ol>\n<li><p>与 ATS 生产系统进行比较，LRB 降低了多少网络流量；</p>\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E4%B8%8EATS%E7%9A%84%E7%BC%BA%E5%A4%B1%E7%8E%87%E6%AF%94%E8%BE%83.jpg\" class=\"\" title=\"与ATS的缺失率比较\">\n</li>\n<li><p>与 ATS 相比，LRB 的开销；</p>\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E5%BC%80%E9%94%80.jpg\" class=\"\" title=\"开销\">\n\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E5%85%83%E6%95%B0%E6%8D%AE%E5%8D%A0%E7%BC%93%E5%AD%98%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%AF%94%E4%BE%8B.jpg\" class=\"\" title=\"元数据占缓存大小的比例\">\n</li>\n<li><p>与 SOTA 算法比较</p>\n <img src=\"/2023/04/20/LRB%EF%BC%88NSDI%2020%EF%BC%89/%E4%B8%8ESOTA%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83.jpg\" class=\"\" title=\"与SOTA算法比较\"></li>\n</ol>\n","categories":["Cache"],"tags":["Cache","ML","论文阅读"]},{"title":"NHC（FAST 21）","url":"/2024/01/18/NHC%EF%BC%88FAST%2021%EF%BC%89/","content":"<p>缓存或者分层思想在性能差异非常大的存储设备上能够很好地应用，尽可能将所有热数据放置在性能层，从而尽最大限度地将请求分发到性能层，使得全局性能接近性能层的峰值。</p>\n<p>然而，现代存储设备之间的性能差异逐渐缩小，比如 DRAM&#x2F;PM 的性能非常接近，此时如果仍尽力将请求分发到性能层，会导致容量层的带宽没有得到充分利用。</p>\n<p>为了解决这个问题，作者以缓存模式举例（容量层必然命中），当性能层 “负载过高时”，可以将部分请求卸载到容量层，以优化指定的性能目标（吞吐，延迟等等）。</p>\n","categories":["Cache"],"tags":["Cache","论文阅读","Hierarchy","Cache Tier"]},{"title":"NyxCache（FAST 22）","url":"/2023/04/12/NyxCache%EF%BC%88FAST%2022%EF%BC%89/","content":"<p><strong>Abstract</strong></p>\n<ol>\n<li>本文提出了一个用于多租户持久性内存（multi-tenant persistent memory）缓存的访问调节框架，<strong>支持轻量级访问调节、每个缓存的资源使用估算和缓存间干扰分析</strong>。</li>\n<li>有了以上机制和准入控制、容量分配逻辑，本文构建了重要的<strong>共享策略</strong>，例如资源限制、Qos-awareness、公平性和按比例共享：Nyx 的资源限制可以准确地限制每个缓存的 PM 使用量，提供比带宽限制方法 5 倍的性能隔离 ；Nyx QoS 可以为延迟敏感型缓存提供 QoS 保证，同时为不受干扰的 best-effort 缓存提供更高的吞吐量（与以前基于 DRAM 的方法相比，最高可达 6 倍）。</li>\n<li>最后，本文也展示了 Nyx 对于真实负载也是有用的，隔离了写入高峰，并确保重要的缓存不会因为增加的 best-effort 流量而减慢。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>为了提高实用性和简化管理，多个缓存实例通常被合并成一个多租户服务器。然而，多租户服务器很难确保每个客户端缓存都能满足性能目标；目前，一系列内存多租户缓存提供了不同的共享策略，比如对使用的内存容量和带宽进行限制，保证一定的 quality-of-service（Qos），并按比例分配资源。</p>\n<p>由于具有大容量、字节低开销、与 DRAM 性能接近的性质，PM 可以作为多租户的缓存。但是，与 DRAM 不同的是，PM 在读写性能上差别很大（最大读带宽 6.6GB&#x2F;s，最大写带宽 2.3GB&#x2F;s），读和写之间有着严重、不公平的干扰（以 1GB&#x2F;s 的速度写入会导致共同运行的读取工作负载的吞吐量和 P99 延迟减慢，这种程度和 8GB&#x2F;s 的速度读取相同）。</p>\n<p>因此，现有多租户 DRAM 和缓存技术无法适配到 PM 上，于是本文就提出针对多租户 PM 键值缓存的访问调节框架，无需硬件支持就能优化 PM。</p>\n<h1 id=\"Motivation-and-Challenges\"><a href=\"#Motivation-and-Challenges\" class=\"headerlink\" title=\"Motivation and Challenges\"></a>Motivation and Challenges</h1><p>评估先前的多租户缓存以及它们对于 PM 的限制。</p>\n<h1 id=\"NyxCache-Design\"><a href=\"#NyxCache-Design\" class=\"headerlink\" title=\"NyxCache Design\"></a>NyxCache Design</h1><p>NyxCache 的设计。</p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><p>评估 Nyx 机制的开销和策略的有效性。</p>\n<h1 id=\"Discussion\"><a href=\"#Discussion\" class=\"headerlink\" title=\"Discussion\"></a>Discussion</h1><p>讨论潜在的扩展。</p>\n<h1 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h1><p>对比相关工作。</p>\n","categories":["Cache"],"tags":["Cache","论文阅读","PM"]},{"title":"Pangu（FAST 23）","url":"/2023/12/18/Pangu%EF%BC%88FAST%2023%EF%BC%89/","content":"<p>随着硬件技术（SSD，RDMA 等）和商业模式（从面向大容量到面向高性能）的演进，阿里盘古存储系统也在不断进化，存储服务从 ms 级延迟逐渐进化到高性能、高可靠的 100us 级。</p>\n<p><strong>如何做到的呢？</strong></p>\n<ol>\n<li>在第一阶段，为适应 SSD 和 RDMA（性能瓶颈逐渐从硬件转移到了软件层），创新了文件系统和设计用户态的存储操作系统，从而能够大幅降低 I&#x2F;O 延迟，提供高吞吐量和 IOPS；</li>\n<li>在第二阶段，为适应盘古从面向容量的商业模式往面向性能的转变，提高了基础设施中 SSD 和 100 Gbps 带宽的 RDMA 的数量。同时也引入了大量关键设计：降低流量放大、远程直接缓存访问（RDCA）、CPU 计算卸载，确保盘古充分收获硬件升级带来的性能提升。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>盘古作为一个<strong>统一存储平台</strong>，为阿里和阿里云（比如淘宝，天猫，蚂蚁金服等等）提供了可扩展，高性能和高可靠的存储服务；一些云服务，比如 <strong>EBS，OSS</strong> 等等也是建立在其之上。现如今，盘古已经达到了 <strong>EB 级存储容量</strong>，管理了<strong>万亿个文件</strong>。</p>\n<h2 id=\"盘古1-0：面向容量的存储服务提供者。\"><a href=\"#盘古1-0：面向容量的存储服务提供者。\" class=\"headerlink\" title=\"盘古1.0：面向容量的存储服务提供者。\"></a>盘古1.0：面向容量的存储服务提供者。</h2><p>2009-2015，盘古构建在商用 CPU 和 HDD 之上，提供 ms 级 I&#x2F;O 延迟和 Gbps 级数据中心网络。基于 Ext4 和内核态的 TCP，盘古设计了分布式内核态的文件系统，能够提供多种文件类型（比如临时文件，日志文件和随机访问文件）供不同存储服务所使用。受限于 HDD 和 Gbps 级网络，盘古已经达到性能上限，且云计算早期，客户也更关注大容量。</p>\n<h2 id=\"新硬件需要新设计\"><a href=\"#新硬件需要新设计\" class=\"headerlink\" title=\"新硬件需要新设计\"></a>新硬件需要新设计</h2><p>SSD 和 RDMA 能够提供高性能、低延迟的 I&#x2F;O，但（1）<strong>允许随机访问的文件类型在 SSD 上的性能很差，而顺序操作能够提供更高的吞吐量</strong>；（2）由于数据拷贝和频繁中断，内核软件栈无法紧跟 SSD 和 RDMA 的高 IOPS 和低延迟；（3）<strong>从以服务器为中心的数据中心架构到资源分解（内存分离？）的数据中心架构（学习和总结其中的动机）</strong>，这种范式转变为实现低 I&#x2F;O 延迟带来了额外的挑战。</p>\n<h2 id=\"盘古2-0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱-SSD-和-RDMA。\"><a href=\"#盘古2-0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱-SSD-和-RDMA。\" class=\"headerlink\" title=\"盘古2.0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱 SSD 和 RDMA。\"></a>盘古2.0第一阶段：通过文件系统重构和用户态存储操作系统，拥抱 SSD 和 RDMA。</h2><p>为简化整体系统的开发和管理，为盘古的文件系统（1）设计了仅添加的持久层和<strong>引入了独立的块布局减少文件写入操作的 I&#x2F;O 延迟</strong>；（2）设计了用户态存储操作系统（USSOS），使用 run-to-completion 线程模型实现用户态存储栈和用户态网络栈的高效协作，并且提出了<em><strong>用户态调度机制，有效实现了 CPU 和内存资源分配</strong></em>；（3）在动态环境下，部署了 SLA 保护机制。</p>\n<p>通过以上设计，在第一阶段就实现了<strong>ms 级 P999 I&#x2F;O 延迟</strong>。</p>\n<h2 id=\"盘古2-0第二阶段：通过增加-SSD-和-100Gbps-RDMA-的数量以及突破网络-x2F-内存-x2F-CPU瓶颈，适应性能导向的商业模式\"><a href=\"#盘古2-0第二阶段：通过增加-SSD-和-100Gbps-RDMA-的数量以及突破网络-x2F-内存-x2F-CPU瓶颈，适应性能导向的商业模式\" class=\"headerlink\" title=\"盘古2.0第二阶段：通过增加 SSD 和 100Gbps RDMA 的数量以及突破网络&#x2F;内存&#x2F;CPU瓶颈，适应性能导向的商业模式\"></a>盘古2.0第二阶段：通过增加 SSD 和 100Gbps RDMA 的数量以及突破网络&#x2F;内存&#x2F;CPU瓶颈，适应性能导向的商业模式</h2><p>增加 SSD 数量：盘古每台服务器配备 96TB 的 SSD；将 RDMA 带宽从 25Gbps 升级到 100 Gbps。</p>\n<p>为充分收获这些硬件带来的性能提升，需要打破网络、内存和 CPU 的性能瓶颈。（1）通过减少网络流量放大比例和动态适应流量优先级，优化网络带宽；（2）通过提出<em><strong>远程直接缓存访问（RDCA），应对内存瓶颈</strong></em>；（3）通过消除数据序列化、反序列负担和引入 CPU 等待指令同步超线程，解决 CPU 瓶颈。</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><p>盘古是一个大规模分布式存储系统，由 Pangu Core，Pangu Service 和 Pangu Monitoring System 组成。</p>\n<img src=\"/2023/12/18/Pangu%EF%BC%88FAST%2023%EF%BC%89/architecture.jpg\" class=\"\" title=\"architecture\">\n\n<p>Pangu Core 由 clients，masters 和 chunkservers 组成。<strong>clients</strong> 为盘古云存储服务（比如 EBS 和 OSS）提供 SDK，接收上层服务的文件请求并与 masters 和 chunkservers 通信；<strong>masters</strong> 管理盘古所有的元数据，并使用基于 Raft 的协议来维护一致性，为更好的水平扩展性，将元数据服务分成 namespace 服务（提供有关文件的信息，例如目录树和命名空间）和 stream 服务（提供从文件到块位置的映射）；<strong>chunkservers</strong> 以块的形式存储数据，并配备定制的用户态存储文件服务（USSFS），USSFS 为不同硬件提供高性能、仅添加存储引擎（例如 HM-SMR SMRSTORE，FAST 23 的另一篇阿里论文）。在早期，每个文件都存储在具有三副本的 chunkserver 中，后来由垃圾回收工作器执行垃圾回收并使用纠删码存储文件，减少了流量放大。</p>\n<p>在 Pangu Core 上层，Pangu Service 提供传统的云存储服务。</p>\n<p>Pangu Monitoring System 为 Pangu Service 和 Pangu Core 提供实时的监控和 AI 辅助的根因分析服务。</p>\n<h1 id=\"阶段一：拥抱-SSD-和-RDMA\"><a href=\"#阶段一：拥抱-SSD-和-RDMA\" class=\"headerlink\" title=\"阶段一：拥抱 SSD 和 RDMA\"></a>阶段一：拥抱 SSD 和 RDMA</h1><p>SSD 和 RDMA 的出现，使得 I&#x2F;O 瓶颈从硬件逐渐转移至软件层，为此，盘古创新了文件系统和设计用户态的存储操作系统。</p>\n<h2 id=\"Append-Only-File-System\"><a href=\"#Append-Only-File-System\" class=\"headerlink\" title=\"Append-Only File System\"></a>Append-Only File System</h2><p>首先引入一个统一的、仅添加的持久层，带有一个名为 FlatLogFile 的仅添加接口，简化架构并具有高吞吐量和低延迟；heavyweight client 可以满足不同存储服务的需求；基于 FlatLogFile 接口，盘古采用仅添加的 chunk 并使用独立的 chunk 布局来管理 chunkservers 上的 chunk；盘古在 masters 上实现了分布式元数据管理，做到高效的元数据操作。</p>\n<h3 id=\"Unified-Append-Only-Persistence-Layer\"><a href=\"#Unified-Append-Only-Persistence-Layer\" class=\"headerlink\" title=\"Unified, Append-Only Persistence Layer\"></a>Unified, Append-Only Persistence Layer</h3><p>早期的盘古为不同的存储服务提供了不同的接口，而现如今，为了简化开发和管理，确保所有存储服务都能在 SSD 上实现高性能、低延迟的 I&#x2F;O 服务，盘古的持久层为它们（例如 EBS 和 OSS）提供了统一的文件类型 FlatLogFile。</p>\n<img src=\"/2023/12/18/Pangu%EF%BC%88FAST%2023%EF%BC%89/FlatLogFile.jpg\" class=\"\" title=\"FlatLogFile\">\n\n<h3 id=\"Heavyweight-Client\"><a href=\"#Heavyweight-Client\" class=\"headerlink\" title=\"Heavyweight Client\"></a>Heavyweight Client</h3><p>client 负责与 chunkservers 交互进行数据操作，与 masters 交互进行元数据信息检索和更新。</p>\n<h3 id=\"Append-only-Chunk-Management\"><a href=\"#Append-only-Chunk-Management\" class=\"headerlink\" title=\"Append-only Chunk Management\"></a>Append-only Chunk Management</h3><p>典型的文件系统（如 Ext4）将文件存储在块中，通过两次 SSD 写操作，将文件及元数据分别写入存储介质，不仅增加了文件写入延迟，而且缩短了 SSD 的寿命。因此，盘古选择在 chunkservers 中以 chunk 形式存储文件，这些 chunk 具有基于 FlatLogFile 的仅添加语义，并设计一个独立的 chunk 布局，其中每个 chunk 都存储数据和自己的元数据（是否导致实际可用存储容量减少？）。这样，一次操作就可完成写入。</p>\n<p>如下是 chunk 布局，一个 chunk 包括多个扇区单元，每个扇区包括三个元素：data，padding 和 footer。footer 存储块元数据，例如块 ID，块长度和 CRC 校验和。</p>\n<img src=\"/2023/12/18/Pangu%EF%BC%88FAST%2023%EF%BC%89/chunk_layout.jpg\" class=\"\" title=\"chunk_layout\">\n\n<h3 id=\"Metadata-Operation-Optimization\"><a href=\"#Metadata-Operation-Optimization\" class=\"headerlink\" title=\"Metadata Operation Optimization\"></a>Metadata Operation Optimization</h3><p>元数据服务分成两种：namespace 和 stream。namespace 负责目录树和文件管理；stream 负责块信息管理，stream 是一组块的抽象，同一 stream 中的块属于同一文件。</p>\n<p><strong>不同 chunk 大小。</strong>大块可以减少元数据数量，同时避免由于客户端频繁请求块而导致的不必要 I&#x2F;O 延迟，有助于提高 SSD 的使用寿命，但是，可能会引入碎片风险。因此，引入长度可变的 chunk，大小分为从 1MB 到 2GB。</p>\n<p><strong>在客户端缓存 chunk 信息</strong>。每个客户端维护一个本地元数据缓存池，以减少元数据查询请求的数量。</p>\n<p><strong>chunk 信息请求批处理。</strong>每个 client 短时间内聚合多个 chunk 信息请求，批量发给 master，提高查询效率。master 批量处理请求，聚合结果并将其发送回 client。client 分解结果并将其分派到相应的应用程序。</p>\n<p><strong>推测性 chunk 信息预取。</strong>当 masters 收到读取请求时，返回相关的 chunk 元数据和其它 chunk 的元数据（过去的经验表征和请求 chunk 频繁一起访问？缓存到 client？）；当 masters 收到写入请求时，返回更多的 chunk 以应对写入异常，而无需再向 masters 请求新的 chunk。</p>\n<p><strong>Data piggybacking to reduce one RTT。</strong>client 从 masters 获取到 chunk 地址后，将 chunk 创建请求和要写入的数据合并成一个请求发送给 chunkservers，从而能够将写入延迟减少一个 RTT。</p>\n<h2 id=\"Chunkserver-USSOS\"><a href=\"#Chunkserver-USSOS\" class=\"headerlink\" title=\"Chunkserver USSOS\"></a>Chunkserver USSOS</h2><p>chunkservers 负责执行所有的数据操作，因此，必须仔细设计运行时操作系统，以确保数据操作能够以低延迟和高吞吐量完成。传统内核态进行数据操作将发生频繁的系统中断，消耗 CPU 资源，而且会在用户态和内核态之间产生不必要的重复。因此，采用绕过内核的设计，为 chunkserver 开发一个高性能的用户态存储操作系统，提供统一的用户态存储软件平台。</p>\n<p>盘古还实现了用户级内存管理，轻量级用户态调度策略和用于 SSD 的定制高性能仅添加用户态存储文件系统（USSFS）。</p>\n<h3 id=\"User-Level-Memory-Management\"><a href=\"#User-Level-Memory-Management\" class=\"headerlink\" title=\"User-Level Memory Management\"></a>User-Level Memory Management</h3><p>chunkserver USSOS 基于现有的用户态技术（如网络栈中的 RDMA、存储栈中的 DPDK 和 SPDK）构建。同时，更进一步</p>\n<ol>\n<li>利用 run-to-completion 线程模型，相比于传统的 pipeline 线程模型，减少了线程上下文切换、通信和同步的开销；</li>\n<li>线程请求大页内存作为网络和存储栈之间的共享内存，通过 RDMA，可以将从网络接收的数据存储在这个共享大页内存中，发送大页内存的元数据（例如地址和大小）后，可以通过 SPDK 帧将数据直接从大页内存写入存储介质。这样，在数据传输和存储过程中实现了网络和存储堆栈之间的零拷贝。</li>\n</ol>\n<h3 id=\"User-Space-Scheduling-Mechanism\"><a href=\"#User-Space-Scheduling-Mechanism\" class=\"headerlink\" title=\"User-Space Scheduling Mechanism\"></a>User-Space Scheduling Mechanism</h3><p>三个优化 CPU 调度的关键设计。</p>\n<p><strong>防止阻塞后续任务。</strong>每个 chunkserver 有固定数量的工作线程，根据请求中文件的哈希值将新请求分派到工作线程，分配给同一工作线程的请求按照 FIFO 顺序执行。而如果一项任务花费太多时间，将导致资源被占用并且后续任务被阻塞。为解决此问题，引入心跳机制监控任务的执行并设置警报，当时间片被用完时，它将被放入后台线程并从关键路径中删除。</p>\n<p><strong>优先级调度保证高 Qos。</strong>盘古为不同的请求分配不同的 Qos 目标，比如用户请求高 Qos，GC 请求低 Qos，通过使用优先级队列保证较高 Qos 目标的请求拥有较高优先级。</p>\n<p><strong>轮询和事件驱动的切换。</strong>NIC（网卡） 提供一个供应用程序监控的 fd，并通过 fd 事件通知应用程序有数据到达。默认情况下，应用程序处于事件驱动模式，而当它们收到来自 NIC 的通知时，它们将进入轮询状态。如果一段事件内没有收到任何 I&#x2F;O 请求，它们将重新切换回事件驱动模式并通知 NIC。</p>\n<h3 id=\"Append-Only-USSFS\"><a href=\"#Append-Only-USSFS\" class=\"headerlink\" title=\"Append-Only USSFS\"></a>Append-Only USSFS</h3><p>USSFS 提供了一系列基于 chunk 的操作接口（打开、关闭、格式化、读取和追加等），支持顺序写入和随机读取。</p>\n<p>通过多种机制最大化 SSD 性能</p>\n<ol>\n<li>充分利用独立的 chunk 布局减少数据操作的数量，而无需使用页面缓存；</li>\n<li>无需像 Ext4 那样在 inode 和文件目录之间建立层次关系，所有对文件的操作会记录到日志文件中，可以通过挂载文件系统时重放日志来重建相应的元数据；</li>\n<li>使用轮询模式而不是像 Ext4 那样的中断机制来最大化 SSD 性能。</li>\n</ol>\n<h2 id=\"High-Performance-SLA-Guarantee\"><a href=\"#High-Performance-SLA-Guarantee\" class=\"headerlink\" title=\"High Performance SLA Guarantee\"></a>High Performance SLA Guarantee</h2><h1 id=\"阶段2：适应面向性能的商业模式\"><a href=\"#阶段2：适应面向性能的商业模式\" class=\"headerlink\" title=\"阶段2：适应面向性能的商业模式\"></a>阶段2：适应面向性能的商业模式</h1><p>泰山服务器（作为 chunkservers）配备了大量 SSD（96TB）和高性能 RDMA（100Gbps），此时，其它资源（例如网络、内存和 CPU）成为了性能瓶颈。</p>\n<h2 id=\"网络瓶颈\"><a href=\"#网络瓶颈\" class=\"headerlink\" title=\"网络瓶颈\"></a>网络瓶颈</h2><p>TODO</p>\n<h2 id=\"内存瓶颈\"><a href=\"#内存瓶颈\" class=\"headerlink\" title=\"内存瓶颈\"></a>内存瓶颈</h2><p>内存瓶颈来自接收主机中网络进程（即执行 DMA 操作的 NIC）和应用程序进程（如数据拷贝、数据复制和垃圾回收）之间的内存带宽竞争。NIC 无法获得足够的内存带宽，导致 NIC 缓冲区被传输中的数据包填满，最终导致溢出，触发网络中的拥塞控制造成整体性能下降。</p>\n<p>对此，分三步解决：（1）在服务器中添加更多小容量 DRAM，以充分利用内存通道；（2）将后台流量从 TCP 切换到 RDMA，以减少服务器的内存带宽小号；（3）设计远程直接缓存访问（RDCA）将内存移出接收方主机的数据路径，并让发送者直接访问接收器的缓存。</p>\n<h3 id=\"加一些小容量内存\"><a href=\"#加一些小容量内存\" class=\"headerlink\" title=\"加一些小容量内存\"></a>加一些小容量内存</h3><p>由于瓶颈在于内存带宽而不是内存容量，因此在服务器中添加更多小容量 DRAM，可以充分利用内存通道并增加每台服务器的可用内存带宽。</p>\n<p>启用非均匀内存访问（NUMA），以避免跨 sockers 内存访问。</p>\n<h3 id=\"后台流量从-TCP-切换到-RDMA\"><a href=\"#后台流量从-TCP-切换到-RDMA\" class=\"headerlink\" title=\"后台流量从 TCP 切换到 RDMA\"></a>后台流量从 TCP 切换到 RDMA</h3><p>原本的 25Gbps 的 RDMA 只能支持前端流量走 RDMA，而后台流量是走的 TCP，而 TCP 比 RDMA 至少多需要 4 个内存副本。而使用 100Gbps 后，后台流量也可以走 RDMA 了。</p>\n<h3 id=\"远程直接缓存访问\"><a href=\"#远程直接缓存访问\" class=\"headerlink\" title=\"远程直接缓存访问\"></a>远程直接缓存访问</h3><p>让发送者绕过接收者的内存直接访问其缓存。</p>\n<p>从 LLC 中拿一小片区域支持 NIC 操作。</p>\n<h2 id=\"CPU-瓶颈\"><a href=\"#CPU-瓶颈\" class=\"headerlink\" title=\"CPU 瓶颈\"></a>CPU 瓶颈</h2><p>TODO</p>\n<h1 id=\"运营经验\"><a href=\"#运营经验\" class=\"headerlink\" title=\"运营经验\"></a>运营经验</h1><p>TODO</p>\n<h1 id=\"教训\"><a href=\"#教训\" class=\"headerlink\" title=\"教训\"></a>教训</h1>","categories":["Storage"],"tags":["论文阅读","Storage"]},{"title":"Pond（ASPLOS 23）","url":"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/","content":"<p>内存池化有望提高 DRAM 利用率，这对公有云有很大的吸引力；但内存池化可能对性能造成伤害，因为远端内存的访问延时大于本地内存的访问延时。</p>\n<p>Pond 是一个建立在 CXL 标准之上的内存池系统，显著降低 DRAM 成本且满足云性能目标。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p><strong>动机。</strong>许多公有云客户以虚拟机（VM）的形式部署 workloads，这给公有云提供商带来了一个重大挑战：如何以具有性价比的硬件成本为不透明的虚拟机（无法查看客户在虚拟机中运行的内容）实现高性能。</p>\n<p>内存是影响性能和成本的关键因素。内存成本非常高，占微软 Azure 和 Meta 服务器成本的 40% 和 50%；然而，在 Azure 生产环境下，<strong>内存搁浅</strong>（一台服务器上 CPU 核心已经全部租给虚拟机使用，但存在部分未分配的内存容量。从云服务提供商的角度看就是 CPU 售尽了，内存没售尽）非常严重，且是内存浪费的主要原因；此外，已分配给虚拟机的内存未必被充分利用，比如虚拟机共有 8GB 内存，但是最多只使用了 6GB，那么如果剩余 2GB 全部来自池内存，也不会影响性能。</p>\n<p><strong>现有最优技术的局限性。</strong>现有的进程级内存压缩技术需要页中断，这会增加微妙延迟，且不再静态分配内存；内存分解可以池化内存，将搁浅的内存返回到内存池中供其它服务器使用，但是，现有的分离内存也具有微秒级访问延迟，并且需要页中断或对 VM guest 进行修改。</p>\n<p><strong>Insights</strong></p>\n<ol>\n<li><p>分析了 Azure 100 个生产集群的跟踪，相比同一 NUMA 上的内存访问，CXL 技术使得 8-16 插槽大小（能够使用该内存池的 CPU 插槽数量）的内存池增加 70-90 ns，使得机架级大小的内存池增加 180 ns 延迟。而 8-16 个插槽的池大小可以节省足够的 DRAM；</p>\n<ul>\n<li><p>在 CPU 资源利用率高的集群中，未分配搁浅内存的比例可高达 25%；</p>\n  <img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/memory_stranding.jpg\" class=\"\" title=\"memory_stranding\"></li>\n</ul>\n</li>\n<li><p>对于 Azure 上的典型 158 工作负载，分析它们在 CXL 内存相比于在本地内存中运行时产生的性能损失。发现（1）40% 左右的工作负载性能损失在 5% 以内；（2）21% 的工作负载遭遇了超过 25% 的性能损失。说明<strong>部分工作负载对内存延迟不敏感，可以完全分配 CXL 内存，而对于延迟敏感的内存，可以配置本地和 CXL 混合内存</strong>。</p>\n <img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/local_and_cxl.jpg\" class=\"\" title=\"local_and_cxl\">\n\n <img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/cxl_slowdown.jpg\" class=\"\" title=\"cxl_slowdown\">\n</li>\n<li><p>对于已分配给虚拟机的内存，50% 的虚拟机占用的租用内存不足 50%，此时，即使<strong>未使用的内存全部来自内存池也不会影响延迟敏感的虚拟机</strong>。如果将池内存作为 zero-core 虚拟 NUMA（zNUMA）节点暴漏给虚拟机的操作系统，这个结论确实成立。</p>\n</li>\n<li><p>如果可以正确预测（1）一个虚拟机是否延迟敏感，（2）虚拟机未使用的内存大小，则分配的 CXL 内存具有和同一 NUMA 上的内存相同的性能；对于失败的预测，引入一种新颖的监视系统，如果检测到内存性能不佳便触发缓存措施，将虚拟机迁移为仅使用同一 NUMA 节点内存。</p>\n</li>\n</ol>\n<p>基于以上 insights，Pond 通过 ML 模型预测虚拟机是否延迟敏感以及未使用的内存大小，静态分配同一 NUMA 节点内存和动态分配池内存，并通过监视系统检测性能，性能不佳时进行适当迁移。</p>\n<h1 id=\"Design\"><a href=\"#Design\" class=\"headerlink\" title=\"Design\"></a>Design</h1><p><strong>设计目标。</strong></p>\n<ol>\n<li>高性能。在采用访问速度相对较慢的外部 CXL 内存后，虚拟机的性能应该与完全使用本地 NUMA 上的 DRAM 性能接近；</li>\n<li>硬件兼容性。与虚拟化加速器兼容；</li>\n<li>软件兼容性。与不透明的虚拟机和客户应用程序相兼容；</li>\n<li>低开销。额外开销尽可能小。</li>\n</ol>\n<h2 id=\"池内存大小\"><a href=\"#池内存大小\" class=\"headerlink\" title=\"池内存大小\"></a>池内存大小</h2><p>作者发现最多可以有 16 个 CPU sockets 可以直接与 EMC（external memory controller） 相连，而如果大于 16，则需要加入 CXL 交换机，这会显著增加内存池的访问延迟和硬件成本。</p>\n<img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/EMC.jpg\" class=\"\" title=\"EMC\">\n\n<img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/pool_size_latency.jpg\" class=\"\" title=\"pool_size_latency\">\n\n<p>此外，作者发现当池大小大于 16 时，收益明显递减。因此，8&#x2F;16 的池大小是性能和成本的平衡点。</p>\n<img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/pool_size_impact.jpg\" class=\"\" title=\"pool_size_impact\">\n\n<h2 id=\"混合内存分配\"><a href=\"#混合内存分配\" class=\"headerlink\" title=\"混合内存分配\"></a>混合内存分配</h2><p>当需要分配一个虚拟机时，首先使用 ML 模型预测需要为虚拟机分配多少本地内存，再通过调度程序将目标主机及其池内存需求信息告知池管理器，池管理器触发内存在线工作流程，调度程序通知虚拟机管理程序在与内存量匹配的 zNUMA 节点上启动虚拟机。</p>\n<img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/pond_control_plane.jpg\" class=\"\" title=\"pond_control_plane\">\n\n<h2 id=\"预测模型\"><a href=\"#预测模型\" class=\"headerlink\" title=\"预测模型\"></a>预测模型</h2><p>首先检查当前请求的虚拟机是否可以和工作负载历史相关联，具体实现方式是通过检查过去的虚拟机是否具有与请求虚拟机相同的元数据（例如客户 ID，虚拟机类型和位置）。如果有过去的工作负载历史记录，便预测该虚拟机是否对内存延迟敏感（比如全使用池内存相比于全使用本地内存，性能下降在 5% 以内可以被视为不敏感）；如果虚拟机没有工作负载历史记录或预计对延迟敏感，再预测其未使用的内存容量，对应部分使用池内存分配。</p>\n<p>与此同时，在虚拟机运行过程中监视其性能，如果使用到了池内存，则需要进行虚拟机迁移，重新进行内存分配。</p>\n<img src=\"/2023/11/28/Pond%EF%BC%88ASPLOS%2023%EF%BC%89/prediction_models.jpg\" class=\"\" title=\"prediction_models\">\n\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>使用 NUMA 模拟。在具有两个 sockets 的服务器上只使用一个 socket，而在另一个 socket 上禁用所有 CPU 核心，模拟成 CXL 内存池。</p>\n","categories":["Memory"],"tags":["论文阅读","CXL","分层"]},{"title":"Redis","url":"/2023/04/20/Redis/","content":"<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"简单动态字符串\"><a href=\"#简单动态字符串\" class=\"headerlink\" title=\"简单动态字符串\"></a>简单动态字符串</h2><p>位于 <code>sds.h</code> 和 <code>sds.c</code> 中。</p>\n<p>C语言用 ‘\\0’ 作为字符串结束符，而如果字符串内容本身就包含 ‘\\0’ 则会被截断，无法做到二进制安全。因此，redis 将<strong>柔性字符数组</strong>包装到 sds 数据结构中，同时使用 len 和 alloc 表示已使用字符数组的长度和预分配长度。</p>\n<p><strong>注</strong>：柔性数组只能放在结构体的末尾，而之所以使用柔性数组而不是字符指针，是因为柔性数组的地址和结构体是连续的，可以方便地通过柔性数组的首地址偏移得到结构体的首地址（<code>buf[-1]</code> 一定是 <code>flags</code> 字段，进一步可以知道所在结构体的大小）。</p>\n<p>为精细地管理字符串，节约内存空间，设置了各个长度范围的字符串结构体。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr5 &#123;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3 个最低有效位表示类型, 同时 5 个最高有效位表示字符串长度 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr8 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> len;         <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> alloc;       <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                            &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr16 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint16_t</span> len;        <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint16_t</span> alloc;      <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                           &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr32 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> len;        <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> alloc;      <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                           &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">__attribute__</span>((__packed__)) sdshdr64 &#123;</span><br><span class=\"line\">    <span class=\"type\">uint64_t</span> len;        <span class=\"comment\">/* 已使用的长度 */</span></span><br><span class=\"line\">    <span class=\"type\">uint64_t</span> alloc;      <span class=\"comment\">/* 分配给 buf 的长度 - 1 （不包含结构体中的元数据以及 buf 中的</span></span><br><span class=\"line\"><span class=\"comment\">                           &#x27;\\0&#x27; 结束符） */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> flags; <span class=\"comment\">/* 3位最低有效位表示类型, 其余5个比特位未被使用 */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> buf[];</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"双向链表\"><a href=\"#双向链表\" class=\"headerlink\" title=\"双向链表\"></a>双向链表</h2><p>位于 <code>adlist.h</code> 和 <code>adlist.c</code> 中。</p>\n<p>redis 双向链表的节点可以存储任意类型的 value，因此，每个 list 可以设置自己的 dup、free 和 match 函数，达到多态目的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* 双端链表节点 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">listNode</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向前驱节点的指针 */</span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">listNode</span> *prev;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向后继节点的指针 */</span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">listNode</span> *next;</span><br><span class=\"line\">    <span class=\"comment\">/* void * 指针，指向具体的元素，节点可以是任意类型 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *value;</span><br><span class=\"line\">&#125; listNode;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 双端链表</span></span><br><span class=\"line\"><span class=\"comment\"> * 有记录头尾两节点，支持从链表头部或者尾部进行遍历，是早期列表键 PUSH/POP 实现高效的关键</span></span><br><span class=\"line\"><span class=\"comment\"> * 每个链表节点有记录前驱节点和后继节点的指针，可以使得列表键支持往后或者往前进行遍历</span></span><br><span class=\"line\"><span class=\"comment\"> * 有额外用 len 存储链表长度，O(1) 的时间复杂度获取节点个数，是 LLEN 命令高效的关键 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">list</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向链表头节点的指针，支持从表头开始遍历 */</span></span><br><span class=\"line\">    listNode *head;</span><br><span class=\"line\">    <span class=\"comment\">/* 指向链表尾节点的指针，支持从表尾开始遍历 */</span></span><br><span class=\"line\">    listNode *tail;</span><br><span class=\"line\">    <span class=\"comment\">/* 各种类型的链表可以定义自己的复制函数 / 释放函数 / 比较函数 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *(*dup)(<span class=\"type\">void</span> *ptr);</span><br><span class=\"line\">    <span class=\"built_in\">void</span> (*free)(<span class=\"type\">void</span> *ptr);</span><br><span class=\"line\">    <span class=\"built_in\">int</span> (*match)(<span class=\"type\">void</span> *ptr, <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 链表长度，即链表节点数量，O(1) 时间复杂度获取 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> len;</span><br><span class=\"line\">&#125; list;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"字典\"><a href=\"#字典\" class=\"headerlink\" title=\"字典\"></a>字典</h2><p>位于 <code>dict.h</code> 和 <code>dict.c</code> 中。</p>\n<p>dictEntry 存储键值对；dict 为字典结构，其中使用了两个哈希表，第二个哈希表一般情况下不用，只有在扩容缩容操作时，避免长时间 rehash 导致阻塞，将新申请空间存在第二个哈希表中，rehash 过程中，添加操作往第二个哈希表中进行，查找、删除和修改在两个哈希表中依次进行，除此之外，第一个哈希表中的键值对需要在服务空闲时重新计算哈希桶索引，全部迁移插入到第二个哈希表中；dictType 为字典类型，针对不同类型的键和值配置合适的复制、析构和比较函数等，达到多态目的。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">dictEntry</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* void * 类型的 key，可以指向任意类型的键 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *key;</span><br><span class=\"line\">    <span class=\"comment\">/* 联合体 v 中包含了指向实际值的指针 *val、无符号的 64 位整数、有符号的 64 位整数，以及 double 双精度浮点数。</span></span><br><span class=\"line\"><span class=\"comment\">     * 这是一种节省内存的方式，因为当值为整数或者双精度浮点数时，由于它们本身就是 64 位的，void *val 指针也是占用 64 位（64 操作系统下），</span></span><br><span class=\"line\"><span class=\"comment\">     * 所以它们可以直接存在键值对的结构体中，避免再使用一个指针，从而节省内存开销（8 个字节）</span></span><br><span class=\"line\"><span class=\"comment\">     * 当然也可以是 void *，存储任何类型的数据，最早 redis1.0 版本就只是 void* */</span></span><br><span class=\"line\">    <span class=\"keyword\">union</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">void</span> *val;</span><br><span class=\"line\">        <span class=\"type\">uint64_t</span> u64;</span><br><span class=\"line\">        <span class=\"type\">int64_t</span> s64;</span><br><span class=\"line\">        <span class=\"type\">double</span> d;</span><br><span class=\"line\">    &#125; v;</span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">dictEntry</span> *next;     <span class=\"comment\">/* 同一个 hash 桶中的下一个条目.</span></span><br><span class=\"line\"><span class=\"comment\">                                 * 通过形成一个链表解决桶内的哈希冲突. */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *metadata[];           <span class=\"comment\">/* 一块任意长度的数据 (按 void* 的大小对齐),</span></span><br><span class=\"line\"><span class=\"comment\">                                 * 具体长度由 &#x27;dictType&#x27; 中的</span></span><br><span class=\"line\"><span class=\"comment\">                                 * dictEntryMetadataBytes() 返回. */</span></span><br><span class=\"line\">&#125; dictEntry;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* 字典类型，因为我们会将字典用在各个地方，例如键空间、过期字典等等等，只要是想用字典（哈希表）的场景都可以用</span></span><br><span class=\"line\"><span class=\"comment\"> * 这样的话每种类型的字典，它对应的 key / value 肯定类型是不一致的，这就需要有一些自定义的方法，例如键值对复制、析构等 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">dictType</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 字典里哈希表的哈希算法，目前使用的是基于 DJB 实现的字符串哈希算法</span></span><br><span class=\"line\"><span class=\"comment\">     * 比较出名的有 siphash，redis 4.0 中引进了它。3.0 之前使用的是 DJBX33A，3.0 - 4.0 使用的是 MurmurHash2 */</span></span><br><span class=\"line\">    <span class=\"built_in\">uint64_t</span> (*hashFunction)(<span class=\"type\">const</span> <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 键拷贝 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *(*keyDup)(dict *d, <span class=\"type\">const</span> <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 值拷贝 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *(*valDup)(dict *d, <span class=\"type\">const</span> <span class=\"type\">void</span> *obj);</span><br><span class=\"line\">    <span class=\"comment\">/* 键比较 */</span></span><br><span class=\"line\">    <span class=\"built_in\">int</span> (*keyCompare)(dict *d, <span class=\"type\">const</span> <span class=\"type\">void</span> *key1, <span class=\"type\">const</span> <span class=\"type\">void</span> *key2);</span><br><span class=\"line\">    <span class=\"comment\">/* 键析构 */</span></span><br><span class=\"line\">    <span class=\"built_in\">void</span> (*keyDestructor)(dict *d, <span class=\"type\">void</span> *key);</span><br><span class=\"line\">    <span class=\"comment\">/* 值析构 */</span></span><br><span class=\"line\">    <span class=\"built_in\">void</span> (*valDestructor)(dict *d, <span class=\"type\">void</span> *obj);</span><br><span class=\"line\">    <span class=\"comment\">/* 字典里的哈希表是否允许扩容 */</span></span><br><span class=\"line\">    <span class=\"built_in\">int</span> (*expandAllowed)(<span class=\"type\">size_t</span> moreMem, <span class=\"type\">double</span> usedRatio);</span><br><span class=\"line\">    <span class=\"comment\">/* 允许调用者向条目 (dictEntry) 中添加额外的元信息.</span></span><br><span class=\"line\"><span class=\"comment\">     * 这段额外信息的内存会在条目分配时被零初始化. */</span></span><br><span class=\"line\">    <span class=\"built_in\">size_t</span> (*dictEntryMetadataBytes)(dict *d);</span><br><span class=\"line\">&#125; dictType;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">dict</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 字典类型，8 bytes */</span></span><br><span class=\"line\">    dictType *type;</span><br><span class=\"line\">    <span class=\"comment\">/* 字典中使用了两个哈希表,</span></span><br><span class=\"line\"><span class=\"comment\">     * (看看那些以 &#x27;ht_&#x27; 为前缀的成员, 它们都是一个长度为 2 的数组)</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * 我们可以将它们视为</span></span><br><span class=\"line\"><span class=\"comment\">     * struct&#123;</span></span><br><span class=\"line\"><span class=\"comment\">     *   ht_table[2];</span></span><br><span class=\"line\"><span class=\"comment\">     *   ht_used[2];</span></span><br><span class=\"line\"><span class=\"comment\">     *   ht_size_exp[2];</span></span><br><span class=\"line\"><span class=\"comment\">     * &#125; hash_table[2];</span></span><br><span class=\"line\"><span class=\"comment\">     * 为了优化字典的内存结构,</span></span><br><span class=\"line\"><span class=\"comment\">     * 减少对齐产生的空洞,</span></span><br><span class=\"line\"><span class=\"comment\">     * 我们将这些数据分散于整个结构体中.</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * 平时只使用下标为 0 的哈希表.</span></span><br><span class=\"line\"><span class=\"comment\">     * 当需要进行 rehash 时 (&#x27;rehashidx&#x27; != -1),</span></span><br><span class=\"line\"><span class=\"comment\">     * 下标为 1 的一组数据会作为一组新的哈希表,</span></span><br><span class=\"line\"><span class=\"comment\">     * 渐进地进行 rehash 避免一次性 rehash 造成长时间的阻塞.</span></span><br><span class=\"line\"><span class=\"comment\">     * 当 rehash 完成时, 将新的哈希表置入下标为 0 的组别中,</span></span><br><span class=\"line\"><span class=\"comment\">     * 同时将 &#x27;rehashidx&#x27; 置为 -1.</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    dictEntry **ht_table[<span class=\"number\">2</span>];</span><br><span class=\"line\">    <span class=\"comment\">/* 哈希表存储的键数量，它与哈希表的大小 size 的比值就是 load factor 负载因子，</span></span><br><span class=\"line\"><span class=\"comment\">     * 值越大说明哈希碰撞的可能性也越大，字典的平均查找效率也越低</span></span><br><span class=\"line\"><span class=\"comment\">     * 理论上负载因子 &lt;=1 的时候，字典能保持平均 O(1) 的时间复杂度查询</span></span><br><span class=\"line\"><span class=\"comment\">     * 当负载因子等于哈希表大小的时候，说明哈希表退化成链表了，此时查询的时间复杂度退化为 O(N)</span></span><br><span class=\"line\"><span class=\"comment\">     * redis 会监控字典的负载因子，在负载因子变大的时候，会对哈希表进行扩容，后面会提到的渐进式 rehash */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> ht_used[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">long</span> rehashidx; <span class=\"comment\">/* rehashing not in progress if rehashidx == -1 */</span></span><br><span class=\"line\">                    <span class=\"comment\">/* rehash 的进度.</span></span><br><span class=\"line\"><span class=\"comment\">                     * 如果此变量值为 -1, 则当前未进行 rehash. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 将小尺寸的变量置于结构体的尾部, 减少对齐产生的额外空间开销. */</span></span><br><span class=\"line\">    <span class=\"type\">int16_t</span> pauserehash; <span class=\"comment\">/* If &gt;0 rehashing is paused (&lt;0 indicates coding error) */</span></span><br><span class=\"line\">                         <span class=\"comment\">/* 如果此变量值 &gt;0 表示 rehash 暂停</span></span><br><span class=\"line\"><span class=\"comment\">                          * (&lt;0 表示编写的代码出错了). */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储哈希表大小的指数表示，通过这个可以直接计算出哈希表的大小，例如 exp = 10, size = 2 ** 10</span></span><br><span class=\"line\"><span class=\"comment\">     * 能避免说直接存储 size 的实际值，以前 8 字节存储的数值现在变成 1 字节进行存储 */</span></span><br><span class=\"line\">    <span class=\"type\">signed</span> <span class=\"type\">char</span> ht_size_exp[<span class=\"number\">2</span>]; <span class=\"comment\">/* exponent of size. (size = 1&lt;&lt;exp) */</span></span><br><span class=\"line\">                                <span class=\"comment\">/* 哈希表大小的指数表示.</span></span><br><span class=\"line\"><span class=\"comment\">                                 * (以 2 为底, 大小 = 1 &lt;&lt; 指数) */</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"跳跃表\"><a href=\"#跳跃表\" class=\"headerlink\" title=\"跳跃表\"></a>跳跃表</h2><p><code>server.h</code> 中的 <code>zskiplist</code> 结构和 <code>zskiplistNode</code> 结构，<code>t_zset.c</code> 中以 <code>zsl</code> 开头的函数。</p>\n<img src=\"/2023/04/20/Redis/%E8%B7%B3%E8%B7%83%E8%A1%A8.jpg\" class=\"\" title=\"跳跃表\">\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> &#123;</span><br><span class=\"line\">    sds ele;\t\t\t\t\t       <span class=\"comment\">// 存储字符串类型数据</span></span><br><span class=\"line\">    <span class=\"type\">double</span> score;                      <span class=\"comment\">// 优先分数 </span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> *backward;    <span class=\"comment\">// 指向当前节点最底层的前一个节点</span></span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistLevel</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> *forward; <span class=\"comment\">// 指向本层下一节点，尾节点指向 NULL</span></span><br><span class=\"line\">        <span class=\"type\">unsigned</span> <span class=\"type\">long</span> span;            <span class=\"comment\">// forward 指向的节点和本节点之间的元素个数，span 值越大，跳过的节点个数越多</span></span><br><span class=\"line\">    &#125; level[]; \t\t\t\t\t\t   <span class=\"comment\">// 柔性数组，每个节点的数组长度不一样</span></span><br><span class=\"line\">&#125; zskiplistNode;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplist</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">struct</span> <span class=\"title class_\">zskiplistNode</span> *header, *tail;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> length;  <span class=\"comment\">// 跳跃表节点个数（等于元素总数，不包括头节点）</span></span><br><span class=\"line\">    <span class=\"type\">int</span> level;             <span class=\"comment\">// 跳跃表高度（除头节点外，层数最多的节点的层高）</span></span><br><span class=\"line\">&#125; zskiplist;</span><br></pre></td></tr></table></figure>\n\n<p>跳跃表的头节点高度为 <code>MAXLEVEL</code>，其它节点创建时随机设置高度，高度越高概率越小。</p>\n<p>重点是 <code>update</code> 和 <code>rank</code> 数组的理解。</p>\n<p><code>update[]</code>：记录每层比插入元素 score 或字典序小的最近节点；</p>\n<p><code>rank[]</code>：记录插入位置在每层中从头节点跨越了多少个节点。</p>\n<h2 id=\"整数集合\"><a href=\"#整数集合\" class=\"headerlink\" title=\"整数集合\"></a>整数集合</h2><p>位于 <code>intset.h</code> 和 <code>intset.c</code> 中。</p>\n<p>整数集合中的元素<strong>从小到大</strong>排列在数组中。为节省空间，有三种底层数组类型，随着插入整数的编码提升，提升整数集合底层数组的类型。（插入和删除操作效率比较低，涉及 <code>memmove</code>）</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* 整数集合 </span></span><br><span class=\"line\"><span class=\"comment\"> * 记录不包含重复元素的各个整数(由小到大的顺序) </span></span><br><span class=\"line\"><span class=\"comment\"> * 底层数组默认是 int16_t 类型, 可能随着新增元素的大小升级至 int32_t 或 int64_t 类型*/</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">intset</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 编码, 记录整数集合底层数组(contents)的类型*/</span></span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> encoding;</span><br><span class=\"line\">    <span class=\"comment\">/* 记录整数集合包含的元素个数 */</span></span><br><span class=\"line\">    <span class=\"type\">uint32_t</span> length;</span><br><span class=\"line\">    <span class=\"comment\">/* 整数集合的底层实现, 虽声明为 int8_t 类型,但真正的类型取决于 encoding */</span></span><br><span class=\"line\">    <span class=\"type\">int8_t</span> contents[];</span><br><span class=\"line\">&#125; intset;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"压缩列表\"><a href=\"#压缩列表\" class=\"headerlink\" title=\"压缩列表\"></a>压缩列表</h2><p>位于 <code>ziplist.h</code> 和 <code>ziplist.c</code> 中。</p>\n<p>压缩列表适合存储小整数以及短字符串，并且元素数量较少。</p>\n<img src=\"/2023/04/20/Redis/%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8.jpg\" class=\"\" title=\"压缩列表\">\n\n<p>压缩列表中的 entry 在内存中有三个字段，previous_entry_length 字段表示<strong>前一个元素</strong>的字节长度，占1个或者5个字节，当前一个元素的长度小于254字节时，用1个字节表示；当前一个元素的长度大于或等于254字节时，用5个字节来表示。而此时 previous_entry_length 字段的第1个字节是固定的0xFE，后面4个字节才真正表示前一个元素的长度。假设已知当前元素的首地址为p，那么 p-previous_entry_length 就是前一个元素的首地址，从而实现压缩列表从尾到头的遍历。</p>\n<img src=\"/2023/04/20/Redis/%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8entry.jpg\" class=\"\" title=\"压缩列表entry\">\n\n<p>encoding 字段同样长度可变，为1字节、2字节或者5字节，用来表示<strong>当前元素</strong>的数据和字节长度；content 字段则是存储的实际内容。</p>\n<img src=\"/2023/04/20/Redis/%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8%E5%85%83%E7%B4%A0%E7%9A%84%E7%BC%96%E7%A0%81.jpg\" class=\"\" title=\"压缩列表元素的编码\">\n\n<p>压缩列表中的 entry 被解码后保存在 zlentry 结构体中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* 这是一个很关键的结构体，将 ziplist 节点信息填充成一个 zlentry 结构体，方便后面进行函数操作</span></span><br><span class=\"line\"><span class=\"comment\"> * 需要注意这并不是一个 ziplist 节点在内存中实际的编码布局，只是为了方便我们使用</span></span><br><span class=\"line\"><span class=\"comment\"> * */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">zlentry</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 存储下面 prevrawlen 所需要的字节数 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> prevrawlensize; <span class=\"comment\">/* Bytes used to encode the previous entry len*/</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储前一个节点的字节长度 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> prevrawlen;     <span class=\"comment\">/* Previous entry len. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储下面 len 所需要的字节数 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> lensize;        <span class=\"comment\">/* Bytes used to encode this entry type/len.</span></span><br><span class=\"line\"><span class=\"comment\">                                    For example strings have a 1, 2 or 5 bytes</span></span><br><span class=\"line\"><span class=\"comment\">                                    header. Integers always use a single byte.*/</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储当前节点的字节长度 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> len;            <span class=\"comment\">/* Bytes used to represent the actual entry.</span></span><br><span class=\"line\"><span class=\"comment\">                                    For strings this is just the string length</span></span><br><span class=\"line\"><span class=\"comment\">                                    while for integers it is 1, 2, 3, 4, 8 or</span></span><br><span class=\"line\"><span class=\"comment\">                                    0 (for 4 bit immediate) depending on the</span></span><br><span class=\"line\"><span class=\"comment\">                                    number range. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* prevrawlensize + lensize 当前节点的头部字节，</span></span><br><span class=\"line\"><span class=\"comment\">     * 其实是 prevlen + encoding 两项占用的字节数 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">int</span> headersize;     <span class=\"comment\">/* prevrawlensize + lensize. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 存储当前节点的数据编码格式 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> encoding;      <span class=\"comment\">/* Set to ZIP_STR_* or ZIP_INT_* depending on</span></span><br><span class=\"line\"><span class=\"comment\">                                    the entry encoding. However for 4 bits</span></span><br><span class=\"line\"><span class=\"comment\">                                    immediate integers this can assume a range</span></span><br><span class=\"line\"><span class=\"comment\">                                    of values and must be range-checked. */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 指向当前节点开头第一个字节的指针 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">char</span> *p;            <span class=\"comment\">/* Pointer to the very start of the entry, that</span></span><br><span class=\"line\"><span class=\"comment\">                                    is, this points to prev-entry-len field. */</span></span><br><span class=\"line\">&#125; zlentry;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"快速列表\"><a href=\"#快速列表\" class=\"headerlink\" title=\"快速列表\"></a>快速列表</h2><p>位于 <code>quicklist.h</code> 和 <code>quicklist.c</code> 中。</p>\n<p>quicklist 是一个双向链表，链表中的每个节点是一个压缩列表。</p>\n<h1 id=\"数据对象\"><a href=\"#数据对象\" class=\"headerlink\" title=\"数据对象\"></a>数据对象</h1><p>Redis 基于上述数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象，每种对象都用到了至少一种我们前面所介绍的数据结构。</p>\n<p>位于 <code>server.h</code> 中的 <code>redisObject</code> 数据结构（robj）的 type 字段表示对象类型；encoding 字段表示底层数据存储结构（针对某一种类型对象，redis 可能会根据情况采用不同的数据结构存储）；ptr 指向数据所在的底层数据结构的存储位置。对象也可能使用多种数据结构存储，比如有序集合可以采用字典和跳跃表同时存储，分别利用两者的单点查询和范围查询优势，由于数据都是用指针存储，因此额外开销只是相关数据结构的 header 相关字段。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* The actual Redis Object */</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_STRING 0    <span class=\"comment\">/* String object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_LIST 1      <span class=\"comment\">/* List object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_SET 2       <span class=\"comment\">/* Set object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ZSET 3      <span class=\"comment\">/* Sorted set object. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_HASH 4      <span class=\"comment\">/* Hash object. */</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* Objects encoding. Some kind of objects like Strings and Hashes can be</span></span><br><span class=\"line\"><span class=\"comment\"> * internally represented in multiple ways. The &#x27;encoding&#x27; field of the object</span></span><br><span class=\"line\"><span class=\"comment\"> * is set to one of this fields for this object. */</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_RAW 0     <span class=\"comment\">/* Raw representation */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_INT 1     <span class=\"comment\">/* Encoded as integer */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_HT 2      <span class=\"comment\">/* Encoded as hash table */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_ZIPMAP 3  <span class=\"comment\">/* No longer used: old hash encoding. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_LINKEDLIST 4 <span class=\"comment\">/* No longer used: old list encoding. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_ZIPLIST 5 <span class=\"comment\">/* No longer used: old list/hash/zset encoding. */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_INTSET 6  <span class=\"comment\">/* Encoded as intset */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_SKIPLIST 7  <span class=\"comment\">/* Encoded as skiplist */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_EMBSTR 8  <span class=\"comment\">/* Embedded sds string encoding */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_QUICKLIST 9 <span class=\"comment\">/* Encoded as linked list of listpacks */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_STREAM 10 <span class=\"comment\">/* Encoded as a radix tree of listpacks */</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OBJ_ENCODING_LISTPACK 11 <span class=\"comment\">/* Encoded as a listpack */</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">redisObject</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> type:<span class=\"number\">4</span>;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> encoding:<span class=\"number\">4</span>;</span><br><span class=\"line\">    <span class=\"type\">unsigned</span> lru:LRU_BITS; <span class=\"comment\">/* LRU time (relative to global lru_clock) or</span></span><br><span class=\"line\"><span class=\"comment\">                            * LFU data (least significant 8 bits frequency</span></span><br><span class=\"line\"><span class=\"comment\">                            * and most significant 16 bits access time). */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> refcount;</span><br><span class=\"line\">    <span class=\"type\">void</span> *ptr;</span><br><span class=\"line\">&#125; robj;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Redis-处理客户端命令\"><a href=\"#Redis-处理客户端命令\" class=\"headerlink\" title=\"Redis 处理客户端命令\"></a>Redis 处理客户端命令</h1><h2 id=\"相关数据结构\"><a href=\"#相关数据结构\" class=\"headerlink\" title=\"相关数据结构\"></a>相关数据结构</h2><p>客户端数据结构，位于 <code>server.h</code> 中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">client</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">uint64_t</span> id;            <span class=\"comment\">/* 客户端唯一 id */</span></span><br><span class=\"line\">    redisDb *db;            <span class=\"comment\">/* 指向当前选择的数据库 */</span></span><br><span class=\"line\">    robj *name;             <span class=\"comment\">/* 客户端名称 */</span></span><br><span class=\"line\">    sds querybuf;           <span class=\"comment\">/* 输入缓冲区，recv 函数接收的客户端命令请求暂时缓存在此处 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> argc;               <span class=\"comment\">/* 命令请求的参数个数 */</span></span><br><span class=\"line\">    robj **argv;            <span class=\"comment\">/* 命令请求的参数内容依次解析到此处 */</span></span><br><span class=\"line\"></span><br><span class=\"line\">    list *reply;            <span class=\"comment\">/* 存储待返回给客户端的命令回复数据 */</span></span><br><span class=\"line\">    <span class=\"type\">unsigned</span> <span class=\"type\">long</span> <span class=\"type\">long</span> reply_bytes; <span class=\"comment\">/* reply 列表所有节点的存储空间之和 */</span></span><br><span class=\"line\">    <span class=\"type\">size_t</span> sentlen;         <span class=\"comment\">/* 已返回给客户端的字节数 */</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">time_t</span> lastinteraction; <span class=\"comment\">/* 客户端上次与服务器交互时间，用于客户端的超时处理 */</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/* Response buffer */</span></span><br><span class=\"line\">    <span class=\"type\">size_t</span> buf_peak; <span class=\"comment\">/* Peak used size of buffer in last 5 sec interval. */</span></span><br><span class=\"line\">    <span class=\"type\">mstime_t</span> buf_peak_last_reset_time; <span class=\"comment\">/* keeps the last time the buffer peak value was reset */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> bufpos;</span><br><span class=\"line\">    <span class=\"type\">size_t</span> buf_usable_size; <span class=\"comment\">/* Usable size of buffer. */</span></span><br><span class=\"line\">    <span class=\"type\">char</span> *buf;</span><br><span class=\"line\">    </span><br><span class=\"line\">    ......</span><br><span class=\"line\">&#125; client;</span><br></pre></td></tr></table></figure>\n\n<p>服务端数据结构，位于 <code>server.h</code> 中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">redisServer</span>&#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> *configfile; \t\t\t\t\t\t<span class=\"comment\">/* 配置文件绝对路径 */</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> dbnum; \t\t\t\t\t\t\t\t<span class=\"comment\">/* 数据库的数目 */</span></span><br><span class=\"line\">\tredisDb *db; \t\t\t\t\t\t\t<span class=\"comment\">/* 数据库数组 */</span></span><br><span class=\"line\">    dict *commands; \t\t\t\t\t\t<span class=\"comment\">/* 命令字典，放置命令名称到命令对象的映射 */</span></span><br><span class=\"line\">    aeEventLoop *el; \t\t\t\t\t\t<span class=\"comment\">/* 事件循环 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> port; \t\t\t\t\t\t\t\t<span class=\"comment\">/* 服务器监听端口号，可通过参数 port 配置，默认为 6379 */</span></span><br><span class=\"line\">\t<span class=\"type\">char</span> * bindaddr[CONFIG_BINDADDR_MAX]; \t<span class=\"comment\">/* 绑定的所有 IP */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> bindaddr_count; \t\t\t\t\t<span class=\"comment\">/* 用户配置的 IP 地址数目 */</span></span><br><span class=\"line\">    socketFds ipfd; \t\t\t\t\t\t<span class=\"comment\">/* 存储所有 IP 地址创建的 socket 文件描述符 */</span></span><br><span class=\"line\">\tlist *clients; \t\t\t\t\t\t\t<span class=\"comment\">/* 当前连接的所有客户端 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> maxidletime; \t\t\t\t\t\t<span class=\"comment\">/* 最大空闲时间 */</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    ......</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>命令数据结构，位于 <code>server.h</code>  中。Redis 支持的所有命令都在全局变量 <code>struct redisCommand redisCommandTable[]</code> 中。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">redisCommand</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> *declared_name;\t\t\t<span class=\"comment\">/* 命令名称 */</span></span><br><span class=\"line\">\tredisCommandProc *proc; \t\t<span class=\"comment\">/* 命令处理函数 */</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> arity;\t\t\t\t\t\t<span class=\"comment\">/* 命令参数数目（-N 表示参数数目必须 &gt;= N；N 表示参数数目必须 = N） */</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> flags;\t\t\t\t\t\t<span class=\"comment\">/* 命令的二进制标志 */</span></span><br><span class=\"line\">\t<span class=\"type\">long</span> <span class=\"type\">long</span> microseconds, calls;\t<span class=\"comment\">/* 服务器启动至今该命令总执行时间；该命令总执行次数。用于统计 */</span></span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>当服务端收到一个命令请求时，如果到 redisCommandTable 中查找命令，耗时太高。因此，Redis 在服务器初始时，将命令表转化为一个字典。</p>\n<p>事件循环数据结构，位于 <code>ae.h</code> 中。文件事件（socket 的可读可写时间）和时间事件（定时任务）都封装在该数据结构中</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* State of an event based program */</span></span><br><span class=\"line\"><span class=\"comment\">/* 事件处理器的状态 */</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">aeEventLoop</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* 目前已经注册的最大文件描述符 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> maxfd;   <span class=\"comment\">/* highest file descriptor currently registered */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 目前追踪的最大文件描述符数量，redis 初始化的时候会设置好的一个固定值</span></span><br><span class=\"line\"><span class=\"comment\">     * 初始化事件循环器的时候会把这个值赋值给 setsize，见 server.h 文件的 CONFIG_FDSET_INCR</span></span><br><span class=\"line\"><span class=\"comment\">     * server.maxclients + RESERVED_FDS（32） + 96 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> setsize; <span class=\"comment\">/* max number of file descriptors tracked */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 用于生成时间事件的 ID */</span></span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"type\">long</span> timeEventNextId;</span><br><span class=\"line\">    <span class=\"comment\">/* 已注册的文件事件，在初始化的时候会初始化 setsize 个位置 */</span></span><br><span class=\"line\">    aeFileEvent *events; <span class=\"comment\">/* Registered events */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 已就绪的文件事件 */</span></span><br><span class=\"line\">    aeFiredEvent *fired; <span class=\"comment\">/* Fired events */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 时间事件链表的头节点 */</span></span><br><span class=\"line\">    aeTimeEvent *timeEventHead;</span><br><span class=\"line\">    <span class=\"comment\">/* 事件处理器的开关 */</span></span><br><span class=\"line\">    <span class=\"type\">int</span> stop;</span><br><span class=\"line\">    <span class=\"comment\">/* 多路复用库的私有数据 */</span></span><br><span class=\"line\">    <span class=\"type\">void</span> *apidata; <span class=\"comment\">/* This is used for polling API specific data */</span></span><br><span class=\"line\">    <span class=\"comment\">/* 在处理事件前要执行的函数 */</span></span><br><span class=\"line\">    aeBeforeSleepProc *beforesleep;</span><br><span class=\"line\">    <span class=\"comment\">/* 在处理事件后要执行的函数 */</span></span><br><span class=\"line\">    aeBeforeSleepProc *aftersleep;</span><br><span class=\"line\">    <span class=\"type\">int</span> flags;</span><br><span class=\"line\">&#125; aeEventLoop;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"服务端启动\"><a href=\"#服务端启动\" class=\"headerlink\" title=\"服务端启动\"></a>服务端启动</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><ol>\n<li><p>初始化配置，包括用户可配置的参数，以及命令表数组初始化成字典；</p>\n<p> 位于 <code>server.c</code> 中的 <code>initServerConfig</code> 函数执行。</p>\n</li>\n<li><p>加载并解析配置文件；</p>\n<p> 位于 <code>config.c</code> 中的 <code>loadServerConfig</code> 函数执行。</p>\n</li>\n<li><p>初始化服务器内部变量，比如数据库、全局变量和共享对象等；</p>\n<p> 位于 <code>server.c</code> 中的 <code>initServer</code> 函数执行。</p>\n</li>\n<li><p>创建事件循环 eventLoop，分配结构体所需内存，创建 epoll；</p>\n<p> 位于 <code>ae.c</code> 中的 <code>aeCreateEventLoop</code> 函数执行。</p>\n</li>\n</ol>\n<h3 id=\"启动监听\"><a href=\"#启动监听\" class=\"headerlink\" title=\"启动监听\"></a>启动监听</h3><ol>\n<li><p>创建 socket 并启动监听；</p>\n<p> 位于 <code>server.c</code> 中的 <code>listenToPort</code> 函数执行。</p>\n</li>\n<li><p>创建文件事件和时间事件；</p>\n</li>\n<li><p>开启事件循环；</p>\n<p> 位于 <code>ae.c</code> 中的 <code>aeMain</code> 函数执行。</p>\n</li>\n</ol>\n<h2 id=\"命令处理\"><a href=\"#命令处理\" class=\"headerlink\" title=\"命令处理\"></a>命令处理</h2><p>服务端启动完成后，只需要等待客户端连接并发送命令请求即可。</p>\n<h3 id=\"命令解析\"><a href=\"#命令解析\" class=\"headerlink\" title=\"命令解析\"></a>命令解析</h3><p>命令请求首先通过 <code>readQueryFromClient</code> 函数处理后存储在客户端的 <code>querybuf</code> 中，并调用 <code>processInputBuffer</code> 函数解析命令请求，将参数个数和各个参数存储在客户端的 <code>argc</code> 和 <code>argv</code> 中。</p>\n<h3 id=\"命令调用\"><a href=\"#命令调用\" class=\"headerlink\" title=\"命令调用\"></a>命令调用</h3><p>解析完成后，会调用 <code>processCommand</code> 函数处理命令请求，包括一系列的校验以及最后调用 <code>call</code> 函数，通过 <code>c-&gt;cmd-&gt;proc(c)</code> 执行命令。**(后续如果想要读命令的执行细节，就只需要关注这一块就可以了)**</p>\n<h3 id=\"返回结果\"><a href=\"#返回结果\" class=\"headerlink\" title=\"返回结果\"></a>返回结果</h3><p>结果保存在客户端的 <code>reply</code> 或 <code>buf</code> 中。</p>\n","categories":["Redis"],"tags":["Redis","源码分析"]},{"title":"TPP（ASPLOS 23）","url":"/2023/12/25/TPP%EF%BC%88ASPLOS%2023%EF%BC%89/","content":"<p>大规模数据中心的内存需求急剧增加，此时，CXL 带来的内存扩展将可能成为这一问题的解决方案。然而，如果没有有效的内存管理（比如页面放置），可能会导致本地内存和 CXL 内存这一分层内存性能下降。</p>\n<p>因此，作者提出一个操作系统内核级的应用程序透明页面放置策略，无需应用程序特定的提示，能够有效地做到从本地内存将冷数据降级到 CXL 内存，从 CXL 内存将热数据提升到本地内存。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li>内存分层是否有益？<ul>\n<li>静态层面上每个应用程序在一定时期内，页面们的冷热度是否保持不变；同一应用程序的不同类型页面是否有不同的冷热度；不同类型的应用程序申请的页面是否有不同的冷热度；<strong>（performance 的前提）</strong></li>\n<li>动态层面上能否以较低的开销实时监测页面的冷热度；<strong>（efficiency 开销是否值得）</strong>。然后作者构建了一个用户态工具 Chameleon，使用现有 CPU 的 PEBS 机制表征应用程序的内存访问行为，并生成不同类型的页面（匿名页面，file page cache，共享内存等等）的热力图。</li>\n</ul>\n</li>\n<li>Chameleon 表征（1）很大一部分的工作集是暖&#x2F;冷的，可以卸载到低速层而不会对性能带来严重影响；（2）大部分匿名内存（堆内存，栈内存，mmap）往往更热，而大部分 file page cache 更冷；（3）页面访问模式一定时间内保持稳定（分钟到小时）；（4）通过新的（取消）分配，实际物理页地址可以相当快地将其行为从热更改为冷，反之亦然。静态页面分配会显着降低性能。</li>\n<li>基于 Chameleon 的观察，提出操作系统级透明页面放置机制 TPP，有效地将页面放置在分层内存系统中，以便相对热的页面保留在快速内存层中，而冷页面则移动到慢速内存层中。三个组件<ul>\n<li>轻量级回收机制：将较冷页面降级到低速层；</li>\n<li>将多 NUMA 系统的分配和回收逻辑解耦，以维持高速层节点上的空闲页面空间；</li>\n<li>页面提升机制：有效的识别低速层中的热页，已将其提升到高速层；</li>\n<li>同时支持页面类型感知分配，可以将匿名页面放置到高速层，file page cache 放置到低速层。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h1><p>数据中心应用对内存的需求日益增长；</p>\n<p>CPU 和内存的紧耦合设计限制了灵活性和可扩展性；而 CXL 的到来有助于独立扩展内存资源，并确保更好地利用闲置资源；</p>\n<p>meta 应用程序分配的内存中有 55%-80% 在任何两分钟间隔内保持空闲状态，如果将这些内存页面迁移至低速层，将可以为更多热页分配到高速层。</p>\n<h1 id=\"Chameleon\"><a href=\"#Chameleon\" class=\"headerlink\" title=\"Chameleon\"></a>Chameleon</h1><h2 id=\"一段时间内页面的冷热度\"><a href=\"#一段时间内页面的冷热度\" class=\"headerlink\" title=\"一段时间内页面的冷热度\"></a>一段时间内页面的冷热度</h2><p>Web，cache，warehouse，Ads 四类应用程序，在任何两分钟间隔内，分配的内存中有 55%-80% 是闲置的（这两分钟内没有被访问），可以将这些页面迁移至低速层。</p>\n<h2 id=\"不同类型页面的冷热度\"><a href=\"#不同类型页面的冷热度\" class=\"headerlink\" title=\"不同类型页面的冷热度\"></a>不同类型页面的冷热度</h2><p>大部分匿名页面是热的，然而 file page 则相对较冷。</p>\n<h2 id=\"随着时间推移，不同类型页面的使用情况\"><a href=\"#随着时间推移，不同类型页面的使用情况\" class=\"headerlink\" title=\"随着时间推移，不同类型页面的使用情况\"></a>随着时间推移，不同类型页面的使用情况</h2><p>Web 服务开启时，将虚拟机的二进制和字节码文件加载至内存，此时 file page 占据大部分内存，随着时间推移，file page 比例逐渐减少，匿名页面比例之间增加。</p>\n<p>Cache 应用程序的 file page 占据 70-82%。</p>\n<p>Data Warehousr 工作负载的匿名页面占据 85%。</p>\n<p>大部分应用程序的匿名页面和 file page 比例比较稳定，因此，页面放置机制在做决策时，应该具备感知页面类型的功能（这有什么关联吗？）</p>\n<h2 id=\"页面类型对性能的影响\"><a href=\"#页面类型对性能的影响\" class=\"headerlink\" title=\"页面类型对性能的影响\"></a>页面类型对性能的影响</h2><p>大多数情况下，随着匿名页面的使用增加，应用程序的吞吐量增加。</p>\n<h2 id=\"页面重用时间粒度\"><a href=\"#页面重用时间粒度\" class=\"headerlink\" title=\"页面重用时间粒度\"></a>页面重用时间粒度</h2><p>Web 和 Cache 应用程序 80% 的页面在十分钟间隔后会得到访问，而 Warehoust 应用程序则只有 20%，Warehoust 会持续分配新页面（扫描工作负载）。</p>\n<h1 id=\"TPP\"><a href=\"#TPP\" class=\"headerlink\" title=\"TPP\"></a>TPP</h1><h2 id=\"轻量降级到-CXL-内存\"><a href=\"#轻量降级到-CXL-内存\" class=\"headerlink\" title=\"轻量降级到 CXL 内存\"></a>轻量降级到 CXL 内存</h2><p>本地内存使用 LRU 链表，当本地内存容量达到降级水位时，将 LRU 尾部对象异步迁移至 CXL 节点；如果有多个 CXL 节点，则降级至距离当前 CPU 最近的那一个。</p>\n<h2 id=\"解耦分配和回收\"><a href=\"#解耦分配和回收\" class=\"headerlink\" title=\"解耦分配和回收\"></a>解耦分配和回收</h2><p>原有逻辑中，如果节点空闲内存低于低水位，TPP 将冷页面降级到 CXL 节点，并且，本地节点的内存分配将中止指导回收器释放了足够的内存达到高水位。由于回收比分配慢，本地内存节点频繁中止分配，更多的页面被分配到 CXL 节点，导致性能下降。</p>\n<p>因此，有必要为本地内存预留足够的容量，如此一下，新的分配可以蜂拥至本地内存节点（新的分配页面往往很流行），并且本地内存节点可以接收来自 CXL 节点的页面提升。</p>\n<p>具体做法就是设置降级水位和分配水位，当达到降级水位时，将 LRU 的尾页面迁移至 CXL 内存，此时仍可以在本地内存节点分配内存。</p>\n<img src=\"/2023/12/25/TPP%EF%BC%88ASPLOS%2023%EF%BC%89/TPP_decouple.jpg\" class=\"\" title=\"TPP_decouple\">\n\n<h2 id=\"来自-CXL-节点的页面提升\"><a href=\"#来自-CXL-节点的页面提升\" class=\"headerlink\" title=\"来自 CXL 节点的页面提升\"></a>来自 CXL 节点的页面提升</h2>","categories":["Memory"],"tags":["论文阅读","CXL","分层"]},{"title":"TinyLFU (TOS 17)","url":"/2023/07/12/TinyLFU%20(TOS%2017)/","content":"<p>SOTA 缓存准入（高效）。</p>\n<h1 id=\"TinyLFU-Architecture\"><a href=\"#TinyLFU-Architecture\" class=\"headerlink\" title=\"TinyLFU Architecture\"></a>TinyLFU Architecture</h1><h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><p>缓存驱逐策略选择驱逐对象，TinyLFU 将驱逐对象和缺失对象进行比较，以决定是否以驱逐对象为代价准入缺失对象。</p>\n<img src=\"/2023/07/12/TinyLFU%20(TOS%2017)/Overview.jpg\" class=\"\" title=\"Overview\">\n\n<h2 id=\"Challenges\"><a href=\"#Challenges\" class=\"headerlink\" title=\"Challenges\"></a>Challenges</h2><ol>\n<li>如何维护最近请求对象的历史信息，删除久远的历史信息；</li>\n<li>减少内存开销。</li>\n</ol>\n<h2 id=\"Freshness-Mechanism\"><a href=\"#Freshness-Mechanism\" class=\"headerlink\" title=\"Freshness Mechanism\"></a>Freshness Mechanism</h2><p>每次请求发往近似 sketch 时，都会增加一个全局计数器 S。一旦该计数器 S 的值达到样本大小（W），就会触发 reset，将 S 和近似 sketch 中的所有其他计数器除以 2。</p>\n<p><strong>优势</strong></p>\n<ol>\n<li>只会额外增加 Log(W) 个 bits 的空间开销；</li>\n<li>久远的请求信息会指数衰减。</li>\n</ol>\n<h2 id=\"Counting-Bloom-Filter\"><a href=\"#Counting-Bloom-Filter\" class=\"headerlink\" title=\"Counting Bloom Filter\"></a>Counting Bloom Filter</h2><img src=\"/2023/07/12/TinyLFU%20(TOS%2017)/count_bloom_filter.jpg\" class=\"\" title=\"count_bloom_filter\">\n\n<p>增加计数：当请求到来时，通过每个哈希函数映射到各自的索引，将最小计数的对应计数器值+1，如 {2, 2, 5} -&gt; {3, 3, 5}（实际代码还是倾向增加所有计数器）；</p>\n<p>估算计数：通过每个哈希函数映射到各自的索引，取对应计数器中的最小值。</p>\n<h3 id=\"Space-Reduction\"><a href=\"#Space-Reduction\" class=\"headerlink\" title=\"Space Reduction\"></a>Space Reduction</h3><ol>\n<li><p>总请求量每到 W，则会对全局计数器和近似 sketch 中的所有计数器除以 2，那么理论上，每个 sketch 中的计数器需要 Log(W) 个 bits，但如果假设请求均匀，对于容量为 C 的缓存，通过牺牲部分准确性（频率最高到达 W&#x2F;C），则只需 Log(W&#x2F;C) 个 bits。；</p>\n</li>\n<li><p>如果很多对象只访问一次，则它们会占据大量的计数器。因此，放置一个常规布隆计数器 DoorKeeper 在 Counting Bloom Filter 之前，对象到来时，先检查是否在 DoorKeeper 中，如果不在，则插入到 DoorKeeper 中，否则，请求 Counting Bloom Filter。每次 reset 操作也会清空 DoorKeeper。</p>\n<p> （门卫，很形象 (:</p>\n</li>\n</ol>\n<h1 id=\"Extension\"><a href=\"#Extension\" class=\"headerlink\" title=\"Extension\"></a>Extension</h1><p>TinyLFU 无法适应对象大小可变的缓存环境，因此，作者在 TOS 22 上提出了一种大小感知的 TinyLFU 策略（AV-TinyLFU，aggregated victims）。</p>\n<p>当准入一个对象需要驱逐多个对象时，使用分离的替换策略选出多个驱逐对象，计算所有驱逐对象的频率和，将之与请求对象的频率进行比较，如果 freq(req) &gt; freq(victims)，则准入；否则，不准入，且将所有 victims 进行 promote。（<strong>考虑 freq&#x2F;size 是否更好</strong>）</p>\n","categories":["Cache"],"tags":["Cache","论文阅读"]},{"title":"cache replacement policies","url":"/2023/04/01/cache_replace_policies/","content":"<p>Abstract：</p>\n<ol>\n<li>本书总结了 CPU 数据缓存的缓存替换策略概况；</li>\n<li>重点讨论算法，因此作者及那个以前的策略分为两大类——粗粒度和细粒度，每类又分为三个子类，以描述解决缓存替换问题的不同方法和每个类别中重要工作的总结；</li>\n<li>探索更多的评价指标，包括不单单局限于缓存缺失率指标、针对多喝设置定制的解决方案、考虑和预取之间的影响、考虑新型内存；</li>\n<li>最后，本书讨论未来工作的趋势和挑战。</li>\n</ol>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li><strong>why cache is important？</strong> 如今传输数据的延迟比执行一条指令的延迟还长，而缓存的存在，可以减少内存延迟和减少内存流量。</li>\n<li>启发式——考虑访问频率、新近度；最近的预测技术。</li>\n<li>如何将缓存替换策略和死块预测结合起来。</li>\n</ol>\n<h1 id=\"A-Taxonomy-of-Cache-Replacement-Policies\"><a href=\"#A-Taxonomy-of-Cache-Replacement-Policies\" class=\"headerlink\" title=\"A Taxonomy of Cache Replacement Policies\"></a>A Taxonomy of Cache Replacement Policies</h1><ol>\n<li><strong>本书为什么这样分类？</strong><ul>\n<li>建立在缓存替换策略解决预测问题的观察之上，其目标是预测是否应允许任何给定对象保留在缓存中；</li>\n<li>预测的决策发生在缓存块中的很多地方，从缓存块插入缓存开始，到它从缓存中淘汰；</li>\n</ul>\n</li>\n<li><strong>本书对缓存替换算法如何分类？</strong><ul>\n<li>首先根据插入决策的粒度分为两类：第一类是<strong>粗粒度</strong>策略，对所有插入对象进行相同的处理，并且仅依据它们在缓存中驻留的表现进行区别对待，例如，对象在缓存中被重用次数增加时优先级会提高；第二类是<strong>细粒度</strong>策略，除了在缓存中驻留的表现会影响优先级外，不同对象在插入时就不一样，这依靠缓存访问模式的历史信息，比如，细粒度策略了解到某个特定指令加载的对象在过去没有被重用就淘汰了，它就会给它在插入时更低的优先级。</li>\n</ul>\n</li>\n<li>缓存替换策略包括：<ul>\n<li>插入策略：How does the replacement policy initialize the replacement state of a new line when it is inserted into the cache?</li>\n<li>提升策略：How does the replacement policy update the replacement state of a line when it hits in the cache?</li>\n<li>衰减策略：How does the replacement policy update the replacement state of a line when a competing line is inserted or promoted?</li>\n<li>驱逐策略：which line does the replacement policy evict to hold new line?</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Coarse-Grained-Policies\"><a href=\"#Coarse-Grained-Policies\" class=\"headerlink\" title=\"Coarse-Grained Policies\"></a>Coarse-Grained Policies</h2><p>粗粒度策略可以认为无“插入策略”，它就区分驻留对象优先级的方式又分为三类：新近度、频率、根据工作负载的变化动态选择不同粗粒度策略的混合策略。</p>\n<h2 id=\"Fine-Grained-Policies\"><a href=\"#Fine-Grained-Policies\" class=\"headerlink\" title=\"Fine-Grained Policies\"></a>Fine-Grained Policies</h2><p>细粒度策略就对象插入时的区分方式分为两类：基于分类（对对象分为缓存友好和缓存不友好两种）、基于重用距离（尝试去预测对象的重用距离）</p>\n<ol>\n<li>分类：通常被认为是最先进的，因为（1）可以利用过去的访问历史为未来做出更正确的决策，（2）可以适应各种缓存访问模式。</li>\n<li>重用距离预测：使用历史信息进行重用距离预测有好有坏，在 4.1 节中进行讨论。</li>\n</ol>\n<h2 id=\"Design-Considerations\"><a href=\"#Design-Considerations\" class=\"headerlink\" title=\"Design Considerations\"></a>Design Considerations</h2><p>缓存替换策略的只要目标是提升缓存命中率，以下这些设计因素有助于实现更高的命中率：</p>\n<ol>\n<li>粒度：</li>\n<li>历史信息：替换策略在做决策时使用了多少历史信息；</li>\n<li>访问模式：替换策略是否适应工作负载。</li>\n</ol>\n<p>在大体趋势上，从左到右是更近时候提出的，使用了更长历史记录并且可以适应更多工作负载的算法，可以看到，细粒度预测非常有优势，它提供了：</p>\n<ol>\n<li>允许细粒度策略仅将缓存空间专用于缓存友好对象；</li>\n<li>允许细粒度策略的每个缓存组动态适应工作负载的变化。</li>\n</ol>\n<img src=\"/2023/04/01/cache_replace_policies/%E6%9B%BF%E6%8D%A2%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB.jpg\" class=\"\" title=\"替换算法分类\">\n\n\n\n<h1 id=\"Coarse-Grained-Replacement-Policies\"><a href=\"#Coarse-Grained-Replacement-Policies\" class=\"headerlink\" title=\"Coarse-Grained Replacement Policies\"></a>Coarse-Grained Replacement Policies</h1><p> 每个对象仅和少量替换状态相关，所有新插入对象被统一初始化，然后在缓存命中时进行简单的提升操作。</p>\n<h2 id=\"Recency-Based-Policies\"><a href=\"#Recency-Based-Policies\" class=\"headerlink\" title=\"Recency-Based Policies\"></a>Recency-Based Policies</h2><ol>\n<li>LRU：无法适应搅拌和扫描工作负载；</li>\n<li>LRU变种：<ul>\n<li>MRU：仅能适应搅拌工作负载；难以适应工作负载模式的变化，因为新来的工作集很难被持续缓存；</li>\n<li>EELRU：主要思想是检测工作集大小是否超过缓存大小，当工作集小于缓存容量时，会驱逐 LRU 端对象，当工作集大于缓存容量时，则驱逐距离 MRU 位置长度为 e 的对象；</li>\n<li>Seg-LRU</li>\n<li>LIP：和 MRU 其实一个道理，但它通过改变插入策略而使得 LIP 的驱逐策略和 LRU 一致，这启发我们在设计 LRU 变体算法时，只需要修改插入和提升操作，驱逐操作保持不变即可；</li>\n<li>BIP：以较大概率插入 LRU 端，较低概率插入 MRU 端；</li>\n<li>SRRIP、BRRIP、PDP、GIPPR</li>\n</ul>\n</li>\n</ol>\n<p>可以看到 LRU 家族很多都在考虑适应搅拌工作负载，因为 CPU 缓存容量小于工作集是较易发生的，但这在内存缓存或者 CDN 缓存中似乎很难出现。</p>\n<h2 id=\"Frequency-Based-Policies\"><a href=\"#Frequency-Based-Policies\" class=\"headerlink\" title=\"Frequency-Based Policies\"></a>Frequency-Based Policies</h2><p>基于频率的策略可以一定程度上抵抗扫描工作负载。</p>\n<ol>\n<li>LFU：容易造成缓存污染；</li>\n<li>Frequency-Based Replacement（FBR）：靠近 MRU 端的对象访问频数不会增加，以避免短时间的高频访问拔高对象优先级；</li>\n<li>Least Recently&#x2F;Frequently Used（LRFU）</li>\n</ol>\n<h2 id=\"Hybrid-Policies\"><a href=\"#Hybrid-Policies\" class=\"headerlink\" title=\"Hybrid Policies\"></a>Hybrid Policies</h2><p>混合策略能够适应工作负载的变化。它面临两个挑战：（1）如何准确确定哪个策略是最有利的；（2）以较低的硬件成本管理多个策略。</p>\n<ol>\n<li>ARC</li>\n<li>DIP</li>\n<li>DRRIP</li>\n</ol>\n<h1 id=\"Fine-Grained-Replacement-Policies\"><a href=\"#Fine-Grained-Replacement-Policies\" class=\"headerlink\" title=\"Fine-Grained Replacement Policies\"></a>Fine-Grained Replacement Policies</h1><p>细粒度策略在插入时就会根据不同对象的历史信息进行区分，以决定它们的插入位置。例如，如果对象上一次加入缓存后没有被重用就被驱逐了，当再次请求该对象时，就可以把它插入到较低的位置。</p>\n<p>细粒度策略基于预测插入优先级的指标分为三类：预测重用距离、预测是否被重用、引入的新预测指标。</p>\n<h2 id=\"Reuse-Distance-Prediction-Policies\"><a href=\"#Reuse-Distance-Prediction-Policies\" class=\"headerlink\" title=\"Reuse Distance Prediction Policies\"></a>Reuse Distance Prediction Policies</h2><p>重用距离的具体值很难被准确预测，因此，更现实的解决方案是估计重用距离分布或其它聚合重用距离统计。</p>\n<h3 id=\"Expiration-Based-Dead-Block-Predictors\"><a href=\"#Expiration-Based-Dead-Block-Predictors\" class=\"headerlink\" title=\"Expiration-Based Dead Block Predictors\"></a>Expiration-Based Dead Block Predictors</h3><p>许多死块预测器使用过去的实际后向重用距离来估计未来的前向重用距离，并驱逐没有在预期重用距离内被重用的对象；</p>\n<p>还有一些使用过去的实际生命周期预测未来的生命周期，当在预测的两倍生命周期时间内没有被重用，该块就从缓存中淘汰。</p>\n<h3 id=\"Reuse-Distance-Ordering\"><a href=\"#Reuse-Distance-Ordering\" class=\"headerlink\" title=\"Reuse Distance Ordering\"></a>Reuse Distance Ordering</h3><h2 id=\"Classification-Based-Policies\"><a href=\"#Classification-Based-Policies\" class=\"headerlink\" title=\"Classification-Based Policies\"></a>Classification-Based Policies</h2><p>基于分类的策略相比重用距离预测简单一下，接下来会考虑一下设计问题：</p>\n<ol>\n<li>Which caching solution is the policy learning?</li>\n<li>What is the prediction mechanism, and at what granularity are the predictions being made?</li>\n<li>What is the aging mechanism for ensuring that inaccurate predictions are eventually evicted?</li>\n</ol>\n<h3 id=\"Sampling-Based-Dead-Block-Prediction（SDBP）\"><a href=\"#Sampling-Based-Dead-Block-Prediction（SDBP）\" class=\"headerlink\" title=\"Sampling Based Dead Block Prediction（SDBP）\"></a>Sampling Based Dead Block Prediction（SDBP）</h3><p>为降低对所有告诉缓存块维护指令跟踪的成本，引入了基于采样的死块预测器。预测结果为死块的不会被插入缓存（缓存准入）。对于上面的三个问题的回答：</p>\n<ol>\n<li>SDBP 从 LRU 采样器中学习缓存决策；</li>\n<li>在 PC 的粒度上使用 skewed 预测器预测死块；</li>\n<li>预测为缓存友好的一次性访问按照基本的替换策略衰减，预测为缓存不友好的可以重用的对象永远无法得到重用（不太好）。</li>\n</ol>\n<h3 id=\"Signature-Based-Hit-Prediction（SHIP）\"><a href=\"#Signature-Based-Hit-Prediction（SHIP）\" class=\"headerlink\" title=\"Signature Based Hit Prediction（SHIP）\"></a>Signature Based Hit Prediction（SHIP）</h3><p>idea 是重用行为与将对象插入缓存的 PC 值相关，在 memory cache 和 CDN cache 中似乎没有类似相关的参照。</p>\n<h3 id=\"Hawkeye\"><a href=\"#Hawkeye\" class=\"headerlink\" title=\"Hawkeye\"></a>Hawkeye</h3><p><strong>个人感觉参考价值比较高，也带有一点 learning 的想法。</strong></p>\n<ol>\n<li><p>主要目标：避免基于启发式的解决方案（例如LRU）的病态；</p>\n</li>\n<li><p>idea：建立在 Belady 方法之上，因为 Belady 方法作为理论最优方法，它是适应所有工作负载的，但它又必须依靠未来的信息，无法使用在实际生产环境中；</p>\n</li>\n<li><p>关键见解：虽然无法知道未来的信息，但可以应用 Belady 于过去的内存引用。如果过去的行为和未来的行为相关性很高，那么效果应该也是很好的；</p>\n</li>\n<li><p><strong>该使用多少历史信息？</strong> 为了精准地找到后向重用距离，应该使用所有的历史信息，但这无疑开销很大而且是越来越大的那种，在这里作者使用实验来指导时间窗口的大小，最终使用 8 倍缓存容量的时间窗口。</p>\n <img src=\"/2023/04/01/cache_replace_policies/hawkeye%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3.jpg\" class=\"\" title=\"hawkeye时间窗口\">\n</li>\n<li><p>8 倍缓存空间的历史信息似乎仍然太大了，于是 Hawkeye 只计算几个抽样集的最优解。</p>\n</li>\n</ol>\n<p>通过使用过去的访问记录来模拟 OPT 算法的行为产生输入来训练 Hawkeye Predictor，再基于 Predictor 做决策。</p>\n<h3 id=\"Perceptron-Based-Prediction\"><a href=\"#Perceptron-Based-Prediction\" class=\"headerlink\" title=\"Perceptron-Based Prediction\"></a>Perceptron-Based Prediction</h3><p><strong>这样从发展脉络来看，AI4Cache 是可以走的</strong>。</p>\n<p>SDBP、SHIP 和 Hawkeye 全都使用基于 PC 的预测器实现了接近 70%-80% 的准确率。后面有研究者使用更好的特征和更好的预测器模型提高了预测准确率。</p>\n<p>比如使用感知机（简单的人工神经元）为 PC 增加更多的特征（PC 的历史、来自内存地址的位、数据的压缩表示、块被访问的次数），每个特征都被用来索引一个不同的饱和计数器表，然后将其相加并与阈值进行比较以生成二进制预测结果。小部分访问被采样以便按照感知机规则去更新模型：如果预测不正确，或者如果总和未能超过某个数量级，则计数器在访问时递减并在驱逐时递增。</p>\n<h2 id=\"Other-Prediction-Metrics\"><a href=\"#Other-Prediction-Metrics\" class=\"headerlink\" title=\"Other Prediction Metrics\"></a>Other Prediction Metrics</h2><h3 id=\"Economic-Value-Added（EVA）\"><a href=\"#Economic-Value-Added（EVA）\" class=\"headerlink\" title=\"Economic Value Added（EVA）\"></a>Economic Value Added（EVA）</h3><p>一方面，对象在缓存中得到重用而创造出价值，另一方面，一个对象在缓存中待得越久它就牺牲了部分缓存空间。</p>\n<p>$EVA &#x3D; 预期命中数 - (缓存命中率&#x2F;缓存大小) * 预期在缓存驻留时间$</p>\n<h1 id=\"Richer-Considerations\"><a href=\"#Richer-Considerations\" class=\"headerlink\" title=\"Richer Considerations\"></a>Richer Considerations</h1><p>（前面都是谈 CPU 缓存替换的具体算法，这一节是讨论缓存替换策略中的具体主义事项，值得借鉴的地方会更多。）</p>\n<p>前面都是狭隘地关注缓存替换问题，无论是从指标方面——缓存命中率，还是在和计算机其它组件的配合方面。</p>\n<h2 id=\"Cost-Aware-Cache-Replacement\"><a href=\"#Cost-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Cost-Aware Cache Replacement\"></a>Cost-Aware Cache Replacement</h2><p>如果所有的缓存未命中代价相同，那么设计最小化全局缺失率的缓存替换算法就可以了，但实际上，不同的缓存未命中对性能产生不同的影响，比如孤立的未命中（低内存级并行性）往往比集群未命中（高内存并行性）代价更高，因为<strong>并行性越高，多个缓存缺失同时发生可能只会造成一次停顿</strong>，再比如在程序关键路径上的未命中对程序性能的影响比不在程序关键路径上的更大。考虑到这些成本的智能替换策略可以优先缓存高成本的未命中（以较低的命中率为代价）以获得更好的程序性能。</p>\n<ol>\n<li>最优成本感知替换策略（The optimal cost-aware replacement policy, CSOPT）目的就是将所有未命中的总成本降至最低，而不是将未命中的数量降至最低。其基本思想是遵循 MIN 策略，同样也是离线算法。</li>\n<li>MLP（memory-level parallelism）考虑并行性缺失和独立性缺失的影响（第一个例子）；</li>\n</ol>\n<h2 id=\"Criticality-Driven-Cache-Optimizations\"><a href=\"#Criticality-Driven-Cache-Optimizations\" class=\"headerlink\" title=\"Criticality-Driven Cache Optimizations\"></a>Criticality-Driven Cache Optimizations</h2><p>关键性是比 MLP 更通用的成本函数，将关键负载定义为需要提前完成以防止处理器停顿的任何负载，而非关键负载是可以容忍长时间延迟的负载。而关键驱动的缓存优化优先考虑关键负载而不是非关键负载。</p>\n<p>将关键负载从低级别缓存预取到高级别缓存。</p>\n<h2 id=\"Mutil-Core-Aware-Cache-Management\"><a href=\"#Mutil-Core-Aware-Cache-Management\" class=\"headerlink\" title=\"Mutil-Core-Aware Cache Management\"></a>Mutil-Core-Aware Cache Management</h2><p>多核竞争访问共享缓存容量可能会发生流式应用程序驱逐其它缓存友好应用程序的有用数据。</p>\n<p>为处理这种共享缓存干扰，可以（1）在内核之间进行对缓存进行分区以避免竞争；（2）修改替换策略以避免病态的访问行为。</p>\n<p>缓存分区保证了强隔离和公平性保证。在实现时它有两个主要考虑因素：（1）如何在缓存中强制执行分区？（2）分区大小如何确定？</p>\n<p>执行缓存分区最常用的机制是为每个应用程序分配专用通道，这样每个应用程序只能从自己的分区中插入和驱逐数据。更高级的做法是避免硬分区，取而代之的是通过修改替换策略来实现隐式分区。</p>\n<p>分区大小可以由用户、操作系统或硬件决定。</p>\n<p>显示分区如 UCP、ASM-Cache，隐式分区如 TADIP、PIPP 等。</p>\n<h2 id=\"Prefetch-Aware-Cache-Replacement\"><a href=\"#Prefetch-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Prefetch-Aware Cache Replacement\"></a>Prefetch-Aware Cache Replacement</h2><p>有效的预取隐藏 DRAM 访问的长延时，因此，将预取器和缓存替换策略结合起来是一个很好的想法。</p>\n<p>设计预取感知的缓存替换策略有两个主要目标：（1）替换策略应该避免不准确的预取导致的<strong>缓存污染</strong>；（2）替换策略应该优先丢弃可以预取的对象而不是难以预取的对象。</p>\n<h3 id=\"Cache-Pollution\"><a href=\"#Cache-Pollution\" class=\"headerlink\" title=\"Cache Pollution\"></a>Cache Pollution</h3><p>大多数预取感知缓存替换策略侧重于通过识别和驱逐不准确的预取来减少缓存污染。这样的解决方案可以分为两大类。</p>\n<ol>\n<li>从预取器那里获取反馈来识别可能不准确的预取请求；</li>\n<li>独立于预取器工作并监视缓存行为以适应替换决策，但可能缺少精确的预取器特定信息。</li>\n</ol>\n<h3 id=\"Deprioritizing-Prefetchable-Lines\"><a href=\"#Deprioritizing-Prefetchable-Lines\" class=\"headerlink\" title=\"Deprioritizing Prefetchable Lines\"></a>Deprioritizing Prefetchable Lines</h3><p>当存在预取行为时，MIN 算法是不完整的，因为它不区分可预取和难以预取的对象。</p>\n<h2 id=\"Cache-Architecture-Aware-Cache-Replacement\"><a href=\"#Cache-Architecture-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Cache Architecture-Aware Cache Replacement\"></a>Cache Architecture-Aware Cache Replacement</h2><p>到目前为止，我们假设由缓存替换策略推断的优先级排序与缓存架构无关。但是，缓存架构的变化可能会对缓存替换产生影响。我们现在讨论缓存架构的两个变化。</p>\n<h3 id=\"Inclusion-Aware-Cache-Replacement\"><a href=\"#Inclusion-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Inclusion-Aware Cache Replacement\"></a>Inclusion-Aware Cache Replacement</h3><p>inclusive 缓存要求多级缓存层次结构中所有较小的缓存内容是 LLC 的子集，这种方式简化了缓存一致性协议，但是限制了缓存层次的有效容量为 LLC 的大小，同时当对象从 LLC 中逐出时，小容量缓存中的该对象被强制无效。</p>\n<p>Inclusion-Aware Cache Replacement 通常在较小缓存中持续驻留热数据，同时延长 LLC 中此类缓存的生命周期。</p>\n<h3 id=\"Compression-Aware-Cache-Replacement\"><a href=\"#Compression-Aware-Cache-Replacement\" class=\"headerlink\" title=\"Compression-Aware Cache Replacement\"></a>Compression-Aware Cache Replacement</h3><p>增加缓存容量可以提高命中率，提高系统性能，但它是以缓存成本和耗电为代价的。压缩缓存提供了另一种解决方案，其中缓存中的数据被压缩以获得更高的有效容量。例如，如果每个缓存块都可以压缩 4 倍，则有效缓存容量可以增加 4 倍。</p>\n<p>当然，并非所有缓存条目都可以压缩，因此压缩会生成可变大小的缓存块，较大的（未压缩的）块比较小的（压缩的）块消耗更多的缓存空间。</p>\n<p>进行驱逐操作时，往往更倾向于驱逐较大的、时间局部性较差的缓存块。</p>\n<h2 id=\"New-Technology-Considerations\"><a href=\"#New-Technology-Considerations\" class=\"headerlink\" title=\"New Technology Considerations\"></a>New Technology Considerations</h2><p>几十年来，缓存一直使用 SRAM 技术构建，但更新的内存技术将改变这一现状，因为传统 SRAM 缓存已经被证明有许多限制。</p>\n<h3 id=\"NVM-Cache\"><a href=\"#NVM-Cache\" class=\"headerlink\" title=\"NVM Cache\"></a>NVM Cache</h3><p>基于 NVM 的 LLC 缓存容量大且能耗低，但是由于其高写入延迟而导致性能下降。</p>\n<p>为了缓解这个问题，有人提出了两种缓存替换策略。首先，引入 Write Congestion Aware Bypass (WCAB) 策略，消除了对 NVM 缓存的大部分写入，同时避免了缓存命中率的大幅降低。其次，建立一个虚拟混合缓存，吸收并消除了冗余写入，否则会导致 NVM 写入缓慢。</p>\n<h3 id=\"DRAM-Cache\"><a href=\"#DRAM-Cache\" class=\"headerlink\" title=\"DRAM Cache\"></a>DRAM Cache</h3><p>使用 DRAM Cache 作为 L3 缓存和主存之间的桥梁。</p>\n<h1 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h1><ol>\n<li>现在的趋势是细粒度策略；</li>\n<li>现有细粒度策略大多直接使用粗粒度策略的老化机制，未来可以为细粒度策略定制老化机制；</li>\n<li>细粒度策略从过去的行为中学习，但影响其性能主要因素时预测准确性和处理不准确预测的能力；（将缓存替换看作监督学习问题 4.2.3节）</li>\n<li>将缓存替换策略与系统其它组件结合起来，例如预取器，似乎还可以考虑准入器；</li>\n</ol>\n","categories":["Cache"],"tags":["Cache"]},{"title":"cmake","url":"/2023/05/16/cmake/","content":"","categories":["工具"],"tags":["C/C++","工具"]},{"title":"Hello World","url":"/2024/04/25/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"S3fifo (SOSP 23)","url":"/2024/04/23/s3fifo%20(SOSP%2023)/","content":"<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>FIFO 优势：简单，速度快，可扩展性高，闪存友好；</p>\n<p>FIFO 劣势：缓存缺失率高。</p>\n<p>Insight：大多数对象在短时间内只会被访问一次，将这些对象尽早地从缓存中驱逐很关键。</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><ol>\n<li>缓存指标<ul>\n<li>高效率：命中率高；</li>\n<li>高性能：缓存命中或驱逐的操作尽可能少，保证系统高吞吐；</li>\n<li>高可扩展性：随着 CPU 核心数量的增长，系统吞吐尽可能线性增长；</li>\n</ul>\n</li>\n<li></li>\n</ol>\n","categories":["Cache"],"tags":["Cache","论文阅读"]},{"title":"初识 CXL","url":"/2023/11/14/%E5%88%9D%E8%AF%86CXL/","content":"<h1 id=\"CXL\"><a href=\"#CXL\" class=\"headerlink\" title=\"CXL\"></a>CXL</h1><p>CXL（<strong>Compute Express Link</strong>，计算快速链接）是处理器、加速器、智能网卡和内存设备之间的高速互连且支持缓存一致性的标准。</p>\n<p>CXL 技术保持了 CPU 内存空间和连接设备上的内存之间的一致性，这样就可以实现资源共享（或池化）以提高性能。</p>\n<h2 id=\"CXL-协议和标准\"><a href=\"#CXL-协议和标准\" class=\"headerlink\" title=\"CXL 协议和标准\"></a>CXL 协议和标准</h2><p>CXL 标准通过三种协议支持各种应用场景：CXL.io、CXL.cache 和 CXL.memory。</p>\n<ol>\n<li>CXL.io：该协议在功能上等同于 PCIe 协议。作为基本的通信协议，CXL.io 用途广泛，可满足各种应用场景的需求；</li>\n<li>CXL.cache：此协议专为更具体的应用程序而设计，使加速器能够有效地访问和缓存主机内存，从而优化性能；</li>\n<li>CXL.memory：此协议使主机（如处理器）能够使用 load&#x2F;store 命令访问设备连接的内存。</li>\n</ol>\n<p>这三种协议共同促进了计算设备（例如 CPU 主机和 AI 加速器）之间内存资源的一致共享。从本质上讲，这通过共享内存实现通信来简化编程。</p>\n<h2 id=\"CXL-与-PCIe-的关系\"><a href=\"#CXL-与-PCIe-的关系\" class=\"headerlink\" title=\"CXL 与 PCIe 的关系\"></a>CXL 与 PCIe 的关系</h2><p>CXL 建立在 PCIe 的物理和电气接口之上（CXL1.1 和 CXL2.0 使用 PCIe5.0 物理层，CXL3.0 使用 PCIe6.0 的物理层），允许备用协议使用 PCIe 的物理层。</p>\n<p>支持 CXL 的设备插入插槽后将默认作为 PCIe 设备，而如果主机处理器也支持 CXL，则它们的 CXL 事务协议将得到激活。 </p>\n<h2 id=\"为什么需要-CXL？\"><a href=\"#为什么需要-CXL？\" class=\"headerlink\" title=\"为什么需要 CXL？\"></a>为什么需要 CXL？</h2><ol>\n<li>为提高性能，数据中心使用大量的异构设备，比如从 CPU 卸载专用工作负载到专用加速器上。有了 CXL 后，<strong>CXL 的内存缓存一致性允许在 CPU 和加速器之间共享内存资源，而无需软件层的控制</strong>；</li>\n<li><strong>CXL 支持部署新的内存层</strong>，以弥合主内存和 SSD 存储之间的性能差距；</li>\n<li><strong>CXL 支持内存扩展功能</strong>，可在当今服务器中提供超出直连 DIMM 插槽的额外容量和带宽，通过连接 CXL 的设备向 CPU 主机处理器添加更多内存，提高内存容量敏感型工作负载（如 AI）的性能。</li>\n</ol>\n<h2 id=\"CXL-的特性和优势\"><a href=\"#CXL-的特性和优势\" class=\"headerlink\" title=\"CXL 的特性和优势\"></a>CXL 的特性和优势</h2><ol>\n<li>简化和改进了低延迟连接和内存一致性，从而显著提高了计算性能和效率;</li>\n<li>CXL 内存扩展功能；</li>\n</ol>\n<h2 id=\"哪些设备从-CXL-互连中受益？\"><a href=\"#哪些设备从-CXL-互连中受益？\" class=\"headerlink\" title=\"哪些设备从 CXL 互连中受益？\"></a>哪些设备从 CXL 互连中受益？</h2><ol>\n<li><strong>智能网卡等通常缺少本地内存的加速器</strong>。通过 CXL.io 和 CXL.cache 协议，这些设备可以与主机处理器的 DDR 内存进行通信；</li>\n<li><strong>CPU、GPU、ASIC 和 FPGA 等配备有 DDR 或 HBM 内存的设备</strong>。通过 CXL.io、CXL.cache 和 CXL.memory 协议，CPU 的内存在本地可以供 GPU 等加速器使用，GPU 等加速器的的内存在本地也可以供 CPU 使用，有助于提升异构工作负载；</li>\n<li><strong>内存设备</strong>。通过 CXL.io 和 CXL.memory 协议，内存设备可以被扩展和池化，为主机处理器提供额外的内存容量。</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.rambus.com/blogs/compute-express-link/\">Compute Express Link (CXL): All you need to know</a></li>\n<li></li>\n</ol>\n","categories":["CXL","新硬件"],"tags":["CXL"]},{"title":"HDD 和 SSD","url":"/2023/11/10/%E5%AD%98%E5%82%A8%E7%A1%AC%E4%BB%B6/","content":"<h1 id=\"存储硬盘\"><a href=\"#存储硬盘\" class=\"headerlink\" title=\"存储硬盘\"></a>存储硬盘</h1><p>经常傻傻地分不清存储硬盘协议或接口之间的关系和区别，因此在这里记录一下。</p>\n<h2 id=\"HDD-和-SSD\"><a href=\"#HDD-和-SSD\" class=\"headerlink\" title=\"HDD 和 SSD\"></a>HDD 和 SSD</h2><p>常用的非易失性存储包括机械硬盘（Hard Disk Drive，HDD）和固态硬盘（Solid State Drive，SSD），这是按照它们的硬盘结构原理区分的。HDD 将数据存储在盘片上，使用磁头在盘片上进行旋转进行读取和处理数据；SSD 则是用固态电子存储芯片阵列而制成的硬盘。</p>\n<h2 id=\"SATA-和-PCIe\"><a href=\"#SATA-和-PCIe\" class=\"headerlink\" title=\"SATA 和 PCIe\"></a>SATA 和 PCIe</h2><p>SATA 和 PCIe 都是传输接口，可以形象地将它们描述成<strong>马路</strong>。PCIe 的路宽而直，数据传输速度上限更快；SATA 的路窄而弯，数据传输速度上限较低。</p>\n<p>HDD 最高理论速度非常低，使用 SATA 接口就足够了；而在 SSD 流行后，SATA 接口成为了 SSD 速度的瓶颈，此时 PCIe 取代 SATA 成为了 SSD 最新的高带宽接口。</p>\n<h2 id=\"AHCI-和-NVMe\"><a href=\"#AHCI-和-NVMe\" class=\"headerlink\" title=\"AHCI 和 NVMe\"></a>AHCI 和 NVMe</h2><p>AHCI 和 NVMe 都是传输协议，可以形象地将它们描述成<strong>交通规则</strong>。</p>\n<p>AHCI 是针对 SATA 这种窄而弯的 “马路” 定制的 “交通规则”，此时修建了 PCIe 这种宽而直的 “马路” 后，仍然采用 “单行道” 实在太浪费 PCIe 通道了，于是就有了 NVMe，可以让 SSD 更有效地利用 PCIe 的高带宽。</p>\n<img src=\"/2023/11/10/%E5%AD%98%E5%82%A8%E7%A1%AC%E4%BB%B6/AHCI%E5%92%8CNVMe.jpg\" class=\"\" title=\"AHCI和NVMe\">\n\n\n\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><ol>\n<li><a href=\"https://baijiahao.baidu.com/s?id=1616207956596122967&wfr=spider&for=pc\">固态硬盘的PCIE，SATA，M2，NVMe，AHCI分别都指什么？别再搞混了</a></li>\n<li><a href=\"https://www.zhihu.com/question/48972075\">SSD中，SATA、m2、PCIE和NVME各有什么意义呢？</a></li>\n<li><a href=\"https://www.userbenchmark.com/Faq/What-s-the-difference-between-SATA-PCIe-and-NVMe/105\">What’s the difference between SATA, PCIe and NVMe?</a></li>\n</ol>\n","categories":["SSD","HDD"],"tags":["硬盘"]},{"title":"测试文章","url":"/2022/10/20/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/","content":"<p>这是一篇测试文章。</p>\n<ul>\n<li>测试在hexo-blog文件夹下运行hexo clean &amp;&amp; hexo deploy是否可以执行成功。√</li>\n<li>测试在hexo-blog文件夹下运行hexo generate &amp;&amp; hexo deploy是否可以执行成功。√</li>\n<li><strong>推荐</strong>：hexo g &amp;&amp; hexo d</li>\n</ul>\n<img src=\"/2022/10/20/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/%E6%98%9F%E5%A4%9C.jpg\" class=\"\" title=\"图片引用方法一\">\n\n","categories":["测试"],"tags":["测试"]},{"title":"牛客网——算法入门（栈）","url":"/2022/11/15/%E7%89%9B%E5%AE%A2%E7%BD%91%E2%80%94%E2%80%94%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%EF%BC%88%E6%A0%88%EF%BC%89/","content":"<h1 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h1><h2 id=\"AB2\"><a href=\"#AB2\" class=\"headerlink\" title=\"AB2\"></a>AB2</h2><h3 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h3><p>输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。</p>\n<ol>\n<li><p>0&lt;&#x3D;pushV.length &#x3D;&#x3D; popV.length &lt;&#x3D;1000</p>\n</li>\n<li><p>-1000&lt;&#x3D;pushV[i]&lt;&#x3D;1000</p>\n</li>\n<li><p>pushV 的所有数字均不相同</p>\n</li>\n</ol>\n<h3 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">输入：[1,2,3,4,5],[4,5,3,2,1]</span><br><span class=\"line\">返回值：true</span><br><span class=\"line\">说明：可以通过push(1)=&gt;push(2)=&gt;push(3)=&gt;push(4)=&gt;pop()=&gt;push(5)=&gt;pop()=&gt;pop()=&gt;pop()=&gt;pop()</span><br><span class=\"line\">这样的顺序得到[4,5,3,2,1]这个序列，返回true</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">输入：[1,2,3,4,5],[4,3,5,1,2]</span><br><span class=\"line\">返回值：false</span><br><span class=\"line\">说明：由于是[1,2,3,4,5]的压入顺序，[4,3,5,1,2]的弹出顺序，要求4，3，5必须在1，2后压入，且1，2不能弹出，但是这样压入的顺序，1又不能在2之前弹出，所以无法形成的，返回false  </span><br></pre></td></tr></table></figure>\n\n<h3 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h3><p>模拟栈操作。</p>\n<ol>\n<li>记popV的第一个元素为p<sub>1</sub>，那么pushV中p<sub>1</sub>之前元素全部顺序入栈，p<sub>1</sub>入栈再出栈；popV的后续元素如果依次等于栈顶元素，持续进行出栈操作，直至不相等或栈为空，记popV此时元素为p<sub>i</sub>。</li>\n<li>pushV中p<sub>1</sub>之后，p<sub>i</sub>之前的元素继续顺序入栈，而p<sub>i</sub>入栈再出栈；popV的后续元素如果依次等于栈顶元素，持续进行出栈操作，直至不相等或栈为空。</li>\n<li>……</li>\n<li>直到pushV中元素全部入栈，此时后续只有一种出栈方式，即栈中元素依次出栈与popV后续元素一一比较。如果某次不相等，则无法以pushV顺序压入，popV顺序弹出。</li>\n</ol>\n<p>代码如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Solution</span> &#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">IsPopOrder</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; pushV,vector&lt;<span class=\"type\">int</span>&gt; popV)</span> </span>&#123;</span><br><span class=\"line\">        stack&lt;<span class=\"type\">int</span>&gt; s;</span><br><span class=\"line\">        <span class=\"type\">int</span> p_length = pushV.<span class=\"built_in\">size</span>(), v_length = popV.<span class=\"built_in\">size</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(p_length == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"type\">int</span> cur_push_pos = <span class=\"number\">0</span>, cur_pop_pos = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">do</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(pushV[cur_push_pos] != popV[cur_pop_pos])&#123;</span><br><span class=\"line\">                s.<span class=\"built_in\">push</span>(pushV[cur_push_pos++]); <span class=\"comment\">// pushV中popV[cur_pop_pos]之前元素全部入栈</span></span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                cur_push_pos++;</span><br><span class=\"line\">                cur_pop_pos++; <span class=\"comment\">// popV[cur_pop_pos]入栈再出栈</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(!s.<span class=\"built_in\">empty</span>())&#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(s.<span class=\"built_in\">top</span>() == popV[cur_pop_pos])&#123; <span class=\"comment\">// 是否连续出栈操作</span></span><br><span class=\"line\">                    s.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">                    cur_pop_pos++;</span><br><span class=\"line\">                &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">while</span>(cur_push_pos &lt; p_length);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(!s.<span class=\"built_in\">empty</span>() &amp;&amp; cur_pop_pos &lt; v_length)&#123; <span class=\"comment\">// pushV全部压入后，此时后续只有一种出栈</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span>(s.<span class=\"built_in\">top</span>() == popV[cur_pop_pos])&#123;</span><br><span class=\"line\">                s.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\">                cur_pop_pos++;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> s.<span class=\"built_in\">empty</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>","categories":["刷题"],"tags":["算法","数据结构","栈"]},{"title":"论文阅读","url":"/2023/04/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","content":"","categories":["论文阅读"],"tags":["论文阅读"]},{"title":"数据处理杂谈","url":"/2023/03/05/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%9D%82%E8%B0%88/","content":"<h1 id=\"数据处理杂谈\"><a href=\"#数据处理杂谈\" class=\"headerlink\" title=\"数据处理杂谈\"></a>数据处理杂谈</h1><h2 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h2><ol>\n<li>对数据集<strong>唯一对象</strong>进行筛选和处理时，可以考虑使用 <code>glib.h</code> 中的 <code>GHashTable</code> 来代替 <code>STL</code> 中的 <code>unordered_map</code>；非常简单易用，例如：<ul>\n<li><code> g_hash_table_new(g_direct_hash, g_direct_equal)</code> 创建  <code>GHashTable</code>；</li>\n<li><code>g_hash_table_contains(obj_table, (gconstpointer)req-&gt;obj_id)</code> 判断 <code>obj_id</code> 是否在哈希表中；</li>\n<li><code>g_hash_table_add(obj_table, (gpointer)req-&gt;obj_id)</code> 将 <code>obj_id</code> 插入哈希表中。</li>\n</ul>\n</li>\n</ol>\n","categories":["数据集"],"tags":["数据集","数据处理"]}]